\documentclass{huhtakm-template-book-v2}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\prob}{P}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\NBin}{NBin}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Bet}{Beta}
\setlength{\parindent}{0pt}
\title{
	\Huge MATH 3423: Statistical Inference
}
\author{
	HU-HTAKM\\
	\small Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: October 31, 2024\\
	Last small update: October 31, 2024
}
\begin{document}
\maketitle
This is MATH 3423 lecture note made by me. Note that some of the notations are slightly different with what is used in the course for better clarity. For example:\\
\begin{tabular}{||c|c|c||}
	\hline
	Name & My notation & Ycw's notation\\
	\hline
	Transpose & $A^{T}$ &  $A'$\\
	Fisher Information & $\mathcal{I}_{X}(\theta)$ & $I_{X}(\theta)$\\
	\hline
\end{tabular}
\tableofcontents
\chapter{Preliminary}
Statistical inference is a statistical process which investigates how to use the information from the data to make an inference about the distribution of the random variable of our interest. In MATH 3423, we will focus on two core concepts of statistical inference: point estimation and hypothesis testing.
\section{Random sample and parametric distribution}
To make a statistical inference about the distribution of the random variable $X$ of our interest, we need to draw a sample of data.
\begin{defn}
	Denote the first observation by $X_{1}$, the second by $X_{2}$, and so on. A set of random variables $\{X_{1},\cdots,X_{n}\}$ are called a \textbf{random sample} of size $n$ from the common distribution of $X$ with a PMF $p_{X}(x)$ or PDF $f_{X}(x)$ if they are independent and identically distributed (i.i.d.).
\end{defn}
\begin{rem}
	Random variables $X_{1},\cdots,X_{n}$ are assumed to be observable with known actual values $x_{1},\cdots,x_{n}$ respectively.
\end{rem}
Under the random sampling setting, it is easy to get the following lemma.
\begin{lem}
	Given a random sample $\{X_{1},\cdots,X_{n}\}$ of a common distribution $X$.
	\begin{enumerate}
		\item If the random sample is discrete with a common PMF $p_{X}(x)$, then the joint PMF of random sample can be obtained by:
		\begin{equation*}
			p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n})=\prod_{i=1}^{n}p_{X_{i}}(x_{i})
		\end{equation*}
		\item If the random sample is continuous with a common PDF $f_{X}(x)$, then the joint PDF of random sample can be obtained by:
		\begin{equation*}
			f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n})=\prod_{i=1}^{n}f_{X_{i}}(x_{i})
		\end{equation*}
	\end{enumerate}
\end{lem}
The underlying distribution of $X$ is assumed to be unknown or partially known in practice. In most situation, it is reasonable to assume that the form of the PMF $p_{X}$ or PDF $f_{X}$ of the distribution is known but it contains some unknown parameters $\theta$.
\begin{defn}
	\textbf{Parametric distribution} is a distribution which the PMF $p_{X}$ and PDF $f_{X}$ contains some unknown parameters $\theta$. The PMF or PDF is said to be \textbf{parametric}.
\end{defn}
\begin{rem}
	Instead of using parametric distributions of the data, we may assume that the form of the distribution is unknown but that the distribution has some properties. E.g. A distribution is continuous. This distribution is called a \textbf{non-parametric distribution} and the corresponding statistical method is called \textbf{non-parametric statistical approach}. If parameters are involved but the form of the distribution is unknown, then such a distribution is called \textbf{semi-parametric distribution} and the corresponding method is called \textbf{semi-parametric statistical approach}.
\end{rem}

\newpage
\begin{eg}
	Data are usually assumed from the normal distribution with mean $\mu$ and variance $\sigma^{2}$, where the parameter:
	\begin{equation*}
		\theta=\begin{pmatrix}
			\mu\\
			\sigma^{2}
		\end{pmatrix}
	\end{equation*}
	is unknown but fixed.
\end{eg}
\begin{lem}
	Given a random sample $\{X_{1},\cdots,X_{n}\}$ of a common distribution $X$.
	\begin{enumerate}
		\item If the random sample is discrete with a common parametric PMF $p_{X}(x|\theta)$, then the joint PMF of random sample can be obtained by:
		\begin{equation*}
			p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}p_{X}(x_{i}|\theta)
		\end{equation*}
		\item If the random sample is continuous with a common parametric PDF $p_{X}(x|\theta)$, then the joint PDF of random sample can be obtained by:
		\begin{equation*}
			f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}f_{X}(x_{i}|\theta)
		\end{equation*}
	\end{enumerate}
\end{lem}
Under parametric setting, we can see that uncertainty of the distribution is the uncertainty of its parameters. One of the central problem in statistics is how to determine which function of data is the best one to estimate $\theta$.
\begin{defn}
	If $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$ is a random vector and $T(\cdot)$ is a real-valued or vector-valued function such that for all $\mathbf{X}\in\Omega$, $T(\mathbf{X})$ does not contain any unknown parameters, then $T(\mathbf{X})$ is called a \textbf{statistic}.
	\begin{enumerate}
		\item If we use the statistic $T(\mathbf{X})$ to estimate an unknown parameter $\theta$, then we call $T(\mathbf{X})$ and $T(\mathbf{x})$ an \textbf{estimator} and an \textbf{estimate} of $\theta$ respectively, where $\mathbf{x}$ is an observed value of $\mathbf{X}$.
	\end{enumerate}
\end{defn}
\begin{rem}
	We usually denote an estimator of $\theta$ by $\hat{\theta}(\mathbf{X})$ or simply $\hat{\theta}$.
\end{rem}
\begin{rem}
	Since $T(\mathbf{X})$ is also random, we have a distribution of $T(\mathbf{X})$ called \textbf{sampling distribution}.
\end{rem}

\section{Moments}
The population moments of a distribution play an important role in theoretical and applied statistics.
\begin{defn}
	For each positive integer $k$, the \textbf{$k$-th population moment} of $X$ about $0$, denoted by $\mu_{k}'$, is defined as:
	\begin{equation*}
		\mu_{k}'=\E(X^{k})
	\end{equation*}
	if the expectation exists.\\
	When $k=1$, $\mu_{1}'$ is the population mean $\mu=E(X)$ of $X$.
\end{defn}
\begin{defn}
	The \textbf{$k$-th population central moment} of $X$, denoted by $\mu_{k}$, is defined as:
	\begin{equation*}
		\mu_{k}=\E(X-\mu)^{k}
	\end{equation*}
	if the expectation exists.\\
	When $k=2$, $\mu_{2}$ is the \textbf{population variance} $\sigma^{2}$ of $X$.
\end{defn}
\begin{rem}
	Don't confuse the population mean $\mu$ and the $k$-th population central moment $\mu_{k}$!
\end{rem}

\newpage
\begin{eg}
	We have terminologies for some useful population moments.
	\begin{enumerate}
		\item \textbf{Skewness}: $\mu_{3}$, a measure of symmetry or skewness.
		\begin{enumerate}
			\item If $\mu_{3}<0$, then the curve is left-skewed (tail is on the left).
			\item If $\mu_{3}>0$, then the curve is right-skewed (tail is on the right).
			\item If $\mu_{3}=0$, the the curve is symmetrical.
		\end{enumerate}
		The ratio $\frac{\mu_{3}}{\sigma^{3}}$ is called the \textbf{coefficient of skewness}.
		\item \textbf{Kurtosis}: $\mu_{4}$, a measure of excess or kurtosis, which is the degree of flatness of a density near its center. We called $\frac{\mu_{4}}{\sigma^{4}}-3$ the \textbf{coefficient of kurtosis}.
		\begin{enumerate}
			\item If $\frac{\mu_{4}}{\sigma^{4}}-3>0$, then the density has a sharper peak than the density of the normal curve.
			\item If $\frac{\mu_{4}}{\sigma^{4}}-3<0$, then the density has a flatter peak than the density of the normal curve.
		\end{enumerate}
	\end{enumerate}
\end{eg}
We usually use sample moments to estimate the population moments.
\begin{defn}
	Let $X_{1},\cdots,X_{n}$ be a random sample of size $n$. For each positive integer $k$,
	\begin{enumerate}
		\item the \textbf{$k$-th sample moment} about $0$, denoted by $\overline{X^{k}}$, is defined as:
		\begin{equation*}
			\overline{X^{k}}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}
		\end{equation*}
		\item the \textbf{$k$-th sample moment} about $\overline{X}$, denoted by $S_{n}^{k}$, is defined as:
		\begin{equation*}
			S_{n}^{k}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{k}
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{eg}
	When $k=1$, $\overline{X}$ is called the \textbf{sample mean} of $X$. When $k=2$, $S_{n}^{2}$ is called the \textbf{sample variance}. However, we usually don't use this version of sample variance. Instead, we use another sample variance, denoted by $S_{n-1}^{2}$, which is defined by:
	\begin{equation*}
		S_{n-1}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}
	\end{equation*}
	We do this because $S_{n-1}^{2}$ is unbiased while $S_{n}^{2}$ is not unbiased.
\end{eg}
\begin{lem}
	Let $X_{1},\cdots,X_{n}$ be a random sample of size $n$. We have:
	\begin{equation*}
		\E(\overline{X^{k}})=\mu_{k}'
	\end{equation*}
	if $\mu_{k}'$ exists. We also have:
	\begin{equation*}
		\Var(\overline{X^{k}})=\frac{1}{n}[\mu_{2k}'-(\mu_{k}')^{2}]
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X_{1},\cdots,X_{n}$ have the same distribution, we have:
	\begin{equation*}
		\E(X_{1}^{k})=\cdots=\E(X_{n}^{k})=\E(X^{k})=\mu_{k}'
	\end{equation*}
	Therefore, we have:
	\begin{equation*}
		\E(\overline{X^{k}})=\E\left[\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\right]=\frac{1}{n}\sum_{i=1}^{n}\E(X_{i}^{k})=\frac{n\mu_{k}'}{n}=\mu_{k}'
	\end{equation*}
	Since $X_{1}^{k},\cdots,X_{n}^{k}$ are independent,
	\begin{align*}
		\Var(\overline{X^{k}})=\Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\right)=\frac{1}{n^{2}}\Var\left(\sum_{i=1}^{n}X_{i}^{k}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}\Var(X_{i}^{k})&=\frac{1}{n^{2}}\sum_{i=1}^{n}\left[\E(X_{i}^{2k})-(\E(X_{i}^{k})^{2})\right]\\
		&=\frac{1}{n}[\mu_{2k}'-(\mu_{k}')^{2}]
	\end{align*}
\end{proofing}

\section{Moment generating function}
It would be useful if we have a function that could generate all moments.
\begin{defn}
	The \textbf{moment generating function} (MGF) of a random variable $X$, denoted by $M_{X}(t)$, is:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})
	\end{equation*}
	if the expectation exists for $t$ in some neighbourhood of $0$.
\end{defn}
\begin{rem}
	More precisely, there exists $h>0$ such that for all $t$ in $(-h,h)$, $\E(e^{tX})$ exists.
\end{rem}
\begin{rem}
	MGF of $X$ may not always exist. However, if it exists, then $M_{X}(t)$ is continuously differentiable in some neighbourhood of the origin.
\end{rem}
\begin{rem}
	If we replace $e^{tX}$ by its taylor series, then we would get:
	\begin{equation*}
		M_{X}(t)=\E\left[\sum_{i=0}^{\infty}\frac{(tX)^{i}}{i!}\right]=\sum_{i=0}^{\infty}\frac{t^{i}}{i!}\E(X^{i})=\sum_{i=0}^{\infty}\frac{t^{i}}{i!}\mu_{i}'
	\end{equation*}
\end{rem}
\begin{lem}
	If $M_{X}(t)$ is the MGF of a random variable $X$, then:
	\begin{equation*}
		\left.\pdv*[order={k}]{M_{X}(t)}{t}\right|_{t=0}=\E(X^{k})=\mu_{k}'
	\end{equation*}
\end{lem}
\begin{proofing}
	From the taylor series of MGF, we would see that:
	\begin{equation*}
		\pdv*[order={k}]{M_{X}(t)}{t}=\sum_{i=k}^{\infty}\frac{t^{i-k}}{(i-k)!}\E(X^{i})
	\end{equation*}
	Therefore, by having $t=0$, we would have:
	\begin{equation*}
		\left.\pdv*[order={k}]{M_{X}(t)}{t}\right|_{t=0}=\E(X^{k})
	\end{equation*}
\end{proofing}
\begin{eg}
	What is the MGF of $X\sim\Bern(p)$? We have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=e^{t(0)}(1-p)+e^{t(1)}(p)=pe^{t}+1-p
	\end{equation*}
\end{eg}
\begin{lem}
	If random variables $X$ and $Y$ are independent, then:
	\begin{equation*}
		M_{X+Y}(t)=M_{X}(t)M_{Y}(t)
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X$ and $Y$ are independent,
	\begin{equation*}
		M_{X+Y}(t)=\E(e^{t(X+Y)})=\E(e^{tX})\E(e^{tY})=M_{X}(t)M_{Y}(t)
	\end{equation*}
\end{proofing}
\begin{eg}
	By definition, if $Y=\Bin(n,p)$, then $Y=X_{1}+\cdots+X_{n}$ where $X_{i}\sim\Bern(p)$ for all $i$ and they are independent. Therefore,
	\begin{equation*}
		M_{Y}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=(pe^{t}+1-p)^{n}
	\end{equation*}
\end{eg}

\newpage
\begin{eg}
	Consider $X\sim\Poisson(\lambda)$, The MGF of $X$ can be obtained by:
	\begin{equation*}
		M_{X}(t)=\sum_{k=0}^{\infty}e^{tk}\frac{\lambda^{k}e^{-\lambda}}{k!}=e^{-\lambda}\sum_{k=0}^{\infty}\frac{(\lambda e^{t})^{k}}{k!}=e^{\lambda(e^{t}-1)}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider $X\sim\Exp(\lambda)$. If $t<\lambda$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=\int_{0}^{\infty}e^{tx}\lambda e^{-\lambda x}\,dx=\lambda\int_{0}^{\infty}e^{-(\lambda-t)x}\,dx=\frac{\lambda}{\lambda-t}
	\end{equation*}
\end{eg}
\begin{eg}
	What is the MGF of $X\sim\N(\mu,\sigma^{2})$? We may first find the MGF of $Z\sim\N(0,1)$.
	\begin{align*}
		M_{Z}(t)=\E(e^{tZ})&=\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z^{2}-2tz)}\,dz\\
		&=\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}((z-t)^{2}-t^{2})}\,dz\\
		&=e^{\frac{t^{2}}{2}}\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z-t)^{2}}\,dz\\
		&=e^{\frac{t^{2}}{2}}
	\end{align*}
	Therefore, by having $X=\sigma Z+\mu$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=e^{\mu t}\E(e^{t\sigma Z})=e^{\mu t+\frac{1}{2}\sigma^{2}t^{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	If $X\sim\NBin(r,p)$, then for $t<-\ln(1-p)$:
	\begin{equation*}
		M_{X}(t)=\left(\frac{pe^{t}}{1-(1-p)e^{t}}\right)^{r}
	\end{equation*}
	If $X\sim\U[a,b]$, then:
	\begin{equation*}
		M_{X}(t)=\frac{e^{bt}-e^{at}}{(b-a)t}
	\end{equation*}
	If $X\sim\Gam(\alpha,\beta)$, then for $t<\beta$:
	\begin{equation*}
		M_{X}(t)=\left(\frac{\beta}{\beta-t}\right)^{\alpha}
	\end{equation*}
\end{eg}
Ultimately, the reason why we use moment generating function is the following fact.
\begin{thm}\named{Uniqueness of MGF}
	Let $X$ and $Y$ be two random variables. Suppose that their MGFs exist and are equal for all $t\in(-h,h)$ for some $h>0$, then we have the distribution function $F_{X}$ and $F_{Y}$ are equal.
\end{thm}
This means that by knowing the MGF of a particular random variable $X$, we can know its distribution.
\begin{eg}
	Assume that $X_{1},\cdots,X_{n}$ are independent and $X_{i}\sim\Bin(m_{i},p)$ for all $i=1,\cdots,n$. Then we have:
	\begin{equation*}
		M_{X_{1}+\cdots+X_{n}}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=\prod_{i=1}^{n}(pe^{t}+1-p)^{m_{i}}=(pe^{t}+1-p)^{\sum_{i=1}^{n}m_{i}}
	\end{equation*}
	Therefore, we have $X_{1}+\cdots+X_{n}\sim\Bin(\sum_{i=1}^{n}m_{i},p)$.
\end{eg}
\begin{eg}
	Assume that $X_{1},\cdots,X_{n}$ are independent and $X_{i}\sim\Poisson(\lambda_{i})$ for all $i=1,\cdots,n$. Then we have:
	\begin{equation*}
		M_{X_{1}+\cdots+X_{n}}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=\prod_{i=1}^{n}e^{\lambda_{i}(e^{t}-1)}=e^{\sum_{i=1}^{n}\lambda_{i}(e^{t}-1)}
	\end{equation*}
	Therefore, we have $X_{1}+\cdots+X_{n}\sim\Poisson(\sum_{i=1}^{n}\lambda_{i})$.
\end{eg}

\newpage
\begin{eg}
	Similarly, given a set of independent random variables $\{X_{1},\cdots,X_{n}\}$.
	\begin{enumerate}
		\item If $X_{i}\sim\NBin(r_{i},p)$, then $X_{1}+\cdots+X_{n}\sim\NBin(\sum_{i=1}^{n}r_{i},p)$.
		\item If $X_{i}\sim\N(\mu_{i},\sigma_{i}^{2})$, then $X_{1}+\cdots+X_{n}\sim\N(\sum_{i=1}^{n}\mu_{i},\sum_{i=1}^{n}\sigma_{i}^{2})$.
		\item If $X_{i}\sim\Gam(\alpha_{i},\beta)$, then $X_{1}+\cdots+X_{n}\sim\Gam(\sum_{i=1}^{n}\alpha_{i},\beta)$.
	\end{enumerate}
\end{eg}
\begin{rem}
	Not all sum of distributions will result in the same type of distribution.
\end{rem}
More generally, we would deal with problems of limiting distribution.
\begin{thm}
	Suppose the $\{X_{n}\}$ is a sequence of random variables, each with MGF $M_{X_{n}}(t)$. If we have:
	\begin{equation*}
		\lim_{n\to\infty}M_{X_{n}}(t)=M_{Y}(t)
	\end{equation*}
	for all $t$ in a neighbourhood of $0$, where $M_{Y}(t)$ is a MGF for some random variables $Y$, then there is an unique distribution function $F_{Y}$ with corresponding $M_{Y}(t)$ such that:
	\begin{equation*}
		\lim_{n\to\infty}F_{X_{n}}(y)=F_{Y}(y)
	\end{equation*}
	for all $y$ where $F_{Y}(y)$ is continuous. We denote by $X_{n}\to Y$ or $X_{n}\xrightarrow{D}X$.
\end{thm}
\begin{rem}
	Simply, the limiting distribution of $X_{n}$ is equal to the distribution of $Y$.
\end{rem}
We may define the limiting convergence in truly theoretical way.
\begin{defn}
	A sequence of random variables $\{X_{n}\}$ \textbf{converges in distribution} to a random variable $X$, denoted by $X_{n}\xrightarrow{D}X$, if for all continuity point $x$ of $F_{X}$, as $n\to\infty$,
	\begin{equation*}
		F_{X_{n}}(x)\to F_{X}(x)
	\end{equation*}
\end{defn}
We also have a more strict convergence.
\begin{defn}
	A sequence of random variables $\{X_{n}\}$ \textbf{converges in probability} to a random variable $X$, denoted by $X_{n}\xrightarrow{\prob}X$, if for any $\varepsilon>0$, as $n\to\infty$,
	\begin{align*}
		\prob(\abs{X_{n}-X}<\varepsilon)&\to 1 & \prob(\abs{X_{n}-X}\geq\varepsilon)&\to 0
	\end{align*}
\end{defn}
\begin{rem}
	If $X_{n}\xrightarrow{P}X$, then $X_{n}\xrightarrow{D}X$. The converse is not necessarily true.
\end{rem}

\section{Limit Theorems}
Using the last two theorems, the next two theorems are very useful in both statistics and probability theory by giving us approximate distribution of an average without a lot of distributional assumption.
\begin{thm}\named{Weak Law of Large Numbers (WLLN)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables. Let $\E(X_{i})=\mu$ for all $i=1,2,\cdots$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$
	\begin{equation*}
		\overline{X}\xrightarrow{D}\mu
	\end{equation*}
\end{thm}
\begin{thm}\named{Classical Central Limit Theorem (CLT)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables whose MGFs exist in a neighbourhood of $0$. Let $\E(X_{i})=\mu$ and $\Var(X_{i})=\sigma^{2}>0$ for all $i=1,2,\cdots$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\to\N(0,1)
	\end{equation*}
\end{thm}
\begin{rem}
	This is just an abuse of notations.
\end{rem}

\newpage
This works generally to most of the distribution. However, it is probably very tedious to find the MGF. We cam apply the following version of CLT instead.
\begin{thm}\named{L\'evy-Linderberg Central Limit Theorem}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables with common population means $\mu$ and common population variance $\sigma^{2}$. Assume that $0<\sigma^{2}<\infty$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\to\N(0,1)
	\end{equation*}
\end{thm}
Sometimes, we will deal with a function of multiple random variables. We must establish how they converges.
\begin{thm}\named{Slutsky's Theorem}
	If $X_{n}\xrightarrow{D}X$ and $Y_{n}\xrightarrow{\prob}c$, then:
	\begin{enumerate}
		\item $X_{n}+Y_{n}\xrightarrow{D}X+c$
		\item $X_{n}Y_{n}\xrightarrow{D}cX$
		\item $\frac{X_{n}}{Y_{n}}\xrightarrow{D}\frac{X}{c}$ if $c\neq 0$.
	\end{enumerate}
\end{thm}
\begin{eg}
	Assume that $X_{i}\sim\Bern(p)$ for all $i$. We want to estimate the unknown $p$. We have common mean $\mu=p$ and common variance $\sigma^{2}=p(1-p)$. By applying CLT, as $n\to\infty$,
	\begin{equation*}
		\overline{X}\to\N\left(p,\frac{p(1-p)}{n}\right)
	\end{equation*}
	Therefore, we can use normal distribution to approximate the unknown parameter. We want an estimation that we can confident about, and commonly we use probability $0.95$.
	\begin{equation*}
		0.95=\prob\left(-z_{0.025}\leq\frac{\overline{X}-p}{\sqrt{\frac{p(1-p)}{n}}}\leq z_{0.025}\right)=\prob\left((\overline{X}-p)^{2}\leq z_{0.025}^{2}\frac{p(1-p)}{n}\right)
	\end{equation*} 
	Solving the inequality, we would find an interval that estimates the parameter $p$. However, this is highly inconvenient. We may use another method. Let us replace $\sqrt{\frac{p(1-p)}{n}}$ with $\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}$. As $n\to\infty$,
	\begin{equation*}
		\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}=\sqrt{\frac{p(1-p)}{n}}\sqrt{\frac{p(1-p)}{\overline{X}(1-\overline{X})}}\to\sqrt{\frac{p(1-p)}{n}}
	\end{equation*}
	since by Slutsky's Theorem, $\sqrt{\frac{p(1-p)}{\overline{X}(1-\overline{X})}}\to 1$ as $\overline{X}\to p$ by WLLN. We have:
	\begin{equation*}
		0.95=\prob\left(-z_{0.025}\leq\frac{\overline{X}-p}{\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}}\leq z_{0.025}\right)=\prob\left(\overline{X}-z_{0.025}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\leq p\leq\overline{X}+z_{0.025}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\right)
	\end{equation*} 
\end{eg}
\begin{eg}
	In a survey before an election, a poll was taken of $300$ potential voters. Among them, $120$ said that they would vote for candidate A. Determine a $95\%$ confidence interval for the population proportion $p_{A}$ of voters who would vote for candidate A in the election.\\
	From the poll, we have a point estimate $\overline{x}=\hat{p}_{A}=\frac{120}{300}=0.4$. From the last example, we have found that the $95\%$ confidence interval is:
	\begin{equation*}
		\left(\overline{x}-z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}},\overline{x}+z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}\right)\approx(0.3446,0.4554)
	\end{equation*}
	Equivalently, the percentage of voters of candidate A would be from $34.46\%$ to $45.54\%$, with a margin of error $5.54\%$.
\end{eg}

\newpage
\begin{eg}
	Following the previous example. Assume that we have been given a margin of error $D$. How many data should we collect in order to to have the margin of error?\\
	From how we find the margin of error:
	\begin{equation*}
		z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}=D\implies n=p(1-p)\frac{z_{0.025}^{2}}{D^{2}}
	\end{equation*}
	Since $p(1-p)\leq\frac{1}{4}$, if we specify that $D=0.05$, we have:
	\begin{equation*}
		n\leq\frac{z_{0.025}^{2}}{4D^{2}}=\frac{1.96^{2}}{4(0.05)^{2}}\leq\frac{2^{2}}{4(0.05)^{2}}=400
	\end{equation*}
	We may use this to determine whether we have obtained enough data.\\
	Assume that we have $n^{*}$ respondents. Is it enough? The number of required respondents is obtained by:
	\begin{equation*}
		n_{\text{required}}=\frac{\overline{x^{*}}(1-\overline{x^{*}})z_{0.025}^{2}}{D^{2}}
	\end{equation*}
	If $n^{*}<n_{\text{required}}$, then the current number of data is not enough. We would need to find more respondents.\\
	If $n^{*}\geq n_{\text{required}}$, then it is enough.
\end{eg}
\begin{eg}
	We try to use Poisson random variables to prove that as $n\to\infty$,
	\begin{equation*}
		e^{-n}\sum_{k=0}^{n}\frac{n^{k}}{k!}\to\frac{1}{2}
	\end{equation*}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Poisson(1)$ for $i=1,2,\cdots$. Let $Y_{n}=\sum_{i=1}^{n}X_{i}$. By CLT, we have:
	\begin{equation*}
		\frac{Y_{n}-n}{\sqrt{n}}\to\N(0,1)
	\end{equation*}
	Therefore, since $Y_{n}\sim\Poisson(n)$, we have:
	\begin{equation*}
		e^{-n}\sum_{k=0}^{n}\frac{n^{k}}{k!}=\prob(Y_{n}\leq n)=\prob(\frac{Y_{n}-n}{\sqrt{n}}\leq 0)\to\frac{1}{2}
	\end{equation*}
\end{eg}
\begin{eg}
	Given a sequence of i.i.d. random variables $\{X_{n}\}$. We want to find the asymptotic distribution for the $k$-th sample moment $\overline{X^{k}}$ as $n\to\infty$. Notice that $X_{i}^{k}$ are independent for $i=1,2,\cdots$. By CLT,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X^{k}}-\mu_{k}')}{\sqrt{\mu_{2k}'-(\mu_{k}')^{2}}}\to\N(0,1)
	\end{equation*}
	Therefore, the asymptotic distribution for $\overline{X^{k}}$ when $n\to\infty$ is $\N(\mu_{k}',\frac{1}{n}(\mu_{2k}'-(\mu_{k}')^{2}))$.
\end{eg}
Central Limit Theorem can provide us a limiting standard normal distribution of sample mean. However, we usually deal with some functions of the sample mean.
\begin{thm}\named{Continuous mapping theorem}
	Let $\{X_{n}\}$ be a sequence of random variables and $X$ be a random variable. Suppose there is a function $g$ has a set of discontinuity points $D_{g}$ such that $\prob(X\in D_{g})=0$, then:
	\begin{enumerate}
		\item If $X_{n}\xrightarrow{D}X$, then $g(X_{n})\xrightarrow{D}g(X)$.
		\item If $X_{n}\xrightarrow{\prob}X$, then $g(X_{n})\xrightarrow{\prob}g(X)$.
	\end{enumerate}
\end{thm}
\begin{thm}\named{Delta method}
	Let $\{X_{n}\}$ be a sequence of random variables such that for constants $a$ and $b>0$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(X_{n}-a)\to\N(0,b^{2})
	\end{equation*}
	Then for a given function $g$, suppose that $g'(a)$ exists and not $0$, as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(g(X_{n})-g(a))\to\N(0,[g'(a)b]^{2})
	\end{equation*}
\end{thm}
\begin{cor}
	\label{Chapter 1 (Corollary) CLT for functions of random variables}
	If $\overline{X}$ is the sample mean of a random sample $X_{1},\cdots,X_{n}$ of size $n$ from a distribution with a finite mean $\mu$ and finite variance $\sigma^{2}>0$. For a given function $g$, suppose that $g'(\mu)$ exists and is not $0$, as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\mu))\to\N(0,[g'(\mu)\sigma]^{2})
	\end{equation*}  
\end{cor}
\begin{eg}
	Assume that there are $70$ respondents, $68$ of which would vote for one candidate.\\
	If we use the same process from previous examples, we would find that the $95\%$ confidence interval is $(0.9324,1.0105)$, which is out of the range. In fact, if the point estimate $\hat{p}$ is quite close to $0$ or $1$, the resulting interval guess may include values that is out of the range of $p$. This is a poor interval guess.\\
	We take a transformation, say $g(p)$, such that $g(p)\in(-\infty,\infty)$. We may find that since $0<p<1$, $\ln{p}<0$. Therefore, we can find that:
	\begin{equation*}
		g(p)=\ln(-\ln{p})\in(-\infty,\infty)
	\end{equation*}
	By Delta method,
	\begin{equation*}
		\frac{g(\overline{X})-g(p)}{g'(p)\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}}\to\N(0,1)
	\end{equation*}
	By WLLN and continuous mapping theorem, we can replace $g'(p)$ with $g'(\overline{X})$. Therefore, we have:
	\begin{align*}
		0.95=\prob\left(-z_{0.025}\leq	\frac{g(\overline{X})-g(p)}{g'(p)\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}}\leq z_{0.025}\right)
	\end{align*}
	Solving the formula and we would get the a good $95\%$ confidence interval of $p$.
\end{eg}
\begin{eg}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Bern(\theta)$ for $i=1,2,\cdots$. Show that:
	\begin{equation*}
		Z_{n}=2\sqrt{n}\left(\sin^{-1}{\sqrt{\overline{X}}}-\sin^{-1}{\sqrt{\theta}}\right)\to\N(0,1)
	\end{equation*}
	Let $g(x)=\sin^{-1}{\sqrt{x}}$. We may obtain that:
	\begin{equation*}
		g'(x)=\frac{1}{2\sqrt{x}\sqrt{1-x}}
	\end{equation*}
	We can find that the derivative is well-defined and non-zero for $0<\theta<1$ by substituting $x=\theta$. Note that $\E(X_{i})=\theta$ and $\Var(X_{i})=\theta(1-\theta)$ for $i=1,\cdots,n$. By Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables},
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\theta))\to\N\left(0,\frac{1}{4}\right)
	\end{equation*}
	Since $Z_{n}=2\sqrt{n}(g(\overline{X})-g(\theta))$, we may find that as $n\to\infty$,
	\begin{equation*}
		Z_{n}\to\N(0,1)
	\end{equation*}
\end{eg}
\begin{eg}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Exp(\theta)$ for $i=1,2,\cdots$. We want to find a variance-stabilizing transformation, which is to find a function $g(x)$ such that the limiting distribution of:
	\begin{equation*}
		Y_{n}=\sqrt{n}[g(\overline{X_{n}})-g(\theta)]
	\end{equation*}
	does not depend on $\theta$.\\
	We may find that $\E(X_{i})=\frac{1}{\theta}$ and $\Var(X_{i})=\frac{1}{\theta^{2}}$ for $i=1,2,\cdots$. We claim that $g(x)=\ln{x}$ is what we want. We can find that:
	\begin{equation*}
		g'(x)=\frac{1}{x}
	\end{equation*}
	By substituting $x=\frac{1}{\theta}$, we can see that the derivative is non-zero. Applying Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables},
	\begin{equation*}
		\sqrt{n}\left(g(\overline{X})-g\left(\frac{1}{\theta}\right)\right)\to\N(0,1)
	\end{equation*}
	Therefore, $g(x)=\ln{x}$ is the variance-stabilizing transformation.
\end{eg}

\newpage
However, we usually deal with more than 1 variables. Before we extend the theorems into multivariate case, we must first introduce the multivariate normal distribution.
\begin{eg}\named{Multivariate Normal Distribution} $\mathbf{X}\sim\N_{k}(\boldsymbol{\mu},\mathbf{\Sigma})$\\
	Given a random vector $\mathbf{X}$. Let the $k\times 1$ vector $\boldsymbol{\mu}$ be the expected value of $\mathbf{X}$ and the $k\times k$ matrix $\mathbf{\Sigma}$ be its variance-covariance matrix. Assume that $\mathbf{\Sigma}$ is positive-definite. (For all non-zero vectors $\mathbf{z}$ with real entries, we have $\mathbf{z}^{T}\mathbf{\Sigma z}>0$) The random vector $\mathbf{X}$ is $k$-dimensional normal if its PDF is:
	\begin{equation*}
		f_{\mathbf{X}}(\mathbf{x})=(2\pi)^{-\frac{k}{2}}\abs{\mathbf{\Sigma}}^{-\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}
	\end{equation*}
\end{eg}
\begin{rem}
	The i-th row and j-th column of the $k\times k$ variance-covariance matrix $\mathbf{\Sigma}$ is the element $a_{ij}$ found by:
	\begin{equation*}
		a_{ij}=\cov(X_{i},X_{j})
	\end{equation*}
	Note that if $i=j$, then $\cov(X_{i},X_{i})=\Var(X_{i})$.
\end{rem}
\begin{eg}
	If $k=2$, then $X\sim\N_{2}(\boldsymbol{\mu},\mathbf{\Sigma})$ is bivariate normal.
\end{eg}
\begin{lem}
	\label{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}
	If $\mathbf{X}\sim\N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$, then for any $q\times p$ matrix $\mathbf{A}$, we have:
	\begin{equation*}
		\mathbf{AX}\sim\N_{q}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\Sigma A}^{T})
	\end{equation*}
\end{lem}
\begin{eg}
	Using this lemma, one can isolate some of the random variables that made up the random vector $\mathbf{X}=(X_{1}\ \cdots\ X_{p})^{T}\sim\N_{p}(\boldsymbol{\mu}_{1},\mathbf{\Sigma}_{1})$. For example, setting $(p-1)\times p$ matrix $\mathbf{A}$ as:
	\begin{equation*}
		\mathbf{A}=\begin{pmatrix}
			0 & 1 & 0 & \hdots & 0\\
			\vdots & \ddots & \ddots & \ddots & \vdots\\
			\vdots & & \ddots & \ddots & 0\\
			0 & \hdots & \hdots & 0 & 1
		\end{pmatrix}=\begin{pmatrix}\begin{array}{c|c}
			0 & \\
			\vdots & \mathbf{I}_{(p-1)\times(p-1)}\\
			0 &
		\end{array}\end{pmatrix}
	\end{equation*}
	We may find that:
	\begin{equation*}
		\mathbf{AX}=\begin{pmatrix}
		X_{2}\\
		\vdots\\
		X_{p}
		\end{pmatrix}\sim\N_{p-1}(\boldsymbol{\mu}_{2},\mathbf{\Sigma}_{2})
	\end{equation*}
	where $\boldsymbol{\mu}_{2}$ is the mean vector of $(X_{2}\ \cdots\ X_{p})^{T}$ and $\mathbf{\Sigma}_{2}$ is the variance-covariance matrix of $(X_{2}\ \cdots\ X_{p})^{T}$.
\end{eg}
\begin{lem}
	\label{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}
	If we have:
	\begin{equation*}
		\begin{pmatrix}X_{1}\\X_{2}\end{pmatrix}\sim\N_{2}\left(\begin{pmatrix}\mu_{1}\\\mu_{2}\end{pmatrix},\begin{pmatrix}\sigma_{11}&\sigma_{12}\\\sigma_{21}&\sigma_{22}\end{pmatrix}\right),
	\end{equation*}
	then $X_{1}$ and $X_{2}$ are independent if and only if $\sigma_{12}=\sigma_{21}=0$.
\end{lem}
\begin{proofing}
	From the properties of covariance,
	\begin{equation*}
		\sigma_{12}=\cov(X_{1},X_{2})=\cov(X_{2},X_{1})=\sigma_{21}
	\end{equation*}
	Assume that $X_{1}$ and $X_{2}$ are independent. We have:
	\begin{equation*}
		\cov(X_{1},X_{2})=\E((X_{1}-\E(X_{1}))(X_{2}-\E(X_{2})))=\E(X_{1}X_{2})-\E(X_{1})\E(X_{2})=\E(X_{1})\E(X_{2})-\E(X_{1})\E(X_{2})=0
	\end{equation*}
	Therefore, $\sigma_{12}=\sigma_{21}=0$.\\
	Assume that $\sigma_{12}=\sigma_{21}=0$. Then:
	\begin{equation*}
		\E(X_{1}X_{2})=\E(X_{1})\E(X_{2})
	\end{equation*}
	Therefore, $X_{1}$ and $X_{2}$ are uncorrelated. Since $X_{1}$ and $X_{2}$ are also bivariate normal, they are independent.
\end{proofing}
\begin{rem}
	Two random variables are uncorrelated does not mean they are independent. It is only true if they are bivariate normal.
\end{rem}

\newpage
We may extend the CLT to multivariate case.
\begin{thm}\named{Multivariate Central Limit Theorem}
	Let $\{\mathbf{X}_{n}=(X_{n1}\ \cdots\ X_{nk})^{T}\in\mathbb{R}^{k}\}$ be a sequence of i.i.d. random vectors with variance-covariance matrix $\mathbf{\Sigma}$. We assume that $\E(X_{ij}^{2})<\infty$ for $i=1,2,\cdots$ and $j=1,\cdots,k$. Define by $\mathbf{\overline{X}}$ the sample mean of the random vectors. Then as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\mathbf{\overline{X}}-\boldsymbol{\mu})\xrightarrow{D}\N_{k}(\mathbf{0},\mathbf{\Sigma})
	\end{equation*}
\end{thm}
We may extend the Delta method to multivariate cases.
\begin{thm}\named{Multivariate 1st-order Delta Method}
	Let $\{\mathbf{X}_{n}\in\mathbb{R}^{k}\}$ be a sequence of random vectors such that for constant vector $\mathbf{a}\in\mathbb{R}^{k}$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\mathbf{X}_{n}-\mathbf{a})\xrightarrow{D}\mathbf{U}
	\end{equation*}
	where $\mathbf{U}$ is a random vector in $\mathbb{R}^{k}$. If a function $h:\mathbb{R}^{k}\to\mathbb{R}$ has a derivative $\nabla h(\mathbf{a})\neq\mathbf{0}$, then as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(h(\mathbf{X}_{n})-h(\mathbf{a}))\xrightarrow{D}\nabla h(\mathbf{a})\mathbf{U}
	\end{equation*}
	where
	\begin{equation*}
		\nabla h=\left(\pdv*[order=1]{h(t_{1},\cdots,t_{k})}{t_{1}},\cdots,\pdv*[order=1]{h(t_{1},\cdots,t_{k})}{t_{k}}\right)
	\end{equation*}
\end{thm}

\section{Commonly used distribution}
Let us recall some useful distributions.
\begin{eg}\named{Binomial distribution} $X\sim\Bin(n,p)$\\
	Random variable $X$ is a Binomial random variable with parameters $n\in\mathbb{N}$ and $p\in[0,1]$ if it has a PMF for $x=0,\cdots,n$:
	\begin{align*}
		p_{X}(x)&=\binom{n}{x}p^{x}(1-p)^{n-x} & \E(X)&=np & \Var(X)&=np(1-p)
	\end{align*}
\end{eg}
\begin{eg}\named{Geometric distribution} $X\sim\Geom(p)$\\
	Random variable $X$ is geometric with parameter $p\in[0,1]$ if it has a PMF for $x=1,2,\cdots$:
	\begin{align*}
		p_{X}(x)&=p(1-p)^{x-1} & \E(X)&=\frac{1}{p} & \Var(X)&=\frac{1-p}{p^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Poisson distribution} $X\sim\Poisson(\lambda)$\\
	Random variable $X$ is a Poisson random variable with parameter $\lambda$ if it has a PMF for $x=0,1,\cdots$:
	\begin{align*}
		p_{X}(x)&=\frac{\lambda^{x}}{x!}e^{-\lambda} & \E(X)&=\lambda & \Var(X)&=\lambda
	\end{align*}
\end{eg}
\begin{eg}\named{Negative Binomial distribution} $X\sim\NBin(r,p)$\\
	Assume that $X_{1},\cdots,X_{r}$ are independent and $X_{i}\sim\Geom(p)$ for $i=1,\cdots,r$. Let $Y=\sum_{i=1}^{n}X_{i}$. Random variable $Y$ is negative Binomial with parameters $r>0$ and $p\in[0,1]$ if for $x>r$:
	\begin{align*}
		p_{X}(x)&=\binom{x-1}{r-1}(1-p)^{x-r}p^{r} & \E(X)&=\frac{r}{p} & \Var(X)&=\frac{r(1-p)}{p^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Cauchy distribution} $X\sim\Cauchy(\theta)$\\
	Random variable $X$ is a Cauchy random variable with parameter $\theta$ if it has a PDF:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\pi(1+(x-\theta)^{2})} & \E(X)&\text{ DNE} & \Var(X)&\text{ DNE}
	\end{align*}
\end{eg}

\newpage
\begin{eg}\named{Uniform distribution} $X\sim\U[a,b]$\\
	Random variable $X$ is uniform if given $a<b$, it has a PDF:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{b-a}, &x\in[a,b]\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{a+b}{2} & \Var(X)&=\frac{(b-a)^{2}}{12}
	\end{align*}
\end{eg}
\begin{eg}\named{Exponential distribution} $X\sim\Exp(\lambda)$\\
	Random variable $X$ is exponential with parameter $\lambda$ if it has a PDF:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\lambda e^{-\lambda x}, &x\geq 0\\
			0, &x<0
		\end{cases} & \E(X)&=\frac{1}{\lambda} & \Var(X)&=\frac{1}{\lambda^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Normal distribution / Gaussian distribution} $X\sim\N(\mu,\sigma^{2})$\\
	Random variable $X$ is normal if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF is:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right) & F_{X}(x)&=\intlu{-\infty}{x}f_{X}(u)\,du & \E(X)&=\mu & \Var(X)&=\sigma^{2}
	\end{align*}
	Random variable $X$ is standard normal if $\mu=0$ and $\sigma^{2}=1$. ($X\sim\N(0,1)$)
	\begin{align*}
		f_{X}(x)&=\phi(x)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^{2}}{2}\right) & F_{X}(x)&=\Phi(x)=\intlu{-\infty}{x}\phi(u)\,du & \E(X)&=0 & \Var(X)&=1
	\end{align*}
\end{eg}
\begin{eg}\named{Gamma distribution} $X\sim\Gam(\alpha,\beta)$\\
	Random variable $X$ is a gamma random variable with parameters $\alpha>0$ and $\beta>0$ if its PDF is:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{\Gamma(\alpha)}\beta^{\alpha}x^{\alpha-1}e^{-\beta x}, &x\geq 0\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{\alpha}{\beta} & \Var(X)&=\frac{\alpha}{\beta^{2}}
	\end{align*}
\end{eg}
\begin{rem}
	Gamma function $\Gamma(z)$ has the following properties:
	\begin{enumerate}
		\item If $z$ is a positive integer, then $\Gamma(z)=(z-1)!$.
		\item For all $z$, $\Gamma(z+1)=z\Gamma(z)$.
		\item If $\Re(z)>0$, then $\Gamma(z)=\int_{0}^{\infty}t^{z-1}e^{-t}\,dt$.
		\item $\Gamma(\frac{1}{2})=\sqrt{\pi}$.
	\end{enumerate}
\end{rem}
\begin{eg}\named{Beta distribution} $X\sim\Bet(\alpha,\beta)$\\
	Random variable $X$ is a beta random variable with parameters $\alpha>0$ and $\beta>0$ if its PDF is: 
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}, &x\in(0,1)\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{\alpha}{\alpha+\beta} & \Var(X)&=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
	\end{align*}
\end{eg}
\begin{rem}
	Beta function $B(z_{1},z_{2})$ has the following properties:
	\begin{enumerate}
		\item $B(z_{1},z_{2})=\frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1}+z_{2})}$
		\item $B(z_{1},z_{2})=\int_{0}^{1}t^{z_{1}-1}(1-t)^{z_{2}-1}\,dt$
	\end{enumerate}
\end{rem}

\newpage
We have some more distributions that is associated with normal distribution. For example, Chi-squared distribution, which is a special case of gamma distribution.
\begin{eg}\named{Chi-squared distribution} $Y\sim\chi^{2}(n)$\\
	Assume that $X_{1},X_{2},\cdots,X_{n}$ are independent and $X_{i}\sim\N(0,1)$ for $i=1,\cdots,n$. Let $Y=\sum_{i=1}^{n}X_{i}^{2}$. Random variable $Y$ has a $\chi^{2}$-distribution with $n$ degree of freedom if:
	\begin{align*}
		f_{Y}(x)&=\begin{cases}
			\frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x>0\\
			0, \text{Otherwise}
		\end{cases} & \E{Y}&=n & \Var(Y)&=2n
	\end{align*}
\end{eg} 
\begin{thm}
	\label{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}
	If the random variable $X\sim\N(\mu,\sigma^{2})$, where $\sigma^{2}>0$, then the random variable $V=\frac{(X-\mu)^{2}}{\sigma^{2}}\sim\chi^{2}(1)$.
\end{thm}
\begin{proofing}
	By the properties of normal distribution, we get that:
	\begin{equation*}
		\frac{X-\mu}{\sigma}\sim\N(0,1)
	\end{equation*}
	Therefore, by the definition of chi-squared distribution,
	\begin{equation*}
		V=\left(\frac{X-\mu}{\sigma}\right)^{2}\sim\chi^{2}(1)
	\end{equation*}
\end{proofing}
\begin{eg}
	Given $Y\sim\chi^{2}(r)$. How do we find the MGF of $Y$?\\
	We can find that chi-squared distribution is actually a special case of gamma distribution. We have $\chi^{2}(r)=\Gamma(\frac{r}{2},\frac{1}{2})$. Therefore, by substituting, we may get:
	\begin{equation*}
		M_{Y}(t)=\left(\frac{\frac{1}{2}}{\frac{1}{2}-t}\right)^{\frac{r}{2}}=(1-2t)^{-\frac{r}{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	Given that $Y\sim\chi^{2}(r)$. How do we find $\E(Y)$ without using the MGF of $Y$?\\
	By definition, we may let $Y=\sum_{i=1}^{r}X_{i}^{2}$, where $X_{i}\sim\N(0,1)$. Therefore,
	\begin{equation*}
		\E(Y)=\sum_{i=1}^{r}\E(X_{i}^{2})=\sum_{i=1}^{r}\left.\odv*[order={2}]{}{t}e^{\frac{1}{2}t^{2}}\right|_{t=0}=\left.r(1+t^{2})e^{\frac{1}{2}t^{2}}\right|_{t=0}=r
	\end{equation*}
\end{eg}
\begin{thm}
	Given a set of random variables $\{X_{1},\cdots,X_{k}\}$. Let $Y=\sum_{i=1}^{k}X_{i}$ and $X_{i}\sim\chi^{2}(r_{i})$ for all $i=1,\cdots,k$. If they are independent, then $Y\sim\chi^{2}(r_{1}+\cdots+r_{k})$.
\end{thm}
\begin{proofing}
	It suffices to prove the relation for two random variables $Z_{1}$ and $Z_{2}$. If $Z_{1}\sim\chi^{2}(n_{1})$ and $Z_{2}\sim\chi^{2}(n_{2})$, then $Z_{1}+Z_{2}\sim\chi^{2}(n_{1}+n_{2})$. By repeating applying the relation for two random variables, one can easily have the desired relation for $k$ random variables.\\
	From definition, $Z_{1}=X_{11}^{2}+\cdots+X_{1n_{1}}^{2}$ and $Z_{2}=X_{21}^{2}+\cdots+X_{2n_{2}}^{2}$, where $X_{1i}\sim\N(0,1)$ and $X_{2j}\sim\N(0,1)$ for $i=1,\cdots,n_{1}$ and $j=1,\cdots,n_{2}$. Therefore,
	\begin{equation*}
		Z_{1}+Z_{2}=X_{11}^{2}+\cdots+X_{1n_{1}}^{2}+X_{21}^{2}+\cdots+X_{2n_{2}}^{2}\sim\chi^{2}(n_{1}+n_{2})
	\end{equation*}
\end{proofing}

\newpage
\begin{thm}
	\label{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}
	If $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n>1$ of a random variable $X\sim\N(\mu,\sigma^{2})$, then we have:
	\begin{enumerate}
		\item Sample mean $\overline{X}\sim\N(\mu,\frac{\sigma^{2}}{n})$
		\item Sample mean $\overline{X}$ and sample variance $S_{n-1}^{2}$ are independent.
		\item 
		\begin{equation*}
			\frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}=\frac{nS_{n}^{2}}{\sigma^{2}}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item From definition,
		\begin{equation*}
			\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}
		\end{equation*}
		Since $X_{i}\sim\N(\mu,\sigma^{2})$ for $i=1,\cdots,n$, we can find that $\overline{X}\sim\N(\mu,\frac{\sigma^{2}}{n})$.
		\item Let $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$. We may find that:
		\begin{align*}
			\begin{pmatrix}
				\overline{X}\\ X_{1}-\overline{X}\\ X_{2}-\overline{X}\\ \vdots\\ X_{n}-\overline{X}
			\end{pmatrix}=\begin{pmatrix}
				\frac{1}{n}X_{1}+\frac{1}{n}X_{2}+\cdots+\frac{1}{n}X_{n}\\
				\left(1-\frac{1}{n}\right)X_{1}-\frac{1}{n}X_{2}-\cdots-\frac{1}{n}X_{n}\\
				-\frac{1}{n}X_{1}+\left(1-\frac{1}{n}\right)X_{2}-\cdots-\frac{1}{n}X_{n}\\
				\vdots\\
				-\frac{1}{n}X_{1}-\frac{1}{n}X_{2}-\cdots+\left(1-\frac{1}{n}\right)X_{n}
			\end{pmatrix}&=\mathbf{AX} & \mathbf{A}&=\begin{pmatrix}
				\frac{1}{n} & \frac{1}{n} & \hdots & \frac{1}{n}\\
				1-\frac{1}{n} & -\frac{1}{n} & \hdots & -\frac{1}{n}\\
				-\frac{1}{n} & 1-\frac{1}{n} & \ddots & \vdots\\
				\vdots & \ddots & \ddots & -\frac{1}{n}\\
				-\frac{1}{n} & \hdots & -\frac{1}{n} & 1-\frac{1}{n}
			\end{pmatrix}
		\end{align*}
		By Lemma \ref{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}, we have $\mathbf{AX}\sim\N_{n+1}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\sigma^{2}I_{n\times n}A}^{T})$, where $\boldsymbol{\mu}=(\mu\ \cdots\ \mu)^{T}$.\\
		Let $\mathbf{X^{*}}=(X_{1}-\overline{X}\ \cdots\ X_{n}-\overline{X})^{T}$ and $\mathbf{\Sigma^{*}}$ be the variance-covariance matrix of $\mathbf{\widetilde{X}}$. We may notice that:
		\begin{equation*}
			\mathbf{A\sigma^{2}I_{n\times n}A}^{T}=\begin{pmatrix}\begin{array}{c|c}
				\Var(\overline{X}) & \cov(\mathbf{X^{*}},\overline{X})\\
				\hline
				\cov(\mathbf{X^{*}},\overline{X}) & \mathbf{\Sigma^{*}}
			\end{array}\end{pmatrix}
		\end{equation*}
		We prove that $\cov(X_{i}-\overline{X},\overline{X})=0$ for $i=1,\cdots,n$. Since $X_{i}$ are independent for all $i$,
		\begin{equation*}
			\cov(X_{i}-\overline{X},\overline{X})=\cov(X_{i},\overline{X})-\Var(\overline{X})=\frac{1}{n}\Var(X_{i})-\frac{\sigma^{2}}{n}=0
		\end{equation*}
		Therefore, we can find that $\cov(\mathbf{X^{*}},\overline{X})=0$. By Lemma \ref{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}, $\overline{X}$ and $\mathbf{X^{*}}$ are independent.\\
		Since $S_{n}^{2}$ is a function of $\mathbf{X^{*}}$, we can conclude that $\overline{X}$ and $S_{n-1}^{2}$ are independent.
		\item We have:
		\begin{equation*}
			\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu+\mu-\overline{X})^{2}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu)^{2}-\frac{n(\overline{X}-\mu)^{2}}{\sigma^{2}}=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)-\left(\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\right)^{2}
		\end{equation*}
		Let $U=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)$ and $V=\left(\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\right)^{2}$. The distribution we are finding is $U-V$.\\
		From definition, we know that $U\sim\chi^{2}(n)$. From Theorem \ref{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}, we find the $V\sim\chi^{2}(1)$. From Part 2, since functions of $\mathbf{X}^{*}$ and $\overline{X}$ are independent, 
		\begin{align*}
			M_{U}(t)&=M_{V}(t)M_{U-V}(t)\\
			(1-2t)^{-\frac{n}{2}}&=(1-2t)^{-\frac{1}{2}}M_{U-V}(t)\\
			M_{U-V}(t)&=(1-2t)^{-\frac{n-1}{2}}
		\end{align*}
		Therefore, we can conclude that:
		\begin{equation*}
			\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{rem}
	From the same proof of above theorem part 2, we can also find that $\overline{X}$ and $S_{n}^{2}$ are independent.
\end{rem}
\begin{eg}\named{Student's t-distribution} $T\sim t(r)$\\
	Assume that $X\sim\N(0,1)$ and $Y\sim\chi^{2}(r)$. Let:
	\begin{equation*}
		T=\frac{X}{\sqrt{\frac{Y}{r}}}
	\end{equation*}
	Then $T$ has a t-distribution with $t$ degree of freedom and:
	\begin{align*}
		f_{T}(t)&=\frac{\Gamma\left(\frac{r+1}{2}\right)}{\sqrt{r\pi}\Gamma\left(\frac{r}{2}\right)}\left(1+\frac{t^{2}}{r}\right)^{-\frac{r+1}{2}} & \E(T)&=\begin{cases}
			\text{Undefined}, &r\leq 1\\
			0, &n>1\\
		\end{cases} & \Var(T)&=\begin{cases}
			\text{Undefined}, &r\leq 1\\
			\infty, &1<r\leq 2\\
			\frac{r}{r-2}, &r>2
		\end{cases}
	\end{align*}
\end{eg}
\begin{rem}
	As $r\to\infty$, $T\to\N(0,1)$ by CLT.
\end{rem}
\begin{rem}
	If we fix $Y=y$, then we can find that $T\sim\N(0,\frac{r}{y})$.
\end{rem}
The t-distribution has the following properties.
\begin{thm}
	If $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n>1$ of random variable $X\sim\N(\mu,\sigma^{2})$, then we have:
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}\sim t(n-1)
	\end{equation*}
\end{thm}
\begin{proofing}
	From Theorem \ref{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}, $\overline{X}$ and $S_{n-1}^{2}$ are independent, and:
	\begin{align*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}&\sim\N(0,1) & \frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}\sim\chi^{2}(n-1)
	\end{align*}
	Therefore, from definition,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}=\frac{\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}}{\sqrt{\frac{1}{n-1}\left(\frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}\right)}}\sim t(n-1)
	\end{equation*}
\end{proofing}
\begin{eg}
	Assume that we want to find the $95\%$ confidence interval of $\mu$ without knowing the population variance $\sigma^{2}$. Then we can find:
	\begin{align*}
		0.95&=\prob\left(-t_{0.025,n-1}\leq\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}\leq t_{0.025,n-1}\right)\\
		&=\prob\left(\overline{X}-t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\leq\mu\leq\overline{X}+t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\right)
	\end{align*}
	Therefore, we can find the $95\%$ confidence interval:
	\begin{equation*}
		\left(\overline{X}-t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}},\overline{X}+t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\right)
	\end{equation*}
	Usually when $n>30$, $t_{0.025,n-1}\approx z_{0.025}$. Therefore, we may find the $95\%$ confidence interval:
	\begin{equation*}
		\left(\overline{X}-z_{0.025}\frac{S_{n-1}}{\sqrt{n}},\overline{X}+z_{0.025}\frac{S_{n-1}}{\sqrt{n}}\right)
	\end{equation*}
\end{eg}

\newpage
\begin{eg}\named{F distribution} $F\sim F(r_{1},r_{2})$\\
	Assume that $X$ and $Y$ are independent random variables with $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Let:
	\begin{equation*}
		F=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
	\end{equation*}
	Then $F$ has a F-distribution with $r_{1}$ and $r_{2}$ degrees of freedom with:
	\begin{equation*}
		f_{F}(w)=\frac{\Gamma\left(\frac{r_{1}+r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right)\Gamma\left(\frac{r_{2}}{2}\right)}\left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}}w^{\frac{r_{1}}{2}-1}\left(1+\frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1}+r_{2}}{2}}
	\end{equation*}
	where $0<w<\infty$. We define $F_{\alpha}(r_{1},r_{2})$ by:
	\begin{equation*}
		\prob(F\geq F_{\alpha}(r_{1},r_{2}))=\alpha
	\end{equation*}
\end{eg}
\begin{lem}
	Let $U\sim F(r_{1},r_{2})$. The F-distribution has the following properties:
	\begin{enumerate}
		\item $\frac{1}{U}\sim F(r_{2},r_{1})$
		\item If $F_{\alpha}(r_{1},r_{2})$ is defined by $\prob(U\geq F_{\alpha}(r_{1},r_{2}))=\alpha$, then:
		\begin{equation*}
			\frac{1}{F_{\alpha}(r_{1},r_{2})}=F_{1-\alpha}(r_{2},r_{1})
		\end{equation*}
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item By definition,
		\begin{equation*}
			U=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
		\end{equation*}
		where $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Therefore,
		\begin{equation*}
			\frac{1}{U}=\frac{\frac{Y}{r_{2}}}{\frac{X}{r_{1}}}\sim F(r_{2},r_{1})
		\end{equation*}
		\item With $\prob(U\geq F_{\alpha}(r_{1},r_{2}))=\alpha$, since $f_{U}(w)$ is only defined for $w>0$, we can get that:
		\begin{align*}
			\prob\left(\frac{1}{U}\leq\frac{1}{F_{\alpha}(r_{1},r_{2})}\right)&=\alpha\\
			\prob\left(\frac{1}{U}>\frac{1}{F_{\alpha}(r_{1},r_{2})}\right)&=1-\alpha
		\end{align*}
		From Part 1, we find that $\frac{1}{U}\sim F(r_{2},r_{1})$. Therefore,
		\begin{equation*}
			\prob\left(\frac{1}{U}\geq F_{1-\alpha}(r_{2},r_{1})\right)=1-\alpha
		\end{equation*}
		We find that $\frac{1}{F_{\alpha}(r_{1},r_{2})}=F_{1-\alpha}(r_{2},r_{1})$
	\end{enumerate}
\end{proofing}

\newpage
\begin{eg}
	Assume that we try to compare two populations. Let $X_{1}\sim\N(\mu_{1},\sigma_{1}^{2})$ represents the random variable of the first population and $X_{2}\sim\N(\mu_{2},\sigma_{2}^{2})$ represents the random variable of the second population. We want to find an interval guess of their ratio of variance $\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}$.\\
	Let $\{X_{11},\cdots,X_{1n}\}$ be a random sample of size $n$ from $X_{1}$ and $\{X_{21},\cdots,X_{2m}\}$ be a random sample of size $m$ from $X_{2}$. We can find that:
	\begin{align*}
		\frac{(n-1)S_{n-1,1}^{2}}{\sigma_{1}^{2}}&\sim\chi^{2}(n-1) & \frac{(m-1)S_{m-1,2}^{2}}{\sigma_{2}^{2}}&\sim\chi^{2}(m-1)
	\end{align*}
	We can also find that $S_{n-1},1$ and $S_{m-1},2$ are independent since they are from different population. Therefore, we have:
	\begin{equation*}
		\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right)=\frac{\frac{1}{m-1}\left(\frac{(m-1)S_{m-1,2}^{2}}{\sigma_{2}^{2}}\right)}{\frac{1}{n-1}\left(\frac{(n-1)S_{n-1,1}^{2}}{\sigma_{1}^{2}}\right)}\sim F(m-1,n-1)
	\end{equation*}
	Then we can find the $95\%$ confidence interval by:
	\begin{align*}
		0.95&=\prob\left(f_{0.975,m-1,n-1}\leq\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right)\leq f_{0.025,m-1,n-1}\right)\\
		&=\prob\left(\frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}}f_{0.975,m-1,n-1}\leq\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\leq \frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}}f_{0.025,m-1,n-1}\right)
	\end{align*}
\end{eg}

\chapter{Point Estimation}
In this chapter, we will study two different general approaches to estimate an unknown parameters of any given parametric distribution.\\
The basic idea of point estimation is that we use a statistic $T$, an estimate $T(\mathbf{x})$ or an estimator $T(\mathbf{X})$ to estimate the unknown parameter $g(\theta)$, where $\mathbf{x}=(x_{1}\ \cdots\ x_{n})^{T}$ is a realization of random vector $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$ with a PDF $f(x|\theta)$ or PMF $p(x|\theta)$ and $\theta$ in parameter space $\Theta$.
\begin{rem}
	Most often, the parameters of our interest to be estimated (\textbf{estimand}) is a function of the unknown distribution parameters $\theta$. E.g. $\mu^{2},\frac{\sigma}{\mu}$.
\end{rem}
\begin{rem}
	We only estimate an unknown parameters. There is no point to estimate an already known parameters.
\end{rem}
\section{Methods of Moments Estimation}
Methods of moments estimation is one of the most popular methods in statistics to estimate an unknown parameters. As the name suggests, it is related to moments. The motivation is that in some situations, the parameter of interest can be written as a function of population moments about $0$.
\begin{defn}
	Suppose that there are $k$ unknown parameters $\theta_{1},\cdots,\theta_{k}$. If we can write them in terms of $k$ or more moments, i.e.:
	\begin{equation*}
		\begin{cases}
			\theta_{1}=g_{1}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)\\
			\theta_{2}=g_{2}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)\\
			\vdots\\
			\theta_{k}=g_{k}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)
		\end{cases}
	\end{equation*}
	then the \textbf{method of moments estimator} (MME), denoted by $(\widetilde{\theta}_{1},\widetilde{\theta}_{2},\cdots,\widetilde{\theta}_{k})$, of $(\theta_{1},\theta_{2},\cdots,\theta_{k})$ is:
	\begin{equation*}
		\begin{cases}
			\widetilde{\theta}_{1}=g_{1}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)\\
			\widetilde{\theta}_{2}=g_{2}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)\\
			\vdots\\
			\widetilde{\theta}_{k}=g_{k}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	This estimation is quick and easy, but the MME obtained are often biased and heavily relies on the existence of the required population moments.
\end{rem}
\begin{rem}
	Do not mix up method of moment estimator or method of moment estimate.
\end{rem}
\begin{rem}
	Do not write the MME as $(\theta_{1},\theta_{2},\cdots,\theta_{k})$. This is wrong.
\end{rem}

\newpage
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\N(10,\sigma^{2})$. We want to estimate $\sigma^{2}$.\\
	We have $k=1$, $\theta_{1}=\sigma^{2}$. We can write it in terms of moments:
	\begin{equation*}
		\sigma^{2}=\E(X^{2})-100
	\end{equation*}
	Therefore, the MME of $\sigma^{2}$ is:
	\begin{equation*}
		\widetilde{\sigma}^{2}=\overline{X^{2}}-100
	\end{equation*}
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\N(\mu,\sigma^{2})$. We want to estimate $\mu$ and $\sigma^{2}$.\\
	We have $k=2$, $(\theta_{1},\theta_{2})=(\mu,\sigma^{2})$. We can write them in terms of moments: 
	\begin{equation*}
		\begin{cases}
			\mu=E(X)\\
			\sigma^{2}=\E(X^{2})-[\E(X)]^{2}
		\end{cases}
	\end{equation*}
	Therefore, the MME of $\mu$ and $\sigma^{2}$ are:
	\begin{equation*}
		\begin{cases}
			\widetilde{\mu}=\overline{X}\\
			\widetilde{\sigma}^{2}=\overline{X^{2}}-(\overline{X})^{2}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}
		\end{cases}
	\end{equation*}
\end{eg}
\begin{rem}
	MME may not be unique because the parameter can be written as different functions of moments. To fix this problem, we usually prefer using fewer or lower moments to get MME.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Poisson(\lambda)$. We want to estimate $\lambda$. We have $k=1$, $\theta_{1}=\lambda$. We have multiple ways to write it in terms of moments. It can be $\lambda=\E(X)$, $\lambda=\E(X^{2})-[\E(X)]^{2}$ or any other combinations. From the remark, we would choose the one with fewer or lower moments, which is:
	\begin{equation*}
		\lambda=\E(X)
	\end{equation*}
	Therefore, the MME of $\lambda$ is:
	\begin{equation*}
		\lambda=\overline{X}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Gam(\alpha,\beta)$. Assume that we know that $\E(X)=3423$. We have $k=2$, $(\theta_{1},\theta_{2})=(\alpha,\beta)$. We can write them in terms of moments:
	\begin{equation*}
		\begin{cases}
			3423=\frac{\alpha}{\beta}\\
			\E(X^{2})=\frac{\alpha}{\beta^{2}}+3423^{2}
		\end{cases}\implies\begin{cases}
			\alpha=\frac{3423^{2}}{\E(X^{2})-3423^{2}}\\
			\beta=\frac{3423}{\E(X^{2})-3423^{2}}
		\end{cases}
	\end{equation*}
	Therefore, the MME of $\alpha$ and $\beta$ is:
	\begin{equation*}
		\begin{cases}
			\widetilde{\alpha}=\frac{3423^{2}}{\overline{X^{2}}-3423^{2}}\\
			\widetilde{\beta}=\frac{3423}{\overline{X^{2}}-3423^{2}}
		\end{cases}
	\end{equation*}
\end{eg}
\begin{lem}\named{Invariance property of MME}
	If $\widetilde{\theta}_{i}$ is the MME for $\theta_{i}$ for $i=1,\cdots,k$, then $h(\widetilde{\theta}_{1},\cdots,\widetilde{\theta}_{k})$ is the MME for $h(\theta_{1},\cdots,\theta_{k})$, where $h$ is a known function.
\end{lem}
\begin{thm}
	\label{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}
	A sequence of MME $\{\widetilde{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent and asymptotically unbiased for $\theta$. It is also asymptotically normally distributed. More precisely, under certain assumption like $\E\abs{X}^{2k}<\infty$, as $n\to\infty$, we have:
	\begin{equation*}
		\sqrt{n}(\widetilde{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathbf{GHG}^{T})
	\end{equation*}
	where $\mathbf{G}$ is a $k\times k$ matrix with $\pdv{g_{i}}{\mu_{j}'}$ as its $(i,j)$-th entry and $\mathbf{H}$ is a $k\times k$ matrix with $\mu_{i+j}'-\mu_{i}'\mu_{j}'$ as its $(i,j)$-th entry, for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{rem}
	In the theorem, "consistent" means convergence in probability. For any $\varepsilon>0$, as $n\to\infty$,
	\begin{equation*}
		\prob(|\widetilde{\theta}_{n}-\theta|>\varepsilon)\to 0
	\end{equation*}
\end{rem}

\newpage
\begin{rem}
	Also in the theorem, "asymptotically unbiased" means that we have:
	\begin{equation*}
		\lim_{n\to\infty}\E(\widetilde{\theta}_{n})=\theta
	\end{equation*}
	Note that $\E(\widetilde{\theta}_{n})\neq\theta$ for some $n$.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from a random variable $X$ with $\E\abs{X}^{4}<\infty$. We take:
	\begin{equation*}
		\theta=\begin{pmatrix}
			\mu\\ \sigma^{2}
		\end{pmatrix}=\begin{pmatrix}
			\mu_{1}'\\ \mu_{2}'-(\mu_{1}')^{2}
		\end{pmatrix}
	\end{equation*}
	We have:
	\begin{align*}
		\mathbf{G}&=\begin{pmatrix}
			1 & 0\\
			-2\mu_{1}' & 1
		\end{pmatrix} & \mathbf{H}&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
			\mu_{3}'-\mu_{2}'\mu_{1}' & \mu_{4}'-(\mu_{2}')^{2}
		\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\mathbf{GHG}^{T}&=\begin{pmatrix}
			1 & 0\\
			-2\mu_{1}' & 1
		\end{pmatrix}\begin{pmatrix}
		\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
		\mu_{3}'-\mu_{2}'\mu_{1}' & \mu_{4}'-(\mu_{2}')^{2}
		\end{pmatrix}\mathbf{G}^{T}\\
		&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
			\mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3} & \mu_{4}'-2\mu_{1}'\mu_{3}'-(\mu_{2}')^{2}+2(\mu_{1}')^{2}\mu_{2}'
		\end{pmatrix}\begin{pmatrix}
			1 & -2\mu_{1}'\\
			0 & 1
		\end{pmatrix}\\
		&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3}\\
			\mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3} & \mu_{4}'-4\mu_{1}'\mu_{3}'-(\mu_{2}')^{2}+8\mu_{2}'(\mu_{1}')^{2}-4(\mu_{1}')^{4}
		\end{pmatrix}
	\end{align*}
	Using the fact that:
	\begin{align*}
		\mu_{3}&=\mu_{3}'-3\mu_{2}'\mu_{1}'+2(\mu_{1}')^{3}\\ \mu_{4}&=\mu_{4}'-4\mu_{3}'\mu_{1}'+6\mu_{2}'(\mu_{1}')^{2}-3(\mu_{1}')^{4}\\
		\sigma^{4}&=(\mu_{2}')^{2}-2\mu_{2}'(\mu_{1}')^{2}+(\mu_{1}')^{4}
	\end{align*}
	We can find the resultant matrix:
	\begin{equation*}
		\mathbf{GHG}^{T}=\begin{pmatrix}
			\sigma^{2} & \mu_{3}\\
			\mu_{3} & \mu_{4}-\sigma^{4}
		\end{pmatrix}
	\end{equation*}
	Using Theorem \ref{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}, denote:
	\begin{equation*}
		\widetilde{\theta}_{n}=\begin{pmatrix}
			\overline{X}_{n}\\ S_{n}^{2}
		\end{pmatrix}
	\end{equation*}
	As $n\to\infty$,
	\begin{equation*}
		\sqrt{n}\left(\begin{pmatrix}
			\overline{X}_{n}\\ S_{n}^{2}
		\end{pmatrix}-\begin{pmatrix}
			\mu\\ \sigma^{2}
		\end{pmatrix}\right)\to\N_{2}\left(\begin{pmatrix}
			0\\ 0
		\end{pmatrix},\begin{pmatrix}
			\sigma^{2} & \mu_{3}\\
			\mu_{3} & \mu_{4}-\sigma^{4}
		\end{pmatrix}\right)
	\end{equation*}
	Based on the properties of variance-covariance matrix, we may find that as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(S_{n}^{2}-\sigma^{2})\to\N(0,\mu_{4}-\sigma^{4})
	\end{equation*}
	By Delta Method, in condition that $\sigma^{2}>0$,
	\begin{equation*}
		\sqrt{n}(S_{n}-\sigma)\to\N\left(0,\frac{\mu_{4}-\sigma^{4}}{4\sigma^{2}}\right)
	\end{equation*}
\end{eg}

\newpage
\section{Maximum Likelihood Estimation}
The method of maximum likelihood is by far the most popular technique for deriving estimators, popularized by Ronald Aylmer Fisher in 1922. Currently, there are still a lot of research studying the properties of this estimation method.
\begin{defn}
	Consider a random sample of size $n$ from a population with a PDF $f(\mathbf{x}|\theta)$ or a PMF $p(\mathbf{x}|\theta)$. Given a realization $\mathbf{x}=(x_{1}\ \cdots\ x_{n})^{T}$. The \textbf{likelihood function} is defined by:
	\begin{equation*}
		L(\theta)=L(\theta_{1},\cdots,\theta_{k}|\mathbf{x})=\begin{cases}
			\prod_{i=1}^{n}f(x_{i}|\theta), &\text{Continuous case}\\
			\prod_{i=1}^{n}p(x_{i}|\theta), &\text{Discrete case}
		\end{cases}
	\end{equation*}
	The likelihood function can be used to quantify how the observed data is likely to occur.
\end{defn}
\begin{rem}
	The likelihood function $L(\theta)$ is a function of $\theta$ with fixed $\mathbf{x}$.
\end{rem}
\begin{rem}
	Do not replace $x_{i}$ with $x$.
	\begin{equation*}
		L(\theta)=\begin{cases}
			\prod_{i=1}^{n}f(x_{i}|\theta)\neq\prod_{i=1}^{n}f(x|\theta), &\text{Continuous case}\\
			\prod_{i=1}^{n}p(x_{i}|\theta)\neq\prod_{i=1}^{n}p(x|\theta), &\text{Discrete case}
		\end{cases}
	\end{equation*}
\end{rem}
The idea is that for each realization of $\mathbf{x}$, we want to estimate a value of $\theta\in\Theta$ at which $L(\theta)$ attains its maximum.
\begin{defn}
	The \textbf{maximum likelihood estimate} (MLE), denoted by $\hat{\theta}$, is obtained by:
	\begin{equation*}
		\hat{\theta}=\argmax_{\theta\in\Theta}L(\theta)
	\end{equation*}
\end{defn}
\begin{rem}
	In some cases, especially when differentiation is used, it is easier to work with \textbf{log likelihood} defined by:
	\begin{equation*}
		l(\theta)=\log{L(\theta)}
	\end{equation*}
	We can do this because $l(\theta)$ and $L(\theta)$ are strictly increasing and they have the same maxima. 
\end{rem}
\begin{eg}
	Consider a random sample of size $n=10$ from $\Bern(\theta)$, where $\theta$ is unknown. Therefore,
	\begin{equation*}
		L(\theta)=\prod_{i=1}^{n}p(x_{i}|\theta)=\theta^{n\overline{x}}(1-\theta)^{n-n\overline{x}}
	\end{equation*}
	Suppose that there are only two possible values of $\theta$. We have either $\theta=0.1$ or $\theta=0.5$.\\
	From the data observed, assumed that we have found that $\overline{x}=0.4$. Substituting gets:
	\begin{align*}
		L(0.1)&=(0.1)^{4}(0.9)^{6}=0.0000531441 & L(0.5)&=(0.5)^{4}(0.5)^{6}=0.0009765625
	\end{align*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=0.5$.
\end{eg}
\begin{eg}
	In the case when $L(\theta)$ is differentiable on the interior of $\theta$. One possible way of finding an MLE of $\theta=(\theta_{1}\ \cdots\ \theta_{k})^{T}$ is to solve the first order equation for $i=1,\cdots,k$:
	\begin{equation*}
		\pdv*{L(\theta)}{\theta_{i}}=0\text{ or }\pdv*{l(\theta)}{\theta_{i}}=0
	\end{equation*}
	and check all the extrema.
\end{eg}
\begin{rem}
	Solving the first-order likelihood only gives you the maximum at critical points. You need to also check the extreme values too.
\end{rem}

\newpage
\begin{eg}
	Consider a random sample of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. We may obtain the log likelihood:
	\begin{equation*}
		l(\theta)=\ln\left(\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_{i}-\theta)^{2}}\right)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\theta)^{2}-\frac{n}{2}\ln(2\pi)
	\end{equation*}
	We find the critical points by solving:
	\begin{equation*}
		0=\pdv*{l(\theta)}{\theta}=\sum_{i=1}^{n}(x_{i}-\theta)
	\end{equation*}
	This has the solution $\hat{\theta}=\overline{x}$. To check that the solution is in fact a global maximum, we can check that:
	\begin{equation*}
		\pdv*[order={2}]{l(\theta)}{\theta}=-n<0
	\end{equation*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=\overline{x}$.
\end{eg}
\begin{eg}
	Continues the previous example. Alternatively, we may find that for any $\theta\in\Theta$,
	\begin{equation*}
		\sum_{i=1}^{n}(x_{i}-\theta)^{2}\geq\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}
	\end{equation*}
	Thus, for any $\theta\in\Theta$,
	\begin{equation*}
		L(\theta)\leq L(\overline{x})
	\end{equation*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=\overline{x}$.
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. Previously, we have found that $\hat{\theta}=\overline{x}$, which maximize the log likelihood. Let us restrict that $\theta\geq 0$.\\
	If $\overline{x}\geq 0$, then it satisfies the constraint $\theta\geq 0$. Therefore, the MLE would be:
	\begin{equation}
		\hat{\theta}=\overline{x}
	\end{equation}
	If $\overline{x}<0$, then it does not satisfy the constraint $\mu\geq 0$. We analyse the log likelihood again:
	\begin{equation*}
		l(\theta)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\theta)^{2}-\frac{n}{2}\ln(2\pi)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}-\frac{n}{2}(\overline{x}-\theta)-\frac{n}{2}\ln(2\pi)
	\end{equation*}
	The term $(\overline{x}-\theta)^{2}$ is minimized while satisfying the constraint is when $\theta=0$.\\
	Therefore, if we restrict $\theta\geq 0$, the MLE of $\theta$ would be:
	\begin{equation*}
		\hat{\theta}=\max(\overline{x},0)
	\end{equation*}
\end{eg}
\begin{rem}
	Remember, when we estimate a parameter, we must use the data we have obtained.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $\U[0,\theta]$, where $\theta\in(0,\infty)$ is unknown. The likelihood function is:
	\begin{equation*}
		L(\theta)=\frac{1}{\theta^{n}}\mathbf{1}_{0\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq\theta}
	\end{equation*}
	where $x_{(i)}$ represents the $i$-th smallest data for $i=1,\cdots,n$. Therefore, the MLE is:
	\begin{equation*}
		\hat{\theta}=x_{(n)}
	\end{equation*}
\end{eg}
\begin{rem}
	MLE may be biased and it may not exist in $\Theta$, especially when $\Theta$ is an open set.
\end{rem}

\newpage
\begin{rem}
	MLE defined may not be unique.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $\U[\theta-1,\theta+1]$, where $\theta$ is unknown. The likelihood function is:
	\begin{equation*}
		L(\theta)=\frac{1}{2^{n}}\mathbf{1}_{\theta-1\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq\theta+1}=\frac{1}{2^{n}}\mathbf{1}_{x_{(n)}-1\leq\theta\leq x_{(1)}+1}
	\end{equation*}
	where $x_{(i)}$ represents the $i$-th smallest data for $i=1,\cdots,n$. We may fins that any estimates in $[x_{(n)}-1,x_{(1)}+1]$ maximize $L(\theta)$. Therefore, there are infinitely many MLEs of $\theta$.
\end{eg}
\begin{lem}\named{Invariance property of MLE}
	If $\hat{\theta}_{i}$ is the MLE of $\theta_{i}$ for $i=1,\cdots,k$, then $h(\hat{\theta}_{1},\cdots,\hat{\theta}_{k})$ is the MLE for $h(\theta_{1},\cdots,\theta_{k})$, where $h$ is a known function.
\end{lem}
\begin{thm}
	\label{Chapter 2 (Thoerem) Sequence of MLE is asympt. normal}
	A sequence of MLE $\{\hat{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, asymptotically efficient and asymptotically normally distributed. More precisely, under regularity assumption, as $n\to\infty$, we have:
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathcal{I}_{X}^{-1}(\theta))
	\end{equation*}
	where $\mathcal{I}_{X}(\theta)$ is known as the \textbf{Fisher Information matrix} and is a $k\times k$ matrix with the $(i,j)$-th entry defined as:
	\begin{equation*}
		\begin{cases}
			\E\left[\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Continuous case}\\
			\E\left[\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Discrete case}
		\end{cases}
	\end{equation*}
	for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{rem}
	In the theorem, "asymptotically efficient" means that the limiting variance is the smallest possible. \textit{If you want to know why reciprocal of Fisher Information is the lowest possible variance of any unbiased estimator, search Cram\'er-Rao bound.}
\end{rem}
Notice that we have used a special matrix called "Fisher Information Matrix". What is Fisher Information?
\begin{defn}
	Given a set of random variables $\{X_{1},\cdots,X_{n}\}$. The \textbf{Fisher Information}, or \textbf{Fisher Information matrix} if more than one unknown parameter is considered, of the set is defined by:
	\begin{equation*}
		\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)=\begin{cases}
			\E\left[\odv*{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}\right]^{2}, &\text{Continuous case}\\
			\E\left[\odv*{}{\theta}\ln{p_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}\right]^{2}, &\text{Discrete case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	Fisher Information is a measure of amount of information about an unknown parameter $\theta$ that a random variable or data carries. It is very important because we do want to know how to quantify this amount appropriately.
\end{rem}
\begin{eg}
	If $X\sim\N(\mu,\sigma^{2})$, where $\sigma^{2}$ is known but $\mu\in(-\infty,\infty)$ is unknown, then the Fisher Information about $\mu$ contained in $X$ is:
	\begin{equation*}
		\mathcal{I}_{X}(\mu)=\E\left[\odv*{\ln{f_{X}(X|\mu)}}{\mu}\right]^{2}=\E\left[\odv*{}{\mu}\left(-\frac{1}{2\sigma^{2}}(X-\mu)^{2}-\frac{1}{2}\ln(2\pi\sigma^{2})\right)\right]^{2}=\E\left[\frac{1}{\sigma^{2}}(X-\mu)\right]^{2}=\frac{1}{\sigma^{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	If $X\sim\Bern(p)$, where $p\in(0,1)$ is unknown, then the Fisher Information about $p$ contained in $X$ is:
	\begin{align*}
		\mathcal{I}_{X}(p)=\E\left[\odv*{\ln{f_{X}(X|p)}}{p}\right]^{2}&=\E\left[\odv*{}{p}\left(X\ln{p}+(1-X)\ln(1-p)\right)\right]^{2}\\
		&=\E\left[\frac{X}{p}-\frac{1-X}{1-p}\right]^{2}\\
		&=\E\left[\frac{X-p}{p(1-p)}\right]^{2}\\
		&=\frac{p(1-p)}{p^{2}(1-p)^{2}}=\frac{1}{p(1-p)}
	\end{align*}
\end{eg}

\newpage
We will see some properties of the Fisher Information. \textit{For simplicity, we will only discuss continuous random variables.} Notice that we used something called "regularity assumption"? The following are the regularity conditions that we need.
\begin{enumerate}
	\item $\odv*{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}}(x_{1},\cdots,x_{n}|\theta)$ exists for all $x_{1},\cdots,x_{n}$ and all $\theta\in\Theta$
	\item For any statistic $T(x_{1},\cdots,x_{n})$,
	\begin{align*}
		&\odv*{}{\theta}\int\cdots\int T(x_{1},\cdots,x_{n})f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)\,dx_{1}\,\cdots\,dx_{n}\\
		=&\int\cdots\int T(x_{1},\cdots,x_{n})\odv*{}{\theta}f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)\,dx_{1}\,\cdots\,dx_{n}
	\end{align*}
	\item $0<\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)<\infty$ for all $\theta\in\Theta$
\end{enumerate}
Condition 2 can be satisfied when the support of $X$ does not depend on $\theta$, where the support of $X$ is defined by $\{x:f_{X}(x|\theta)>0\}$. Note that $U[0,\theta]$ violates this condition.
\begin{lem}
	\label{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}
	Suppose the $X$ is a random variable with PDF $f_{X}$. Under the regularity condition, we have:
	\begin{equation*}
		\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]=0
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		0=\odv*{}{\theta}\intinfty f_{X}(x|\theta)\,dx\intinfty\odv*{}{\theta}f_{X}(x|\theta)\,dx=\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]
	\end{equation*}
\end{proofing}
\begin{rem}
	Using this lemma, we can find that:
	\begin{equation*}
		\mathcal{I}_{X}(\theta)=\Var\left(\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right)
	\end{equation*}
\end{rem}
\begin{lem}
	\label{Chapter 2 (Lemma) Relationship between expectation of l'' and I}
	Suppose that $\{X_{1},\cdots,X_{n}\}$ is a set of random variables. Under the regularity conditions and the assumption that $\odv*[order={2}]{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)}$ exists for all $x_{1},\cdots,x_{n}$ and all $\theta\in\Theta$, we have:
	\begin{equation*}
		\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}=-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]
	\end{equation*}
\end{lem}
\begin{proofing}
	From the proof of last Lemma,
	\begin{align*}
		0&=\odv*{}{\theta}\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx\\
		&=\intinfty\odv*{}{\theta}\left[\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\right]\,dx\\
		&=\intinfty\left(\odv*[order=2]{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx+\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)\odv*{}{\theta}f_{X}(x|\theta)\,dx\\
		&=\intinfty\left(\odv*[order=2]{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx+\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)^{2}f_{X}(x|\theta)\,dx\\
		&=\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]+\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}\\
		-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}
	\end{align*}
\end{proofing}

\newpage
Assume that we consider two independent random variables $X$ and $Y$. We can find the Fisher Information about $\theta$ contained in $(X,Y)$ by finding the Fisher Information about $\theta$ contained in each of them.
\begin{lem}
	If $X$ and $Y$ are independent and their PDFs satisfy the regularity conditions, then:
	\begin{equation*}
		\mathcal{I}_{X,Y}(\theta)=\mathcal{I}_{X}(\theta)+\mathcal{I}_{Y}(\theta)
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X$ and $Y$ are independent,
	\begin{align*}
		\mathcal{I}_{X,Y}(\theta)&=\E\left[\odv*{}{\theta}\ln{f_{X,Y}(X,Y|\theta)}\right]^{2}\\
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}+\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}+2\E\left[\left(\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right)\left(\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right)\right]+\E\left[\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		\tag{Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}}
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}+\E\left[\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		&=\mathcal{I}_{X}(\theta)+\mathcal{I}_{Y}(\theta)
	\end{align*}
\end{proofing}
By applying the same result to a random sample of size $n$, we can obtain the following property.
\begin{lem}
	Suppose the $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n$ from a distribution. Then,
	\begin{equation*}
		\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)=\sum_{i=1}^{n}\mathcal{I}_{X_{i}}(\theta)=n\mathcal{I}_{X_{1}}(\theta)
	\end{equation*}
\end{lem}
\begin{rem}
	For any $i\neq j$, $\mathcal{I}_{X_{i}}(\theta)=\mathcal{I}_{X_{j}}(\theta)$ only means that $X_{i}$ and $X_{j}$ carries the same amount of the information about $\theta$. It does not mean they carry identical information.
\end{rem}
\begin{eg}
	Consider a set of i.i.d. random variables $\{X_{1},\cdots,X_{n}\}$ where for all $i=1,\cdots,n$, $X_{i}\sim\Cauchy(\theta)$ and has a PDF:
	\begin{equation*}
		f_{X_{i}}(x|\theta)=\frac{1}{\pi(1+(x-\theta)^{2})}
	\end{equation*}
	We may find that:
	\begin{align*}
		\mathcal{I}_{X_{i}}(\theta)=\E\left[\odv*{}{\theta}\ln{f_{X_{i}}(X_{i}|\theta)}\right]^{2}&=\E\left(\frac{\frac{2(X_{i}-\theta)}{\pi(1+(X_{i}-\theta)^{2})^{2}}}{\frac{1}{\pi(1+(X_{i}-\theta)^{2})}}\right)^{2}\\
		&=\E\left(\frac{2(X_{i}-\theta)}{1+(X_{i}-\theta)^{2}}\right)^{2}\\
		&=\intinfty\left(\frac{2(x-\theta)}{1+(x-\theta)^{2}}\right)^{2}\frac{1}{\pi(1+(x-\theta)^{2})}\,dx\\
		\tag{$u=x-\theta$, $du=dx$}
		&=\frac{4}{\pi}\intinfty\frac{u^{2}}{(1+u^{2})^{3}}\,du\\
		&=\frac{8}{\pi}\int_{0}^{\infty}\frac{u^{2}}{(1+u^{2})^{3}}\,du\\
		\tag{$y=\frac{1}{1+u^{2}}$, $dy=-\frac{2u}{(1+u^{2})^{2}}\,du$}
		&=\frac{4}{\pi}\int_{0}^{1}\sqrt{y}\sqrt{1-y}\,dy\\
		\tag{Beta integral}
		&=\frac{4}{\pi}\int_{0}^{1}(y)^{\frac{3}{2}-1}(1-y)^{\frac{3}{2}-1}\,dy\\
		\tag{$\frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1}+z_{2})}=\int_{0}^{1}t^{z_{1}-1}(1-t)^{z_{2}-1}\,dt$}
		&=\frac{4\Gamma(\frac{3}{2})\Gamma(\frac{3}{2})}{\pi\Gamma(3)}=\frac{4(0.5\sqrt{\pi})^{2}}{2!}=\frac{1}{2}
	\end{align*}
	Therefore, $I_{X_{1},\cdots,X_{n}}(\theta)=nI_{X_{1}}(\theta)=\frac{n}{2}$.
\end{eg}

\newpage
Note that a statistic or an estimator can be considered as a function for data condensation because we condense a random sample into a lower-dimensional quantity.
\begin{lem}
	Suppose that $\mathbf{X}$ is a random vector. Under the regularity conditions, for any statistic $T(\mathbf{X})$ for $\theta$, we have:
	\begin{equation*}
		\mathcal{I}_{T(\mathbf{X})}(\theta)\leq\mathcal{I}_{\mathbf{X}}(\theta)
	\end{equation*}
\end{lem}
\begin{rem}
	The Fisher Information of $T(\mathbf{X})$ is defined by:
	\begin{equation*}
		\mathcal{I}_{T(\mathbf{X})}(\theta)=\E\left[\odv*{}{\theta}\ln{f_{T(\mathbf{X})}(T(\mathbf{X})|\theta)}\right]^{2}
	\end{equation*}
\end{rem}
We may prove Theorem \ref{Chapter 2 (Thoerem) Sequence of MLE is asympt. normal} in one-parameter case:
\begin{thm}
	Consider a random sample $\{X_{1},\cdots,X_{n}\}$ of size $n$ from a parametric distribution with a PDF $f_{X}$. Then under the regularity and some other conditions, for $\theta\in\mathbb{R}$, a sequence of MLE $\{\hat{\theta}_{n}\in\mathbb{R}\}$ allows,
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)\to\N\left(0,\frac{1}{\mathcal{I}_{X}(\theta)}\right)
	\end{equation*}
\end{thm}
\begin{proofing}
	Since the MLE $\hat{\theta}_{n}$ is the solution to $l'(\theta)=0$, we can apply a Taylor expansion of $l'(\hat{\theta}_{n})$ at $\theta$ to find that:
	\begin{align*}
		0&=l'(\theta)+l''(\theta)(\hat{\theta}_{n}-\theta)+o((\hat{\theta}_{n}-\theta))\\
		\sqrt{n}(\hat{\theta}_{n}-\theta)&=\frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)}-o((\hat{\theta}_{n}-\theta))
	\end{align*}
	We first consider the numerator. Note that $\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)},\cdots,\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}$ are i.i.d.. By CLT, we have:
	\begin{equation*}
		\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}\odv*{}{\theta}\ln{f_{X}(X_{i}|\theta)}-\E\left[\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}\right]\right)\to\N\left(0,\Var\left[\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}\right]\right)
	\end{equation*}
	By Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}, we have:
	\begin{equation*}
		\frac{1}{\sqrt{n}}l'(\theta)=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\odv*{}{\theta}\ln{f_{X}(X_{i}|\theta)}\to\N(0,\mathcal{I}_{X}(\theta))
	\end{equation*}
	Now we consider the denominator. By WLLN and Lemma \ref{Chapter 2 (Lemma) Relationship between expectation of l'' and I}, since $\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{1}|\theta)},\cdots,\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{1}|\theta)}$ are i.i.d.,
	\begin{equation*}
		-\frac{1}{n}l''(\theta)=-\frac{1}{n}\sum_{i=1}^{n}\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{i}|\theta)}\to-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}=\mathcal{I}_{X}(\theta)
	\end{equation*}
	Consequently, we have:
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)=\frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)}-o((\hat{\theta}_{n}-\theta))\to\N\left(0,\frac{1}{\mathcal{I}_{X}(\theta)}\right)
	\end{equation*}
\end{proofing}
\begin{rem}
	Sometime, $\mathcal{I}_{X}(\theta)$ cannot be determined easily. We will replace it by an observed Fisher Information defined by $-\frac{1}{n}l''(\hat{\theta}_{n})$. Since $\hat{\theta}_{n}$ is consistent for $\theta$, $-\frac{1}{n}l''(\hat{\theta}_{n})$ is also consistent for $\mathcal{I}_{X}(\theta)$ by continuous mapping theorem. Therefore,
	\begin{equation*}
		\sqrt{-l''(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)=\sqrt{n}\sqrt{-\frac{1}{n}l''(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)\to\N(0,1)
	\end{equation*}
\end{rem}

\newpage
\begin{eg}\named{Principal of Numerical solution to likelihood equations}
	Consider a random sample of size $n$ from $X\sim\Cauchy(\theta)$, similar to the previous example. We want to find the MLE of $\theta$. We have:
	\begin{equation*}
		l(\theta)=-n\ln{\pi}-\sum_{i=1}^{n}\ln(1+(x_{i}-\theta)^{2})
	\end{equation*}
	We want to find the solution of $l'(\theta)=0$, which is the MLE. Setting:
	\begin{equation*}
		\sum_{i=1}^{n}\frac{2(x_{i}-\theta)}{1+(x_{i}-\theta)^{2}}
	\end{equation*}
	However, this cannot be solved explicitly in this case.
\end{eg}
\begin{eg}\named{Newton-Raphson Algorithm}
	By Taylor expansion, we can get:
	\begin{equation*}
		0=\frac{1}{n}l'(\hat{\theta})\approx\frac{1}{n}l'(\theta)+\frac{1}{n}(\hat{\theta}-\theta)l''(\theta)
	\end{equation*}
	We may modify it to find that:
	\begin{equation*}
		\hat{\theta}\approx\theta-\frac{l'(\theta)}{l''(\theta)}
	\end{equation*}
	We may initially guess a number, say $\theta_{0}$. By iteratively applying the procedure for $j=0,1,\cdots$:
	\begin{equation*}
		\theta_{j+1}=\theta_{j}-\frac{l'(\theta_{j})}{l''(\theta_{j})}
	\end{equation*}
	and stop it at a certain stopping criterion, say $\abs{\theta_{j+1}-\theta_{j}}<K$ for some chosen constant $K$. E.g. $K=10^{-5}$.\\
	Using this algorithm, we can obtain or approximate the MLE of $\theta$.
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Gam(\alpha,\beta)$, where $\beta=3423$ and $\alpha$ is unknown. The PDF is defined by:
	\begin{equation*}
		f_{X}(x|\theta)=\begin{cases}
			\frac{3423^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-3423x}, &x>0\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
	We can find the log likelihood:
	\begin{equation*}
		l(\alpha)=\sum_{i=1}^{n}\ln{f_{X}(x_{i}|\alpha)}=n\alpha\ln{3423}-n\ln(\Gamma(\alpha))+(\alpha-1)\sum_{i=1}^{n}\ln{x_{i}}-3423\sum_{i=1}^{n}x_{i}
	\end{equation*}
	To find MLE, we try to solve the equation:
	\begin{equation*}
		0=\odv*{}{\alpha}l(\alpha)=n\ln{3423}-n\odv*{}{\alpha}\ln(\Gamma(\alpha))+\sum_{i=1}^{n}\ln{x_{i}}
	\end{equation*}
	However, we have no idea how to find $\odv*{}{\alpha}\ln(\Gamma(\alpha))$. Therefore, instead, we would use the numerical methods to approximate the MLE.
\end{eg}
\end{document}