\documentclass{huhtakm-template-book-v2}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator{\prob}{\mathbb{P}}
\DeclareMathOperator{\E}{\mathbb{E}}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\cov}{cov}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator{\Bern}{Bern}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\Geom}{Geom}
\DeclareMathOperator{\Poisson}{Poisson}
\DeclareMathOperator{\Exp}{Exp}
\DeclareMathOperator{\N}{N}
\DeclareMathOperator{\Cauchy}{Cauchy}
\DeclareMathOperator{\NBin}{NBin}
\DeclareMathOperator{\U}{U}
\DeclareMathOperator{\Gam}{Gamma}
\DeclareMathOperator{\Bet}{Beta}
\setlength{\parindent}{0pt}
\title{
	\Huge MATH 3423: Statistical Inference
}
\author{
	HU-HTAKM\\
	\small Website: \url{https://htakm.github.io/htakm_test/}
}
\date{
	Last major change: November 28, 2024\\
	Last small update: November 28, 2024
}
\begin{document}
\maketitle
This is MATH 3423 lecture note made by me. Note that some of the notations are slightly different with what is used in the course for better clarity. For example:\\
\begin{tabular}{||c|c|c||}
	\hline
	Name & My notation & Ycw's notation\\
	\hline
	Transpose & $\mathbf{A}^{T}$ &  $\mathbf{A}'$\\
	Set of real numbers & $\mathbb{R}$ & $R$\\
	Fisher Information & $\mathcal{I}_{X}(\theta)$ & $I_{X}(\theta)$\\
	Convergence in distribution & $X_{n}\xrightarrow{D}X$ & $X_{n}\xrightarrow{d}X$\\
	Probability & $\prob$ & $P$\\
	Expected value & $\E$ & $E$ or $E_{X}$\\
	\hline
\end{tabular}
\tableofcontents
\chapter{Preliminary}
Statistical inference is a statistical process which investigates how to use the information from the data to make an inference about the distribution of the random variable of our interest. In MATH 3423, we will focus on two core concepts of statistical inference: point estimation and hypothesis testing.
\section{Random sample and parametric distribution}
To make a statistical inference about the distribution of the random variable $X$ of our interest, we need to draw a sample of data.
\begin{defn}
	Denote the first observation by $X_{1}$, the second by $X_{2}$, and so on. A set of random variables $\{X_{1},\cdots,X_{n}\}$ are called a \textbf{random sample} of size $n$ from the common distribution of $X$ with a PMF $p_{X}(x)$ or PDF $f_{X}(x)$ if they are independent and identically distributed (i.i.d.).
\end{defn}
\begin{rem}
	Random variables $X_{1},\cdots,X_{n}$ are assumed to be observable with known actual values $x_{1},\cdots,x_{n}$ respectively.
\end{rem}
Under the random sampling setting, it is easy to get the following lemma.
\begin{lem}
	Given a random sample $\{X_{1},\cdots,X_{n}\}$ of a common distribution $X$.
	\begin{enumerate}
		\item If the random sample is discrete with a common PMF $p_{X}(x)$, then the joint PMF of random sample can be obtained by:
		\begin{equation*}
			p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n})=\prod_{i=1}^{n}p_{X_{i}}(x_{i})
		\end{equation*}
		\item If the random sample is continuous with a common PDF $f_{X}(x)$, then the joint PDF of random sample can be obtained by:
		\begin{equation*}
			f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n})=\prod_{i=1}^{n}f_{X_{i}}(x_{i})
		\end{equation*}
	\end{enumerate}
\end{lem}
The underlying distribution of $X$ is assumed to be unknown or partially known in practice. In most situation, it is reasonable to assume that the form of the PMF $p_{X}$ or PDF $f_{X}$ of the distribution is known but it contains some unknown parameters $\theta$.
\begin{defn}
	\textbf{Parametric distribution} is a distribution which the PMF $p_{X}$ and PDF $f_{X}$ contains some unknown parameters $\theta$. The PMF or PDF is said to be \textbf{parametric}.
\end{defn}
\begin{rem}
	Instead of using parametric distributions of the data, we may assume that the form of the distribution is unknown but that the distribution has some properties. E.g. A distribution is continuous. This distribution is called a \textbf{non-parametric distribution} and the corresponding statistical method is called \textbf{non-parametric statistical approach}. If parameters are involved but the form of the distribution is unknown, then such a distribution is called \textbf{semi-parametric distribution} and the corresponding method is called \textbf{semi-parametric statistical approach}.
\end{rem}

\newpage
\begin{eg}
	Data are usually assumed from the normal distribution with mean $\mu$ and variance $\sigma^{2}$, where the parameter:
	\begin{equation*}
		\theta=\begin{pmatrix}
			\mu\\
			\sigma^{2}
		\end{pmatrix}
	\end{equation*}
	is unknown but fixed.
\end{eg}
\begin{lem}
	Given a random sample $\{X_{1},\cdots,X_{n}\}$ of a common distribution $X$.
	\begin{enumerate}
		\item If the random sample is discrete with a common parametric PMF $p_{X}(x|\theta)$, then the joint PMF of random sample can be obtained by:
		\begin{equation*}
			p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}p_{X}(x_{i}|\theta)
		\end{equation*}
		\item If the random sample is continuous with a common parametric PDF $p_{X}(x|\theta)$, then the joint PDF of random sample can be obtained by:
		\begin{equation*}
			f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}f_{X}(x_{i}|\theta)
		\end{equation*}
	\end{enumerate}
\end{lem}
Under parametric setting, we can see that uncertainty of the distribution is the uncertainty of its parameters. One of the central problem in statistics is how to determine which function of data is the best one to estimate $\theta$.
\begin{defn}
	If $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$ is a random vector and $T(\cdot)$ is a real-valued or vector-valued function such that for all $\mathbf{X}\in\Omega$, $T(\mathbf{X})$ does not contain any unknown parameters, then $T(\mathbf{X})$ is called a \textbf{statistic}.
	\begin{enumerate}
		\item If we use the statistic $T(\mathbf{X})$ to estimate an unknown parameter $\theta$, then we call $T(\mathbf{X})$ and $T(\mathbf{x})$ an \textbf{estimator} and an \textbf{estimate} of $\theta$ respectively, where $\mathbf{x}$ is an observed value of $\mathbf{X}$.
	\end{enumerate}
\end{defn}
\begin{rem}
	We usually denote an estimator of $\theta$ by $\hat{\theta}(\mathbf{X})$ or simply $\hat{\theta}$.
\end{rem}
\begin{rem}
	Since $T(\mathbf{X})$ is also random, we have a distribution of $T(\mathbf{X})$ called \textbf{sampling distribution}.
\end{rem}

\section{Moments}
The population moments of a distribution play an important role in theoretical and applied statistics.
\begin{defn}
	For each positive integer $k$, the \textbf{$k$-th population moment} of $X$ about $0$, denoted by $\mu_{k}'$, is defined as:
	\begin{equation*}
		\mu_{k}'=\E(X^{k})
	\end{equation*}
	if the expectation exists.\\
	When $k=1$, $\mu_{1}'$ is the population mean $\mu=E(X)$ of $X$.
\end{defn}
\begin{defn}
	The \textbf{$k$-th population central moment} of $X$, denoted by $\mu_{k}$, is defined as:
	\begin{equation*}
		\mu_{k}=\E(X-\mu)^{k}
	\end{equation*}
	if the expectation exists.\\
	When $k=2$, $\mu_{2}$ is the \textbf{population variance} $\sigma^{2}$ of $X$.
\end{defn}
\begin{rem}
	Don't confuse the population mean $\mu$ and the $k$-th population central moment $\mu_{k}$!
\end{rem}

\newpage
\begin{eg}
	We have terminologies for some useful population moments.
	\begin{enumerate}
		\item \textbf{Skewness}: $\mu_{3}$, a measure of symmetry or skewness.
		\begin{enumerate}
			\item If $\mu_{3}<0$, then the curve is left-skewed (tail is on the left).
			\item If $\mu_{3}>0$, then the curve is right-skewed (tail is on the right).
			\item If $\mu_{3}=0$, the the curve is symmetrical.
		\end{enumerate}
		The ratio $\frac{\mu_{3}}{\sigma^{3}}$ is called the \textbf{coefficient of skewness}.
		\item \textbf{Kurtosis}: $\mu_{4}$, a measure of excess or kurtosis, which is the degree of flatness of a density near its center. We called $\frac{\mu_{4}}{\sigma^{4}}-3$ the \textbf{coefficient of kurtosis}.
		\begin{enumerate}
			\item If $\frac{\mu_{4}}{\sigma^{4}}-3>0$, then the density has a sharper peak than the density of the normal curve.
			\item If $\frac{\mu_{4}}{\sigma^{4}}-3<0$, then the density has a flatter peak than the density of the normal curve.
		\end{enumerate}
	\end{enumerate}
\end{eg}
We usually use sample moments to estimate the population moments.
\begin{defn}
	Let $X_{1},\cdots,X_{n}$ be a random sample of size $n$. For each positive integer $k$,
	\begin{enumerate}
		\item the \textbf{$k$-th sample moment} about $0$, denoted by $\overline{X^{k}}$, is defined as:
		\begin{equation*}
			\overline{X^{k}}=\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}
		\end{equation*}
		\item the \textbf{$k$-th sample moment} about $\overline{X}$, denoted by $S_{n}^{k}$, is defined as:
		\begin{equation*}
			S_{n}^{k}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{k}
		\end{equation*}
	\end{enumerate}
\end{defn}
\begin{eg}
	When $k=1$, $\overline{X}$ is called the \textbf{sample mean} of $X$. When $k=2$, $S_{n}^{2}$ is called the \textbf{sample variance}. However, we usually don't use this version of sample variance. Instead, we use another sample variance, denoted by $S_{n-1}^{2}$, which is defined by:
	\begin{equation*}
		S_{n-1}^{2}=\frac{1}{n-1}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}
	\end{equation*}
	We do this because $S_{n-1}^{2}$ is unbiased while $S_{n}^{2}$ is not unbiased.
\end{eg}
\begin{lem}
	Let $X_{1},\cdots,X_{n}$ be a random sample of size $n$. We have:
	\begin{equation*}
		\E(\overline{X^{k}})=\mu_{k}'
	\end{equation*}
	if $\mu_{k}'$ exists. We also have:
	\begin{equation*}
		\Var(\overline{X^{k}})=\frac{1}{n}[\mu_{2k}'-(\mu_{k}')^{2}]
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X_{1},\cdots,X_{n}$ have the same distribution, we have:
	\begin{equation*}
		\E(X_{1}^{k})=\cdots=\E(X_{n}^{k})=\E(X^{k})=\mu_{k}'
	\end{equation*}
	Therefore, we have:
	\begin{equation*}
		\E(\overline{X^{k}})=\E\left[\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\right]=\frac{1}{n}\sum_{i=1}^{n}\E(X_{i}^{k})=\frac{n\mu_{k}'}{n}=\mu_{k}'
	\end{equation*}
	Since $X_{1}^{k},\cdots,X_{n}^{k}$ are independent,
	\begin{align*}
		\Var(\overline{X^{k}})=\Var\left(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\right)=\frac{1}{n^{2}}\Var\left(\sum_{i=1}^{n}X_{i}^{k}\right)=\frac{1}{n^{2}}\sum_{i=1}^{n}\Var(X_{i}^{k})&=\frac{1}{n^{2}}\sum_{i=1}^{n}\left[\E(X_{i}^{2k})-(\E(X_{i}^{k})^{2})\right]\\
		&=\frac{1}{n}[\mu_{2k}'-(\mu_{k}')^{2}]
	\end{align*}
\end{proofing}

\section{Conditional distribution}
Sometimes, we would deal with cases when a certain information is given. 
\begin{defn}
	Suppose that $X$ and $Y$ are two random variables. The \textbf{conditional distribution function} of $Y$ given $X=x$ for any $x$ such that PMF $p_{X}(x)>0$ or PDF $f_{X}(x)>0$ is defined by:
	\begin{equation*}
		F_{Y|X}(y|x)=\prob(Y\leq y|X=x)
	\end{equation*}
	The \textbf{conditional PDF/PMF} of $Y$ given $X=x$ for any $x$ such that PMF $p_{X}(x)>0$ or PDF $f_{X}(x)>0$ is defined by:
	\begin{equation*}
		\begin{cases}
			p_{Y|X}(y|x)=\frac{p_{X,Y}(x,y)}{p_{X}(x)}, &\text{Discrete case}\\
			f_{Y|X}(y|x)=\frac{f_{X,Y}(x,y)}{f_{X}(x)}, &\text{Continuous case}
		\end{cases}
	\end{equation*}
\end{defn}
The conditional distribution has its corresponding expectation.
\begin{defn}
	Suppose that $X$ and $Y$ are two random variables. The \textbf{conditional expectation} of $Y$ given $X=x$ for any $x$ such that PMF $p_{X}(x)>0$ or PDF $f_{X}(x)>0$ is defined by:
	\begin{equation*}
		\E(Y|X=x)=\begin{cases}
			\sum_{y}yp_{Y|X}(y|x), &\text{Discrete case}\\
			\intinfty yf_{Y|X}(y|x)\,dy, &\text{Continuous case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	$\E(Y|X=x)$ is a function of $x$. Similarly, $\E(Y|X)$ is a function of $X$.
\end{rem}
\begin{eg}
	Suppose that the joint PDF of $X$ and $Y$ is given by:
	\begin{equation*}
		f_{X,Y}(x,y)=\begin{cases}
			\frac{1}{y}e^{x}, &x>0, y>0\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
	We want to compute $\E(X|Y=y)$. We may find that:
	\begin{align*}
		f_{Y}(y)&=\intinfty f_{X,Y}(x,y)\,dx=e^{-y}\intinfty\frac{1}{y}e^{-\frac{x}{y}}\,dx=e^{-y} & f_{X|Y}(x|y)&=\frac{f_{X,Y}(x,y)}{f_{Y}(y)}=\frac{1}{y}e^{-\frac{x}{y}}
	\end{align*}
	We may see that $(X|Y=y)\sim\Exp(\frac{1}{y})$. Therefore, $\E(X|Y=y)=y$.
\end{eg}
Conditional expectation has the following properties.
\begin{lem}
	\label{Chapter 1 (Lemma) Properties of conditional expectation}
	Suppose that $X$, $Y$ and $Z$ are three random variables. The conditional expectation has the following properties:
	\begin{enumerate}
		\item $\E(aY+bZ|X)=a\E(Y|X)+b\E(Z|X)$ for $a,b\in\mathbb{R}$.
		\item $\E(Y|X)\geq 0$ if $Y\geq 0$.
		\item If $X$ and $Y$ are independent, then $\E(Y|X)=\E(Y)$.
	\end{enumerate}
\end{lem}
\begin{proofing}
	Proof for discrete case is similar to continuous case.
	\begin{enumerate}
		\item 
		\begin{align*}
			\E(aY+bZ|X)&=\intinfty\intinfty(ay+bz)f_{Y,Z|X}(y,z|X)\,dy\,dz\\
			&=a\intinfty\intinfty yf_{Y,Z|X}(y,z|X)\,dy\,dz+b\intinfty\intinfty zf_{Y,Z|X}(y,z|X)\,dy\,dz\\
			&=a\intinfty yf_{Y|X}(y|X)\,dy+b\intinfty zf_{Z|X}(z|X)\,dz=a\E(Y|X)+b\E(Z|X)
		\end{align*}
		\item If $Y\geq 0$, then since $f_{Y|X}(y|x)\geq 0$ for any $x$ such that $f_{X}(x)>0$:
		\begin{equation*}
			\E(Y|X)=\int_{0}^{\infty}yf_{Y|X}(y|X)\,dy\geq 0
		\end{equation*} 
		\item If $X$ and $Y$ are independent, then:
		\begin{equation*}
			\E(Y|X)=\intinfty yf_{Y|X}(y|X)\,dy=\intinfty yf_{Y}(y)\,dy=\E(Y)
		\end{equation*}
	\end{enumerate}
\end{proofing}
If $\E(Y|X)$ is a function of $X$, then what is its expectation?
\begin{thm}\named{Law of total expectation}
	Given two random variables $X$ and $Y$. We have:
	\begin{equation*}
		\E(Y)=\E(\E(Y|X))
	\end{equation*}
	if both expectations exist. 
\end{thm}
\begin{proofing}
	We may proof in continuous case. Discrete case works similarly.
	\begin{align*}
		\E(\E(Y|X))=\intinfty\E(Y|x)f_{X}(x)\,dx&=\intinfty\intinfty y f_{Y|X}(y|x)f_{X}(x)\,dy\,dx\\
		&=\intinfty\intinfty f_{Y|X}(y|x)f_{X}(x)\,dx\,dy\\
		&=\intinfty\intinfty f_{X,Y}(x,y)\,dx\,dy\\
		&=\intinfty f_{Y}(y)\,dy=\E(Y)
	\end{align*}
\end{proofing}
The following theorem is the generalization of Law of total expectation. We will omit the proof.
\begin{lem}
	\label{Chapter 1 (Lemma) Generalization of Law of total expectation}
	Given two random variables $X$ and $Y$. For any function $g$, we have:
	\begin{equation*}
		\E(\E(Y|X)g(X))=\E(Yg(X))
	\end{equation*}
	if both expectations exist.
\end{lem}
Similarly, we have a conditional variance.
\begin{defn}
	Given two random variables $X$ and $Y$. The \textbf{conditional variance} of $Y$ given $X$ is defined by:
	\begin{equation*}
		\Var(Y|X)=\E\left[(Y-\E(Y|X))^{2}|X\right]
	\end{equation*}
\end{defn}
\begin{lem}
	\label{Chapter 1 (Lemma) Conditional variance decomposition}
	Given two random variables $X$ and $Y$. We have:
	\begin{equation*}
		\Var(Y|X)=\E(Y^{2}|X)-[\E(Y|X)]^{2}
	\end{equation*}
\end{lem}
\begin{proofing}
	By Lemma \ref{Chapter 1 (Lemma) Properties of conditional expectation} and Lemma \ref{Chapter 1 (Lemma) Generalization of Law of total expectation},
	\begin{align*}
		\Var(Y|X)&=\E\left[(Y-\E(Y|X))^{2}|X\right]\\
		&=\E(Y^{2}|X)-2\E(Y\E(Y|X)|X)+\E((\E(Y|X))^{2}|X)\\
		\tag{$\E(Y|X)$ is a function of $X$}
		&=\E(Y^{2}|X)-[\E(Y|X)]^{2}
	\end{align*}
\end{proofing}
We have the law of total variance.
\begin{thm}\named{Law of total variance}
	Given two random variables $X$ and $Y$. We have:
	\begin{equation*}
		\Var(Y)=\E[\Var(Y|X)]+\Var[\E(Y|X)]
	\end{equation*}
	if the expectation and variances exist.
\end{thm}
\begin{proofing}
	By Lemma \ref{Chapter 1 (Lemma) Conditional variance decomposition} and Law of total expectation,
	\begin{align*}
		\E[\Var(Y|X)]+\Var[\E(Y|X)]&=\E[\E(Y^{2}|X)]-\E[\E(Y|X)]^{2}+\E[\E(Y|X)]^{2}-\left[\E[\E(Y|X)]\right]^{2}\\
		&=\E(Y^{2})-[\E(Y)]^{2}=\Var(Y)
	\end{align*}
\end{proofing}

\section{Commonly used distribution}
Let us recall some useful distributions.
\begin{eg}\named{Binomial distribution} $X\sim\Bin(n,p)$\\
	Random variable $X$ is a Binomial random variable with parameters $n\in\mathbb{N}$ and $p\in[0,1]$ if it has a PMF for $x=0,\cdots,n$:
	\begin{align*}
		p_{X}(x)&=\binom{n}{x}p^{x}(1-p)^{n-x} & \E(X)&=np & \Var(X)&=np(1-p)
	\end{align*}
\end{eg}
\begin{eg}\named{Geometric distribution} $X\sim\Geom(p)$\\
	Random variable $X$ is geometric with parameter $p\in[0,1]$ if it has a PMF for $x=1,2,\cdots$:
	\begin{align*}
		p_{X}(x)&=p(1-p)^{x-1} & \E(X)&=\frac{1}{p} & \Var(X)&=\frac{1-p}{p^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Poisson distribution} $X\sim\Poisson(\lambda)$\\
	Random variable $X$ is a Poisson random variable with parameter $\lambda$ if it has a PMF for $x=0,1,\cdots$:
	\begin{align*}
		p_{X}(x)&=\frac{\lambda^{x}}{x!}e^{-\lambda} & \E(X)&=\lambda & \Var(X)&=\lambda
	\end{align*}
\end{eg}
\begin{eg}\named{Negative Binomial distribution} $X\sim\NBin(r,p)$\\
	Assume that $X_{1},\cdots,X_{r}$ are independent and $X_{i}\sim\Geom(p)$ for $i=1,\cdots,r$. Let $Y=\sum_{i=1}^{n}X_{i}$. Random variable $Y$ is negative Binomial with parameters $r>0$ and $p\in[0,1]$ if for $x>r$:
	\begin{align*}
		p_{X}(x)&=\binom{x-1}{r-1}(1-p)^{x-r}p^{r} & \E(X)&=\frac{r}{p} & \Var(X)&=\frac{r(1-p)}{p^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Cauchy distribution} $X\sim\Cauchy(\theta)$\\
	Random variable $X$ is a Cauchy random variable with parameter $\theta$ if it has a PDF:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\pi(1+(x-\theta)^{2})} & \E(X)&\text{ DNE} & \Var(X)&\text{ DNE}
	\end{align*}
\end{eg}
\begin{eg}\named{Uniform distribution} $X\sim\U[a,b]$\\
	Random variable $X$ is uniform if given $a<b$, it has a PDF:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{b-a}, &x\in[a,b]\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{a+b}{2} & \Var(X)&=\frac{(b-a)^{2}}{12}
	\end{align*}
\end{eg}
\begin{eg}\named{Exponential distribution} $X\sim\Exp(\lambda)$\\
	Random variable $X$ is exponential with parameter $\lambda$ if it has a PDF:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\lambda e^{-\lambda x}, &x\geq 0\\
			0, &x<0
		\end{cases} & \E(X)&=\frac{1}{\lambda} & \Var(X)&=\frac{1}{\lambda^{2}}
	\end{align*}
\end{eg}
\begin{eg}\named{Normal distribution / Gaussian distribution} $X\sim\N(\mu,\sigma^{2})$\\
	Random variable $X$ is normal if it has two parameters $\mu$ and $\sigma^{2}$, and its PDF and CDF is:
	\begin{align*}
		f_{X}(x)&=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right) & F_{X}(x)&=\intlu{-\infty}{x}f_{X}(u)\,du & \E(X)&=\mu & \Var(X)&=\sigma^{2}
	\end{align*}
	Random variable $X$ is standard normal if $\mu=0$ and $\sigma^{2}=1$. ($X\sim\N(0,1)$)
	\begin{align*}
		f_{X}(x)&=\phi(x)=\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{x^{2}}{2}\right) & F_{X}(x)&=\Phi(x)=\intlu{-\infty}{x}\phi(u)\,du & \E(X)&=0 & \Var(X)&=1
	\end{align*}
\end{eg}
\begin{eg}\named{Gamma distribution} $X\sim\Gam(\alpha,\beta)$\\
	Random variable $X$ is a gamma random variable with parameters $\alpha>0$ and $\beta>0$ if its PDF is:
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{1}{\Gamma(\alpha)}\beta^{\alpha}x^{\alpha-1}e^{-\beta x}, &x\geq 0\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{\alpha}{\beta} & \Var(X)&=\frac{\alpha}{\beta^{2}}
	\end{align*}
\end{eg}
\begin{rem}
	Gamma function $\Gamma(z)$ has the following properties:
	\begin{enumerate}
		\item If $z$ is a positive integer, then $\Gamma(z)=(z-1)!$.
		\item For all $z$, $\Gamma(z+1)=z\Gamma(z)$.
		\item If $\Re(z)>0$, then $\Gamma(z)=\int_{0}^{\infty}t^{z-1}e^{-t}\,dt$.
		\item $\Gamma(\frac{1}{2})=\sqrt{\pi}$.
	\end{enumerate}
\end{rem}
\begin{eg}\named{Beta distribution} $X\sim\Bet(\alpha,\beta)$\\
	Random variable $X$ is a beta random variable with parameters $\alpha>0$ and $\beta>0$ if its PDF is: 
	\begin{align*}
		f_{X}(x)&=\begin{cases}
			\frac{x^{\alpha-1}(1-x)^{\beta-1}}{B(\alpha,\beta)}, &x\in(0,1)\\
			0, &\text{Otherwise}
		\end{cases} & \E(X)&=\frac{\alpha}{\alpha+\beta} & \Var(X)&=\frac{\alpha\beta}{(\alpha+\beta)^{2}(\alpha+\beta+1)}
	\end{align*}
\end{eg}
\begin{rem}
	Beta function $B(z_{1},z_{2})$ has the following properties:
	\begin{enumerate}
		\item $B(z_{1},z_{2})=\frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1}+z_{2})}$
		\item $B(z_{1},z_{2})=\int_{0}^{1}t^{z_{1}-1}(1-t)^{z_{2}-1}\,dt$
	\end{enumerate}
\end{rem}
We have some more distributions that are associated with normal distribution. For example, Chi-squared distribution, which is a special case of gamma distribution.
\begin{eg}\named{Chi-squared distribution} $Y\sim\chi^{2}(n)$\\
	Assume that $X_{1},X_{2},\cdots,X_{n}$ are independent and $X_{i}\sim\N(0,1)$ for $i=1,\cdots,n$. Let $Y=\sum_{i=1}^{n}X_{i}^{2}$. Random variable $Y$ has a $\chi^{2}$-distribution with $n$ degree of freedom if:
	\begin{align*}
		f_{Y}(x)&=\begin{cases}
			\frac{1}{\Gamma(\frac{n}{2})}2^{-\frac{n}{2}}x^{\frac{n}{2}-1}e^{-\frac{x}{2}}, &x>0\\
			0, \text{Otherwise}
		\end{cases} & \E{Y}&=n & \Var(Y)&=2n
	\end{align*}
\end{eg} 
\begin{thm}
	\label{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}
	If the random variable $X\sim\N(\mu,\sigma^{2})$, where $\sigma^{2}>0$, then the random variable $V=\frac{(X-\mu)^{2}}{\sigma^{2}}\sim\chi^{2}(1)$.
\end{thm}
\begin{proofing}
	By the properties of normal distribution, we get that:
	\begin{equation*}
		\frac{X-\mu}{\sigma}\sim\N(0,1)
	\end{equation*}
	Therefore, by the definition of chi-squared distribution,
	\begin{equation*}
		V=\left(\frac{X-\mu}{\sigma}\right)^{2}\sim\chi^{2}(1)
	\end{equation*}
\end{proofing}
\begin{thm}
	Given a set of random variables $\{X_{1},\cdots,X_{k}\}$. Let $Y=\sum_{i=1}^{k}X_{i}$ and $X_{i}\sim\chi^{2}(r_{i})$ for all $i=1,\cdots,k$. If they are independent, then $Y\sim\chi^{2}(r_{1}+\cdots+r_{k})$.
\end{thm}
\begin{proofing}
	It suffices to prove the relation for two random variables $Z_{1}$ and $Z_{2}$. If $Z_{1}\sim\chi^{2}(n_{1})$ and $Z_{2}\sim\chi^{2}(n_{2})$, then $Z_{1}+Z_{2}\sim\chi^{2}(n_{1}+n_{2})$. By repeating applying the relation for two random variables, one can easily have the desired relation for $k$ random variables.\\
	From definition, $Z_{1}=X_{11}^{2}+\cdots+X_{1n_{1}}^{2}$ and $Z_{2}=X_{21}^{2}+\cdots+X_{2n_{2}}^{2}$, where $X_{1i}\sim\N(0,1)$ and $X_{2j}\sim\N(0,1)$ for $i=1,\cdots,n_{1}$ and $j=1,\cdots,n_{2}$. Therefore,
	\begin{equation*}
		Z_{1}+Z_{2}=X_{11}^{2}+\cdots+X_{1n_{1}}^{2}+X_{21}^{2}+\cdots+X_{2n_{2}}^{2}\sim\chi^{2}(n_{1}+n_{2})
	\end{equation*}
\end{proofing}
\begin{thm}
	\label{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}
	If $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n>1$ of a random variable $X\sim\N(\mu,\sigma^{2})$, then we have:
	\begin{enumerate}
		\item Sample mean $\overline{X}\sim\N(\mu,\frac{\sigma^{2}}{n})$
		\item Sample mean $\overline{X}$ and sample variance $S_{n-1}^{2}$ are independent.
		\item 
		\begin{equation*}
			\frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}=\frac{nS_{n}^{2}}{\sigma^{2}}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item From definition,
		\begin{equation*}
			\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}
		\end{equation*}
		Since $X_{i}\sim\N(\mu,\sigma^{2})$ for $i=1,\cdots,n$, we can find that $\overline{X}\sim\N(\mu,\frac{\sigma^{2}}{n})$.
		\item Let $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$. We may find that:
		\begin{align*}
			\begin{pmatrix}
				\overline{X}\\ X_{1}-\overline{X}\\ X_{2}-\overline{X}\\ \vdots\\ X_{n}-\overline{X}
			\end{pmatrix}=\begin{pmatrix}
				\frac{1}{n}X_{1}+\frac{1}{n}X_{2}+\cdots+\frac{1}{n}X_{n}\\
				\left(1-\frac{1}{n}\right)X_{1}-\frac{1}{n}X_{2}-\cdots-\frac{1}{n}X_{n}\\
				-\frac{1}{n}X_{1}+\left(1-\frac{1}{n}\right)X_{2}-\cdots-\frac{1}{n}X_{n}\\
				\vdots\\
				-\frac{1}{n}X_{1}-\frac{1}{n}X_{2}-\cdots+\left(1-\frac{1}{n}\right)X_{n}
			\end{pmatrix}&=\mathbf{AX} & \mathbf{A}&=\begin{pmatrix}
				\frac{1}{n} & \frac{1}{n} & \hdots & \frac{1}{n}\\
				1-\frac{1}{n} & -\frac{1}{n} & \hdots & -\frac{1}{n}\\
				-\frac{1}{n} & 1-\frac{1}{n} & \ddots & \vdots\\
				\vdots & \ddots & \ddots & -\frac{1}{n}\\
				-\frac{1}{n} & \hdots & -\frac{1}{n} & 1-\frac{1}{n}
			\end{pmatrix}
		\end{align*}
		By Lemma \ref{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}, we have $\mathbf{AX}\sim\N_{n+1}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\sigma^{2}I_{n\times n}A}^{T})$, where $\boldsymbol{\mu}=(\mu\ \cdots\ \mu)^{T}$.\\
		Let $\mathbf{X^{*}}=(X_{1}-\overline{X}\ \cdots\ X_{n}-\overline{X})^{T}$ and $\mathbf{\Sigma^{*}}$ be the variance-covariance matrix of $\mathbf{\widetilde{X}}$. We may notice that:
		\begin{equation*}
			\mathbf{A\sigma^{2}I_{n\times n}A}^{T}=\begin{pmatrix}\begin{array}{c|c}
					\Var(\overline{X}) & \cov(\mathbf{X^{*}},\overline{X})\\
					\hline
					\cov(\mathbf{X^{*}},\overline{X}) & \mathbf{\Sigma^{*}}
			\end{array}\end{pmatrix}
		\end{equation*}
		We prove that $\cov(X_{i}-\overline{X},\overline{X})=0$ for $i=1,\cdots,n$. Since $X_{i}$ are independent for all $i$,
		\begin{equation*}
			\cov(X_{i}-\overline{X},\overline{X})=\cov(X_{i},\overline{X})-\Var(\overline{X})=\frac{1}{n}\Var(X_{i})-\frac{\sigma^{2}}{n}=0
		\end{equation*}
		Therefore, we can find that $\cov(\mathbf{X^{*}},\overline{X})=0$. By Lemma \ref{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}, $\overline{X}$ and $\mathbf{X^{*}}$ are independent.\\
		Since $S_{n}^{2}$ is a function of $\mathbf{X^{*}}$, we can conclude that $\overline{X}$ and $S_{n-1}^{2}$ are independent.
		\item We have:
		\begin{equation*}
			\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu+\mu-\overline{X})^{2}=\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\mu)^{2}-\frac{n(\overline{X}-\mu)^{2}}{\sigma^{2}}=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)-\left(\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\right)^{2}
		\end{equation*}
		Let $U=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)$ and $V=\left(\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\right)^{2}$. The distribution we are finding is $U-V$.\\
		From definition, we know that $U\sim\chi^{2}(n)$. From Theorem \ref{Chapter 1 (Theorem) chi-square distribution with 1 degree of freedom}, we find the $V\sim\chi^{2}(1)$. From Part 2, since functions of $\mathbf{X}^{*}$ and $\overline{X}$ are independent, 
		\begin{align*}
			M_{U}(t)&=M_{V}(t)M_{U-V}(t)\\
			(1-2t)^{-\frac{n}{2}}&=(1-2t)^{-\frac{1}{2}}M_{U-V}(t)\\
			M_{U-V}(t)&=(1-2t)^{-\frac{n-1}{2}}
		\end{align*}
		Therefore, we can conclude that:
		\begin{equation*}
			\frac{1}{\sigma^{2}}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}\sim\chi^{2}(n-1)
		\end{equation*}
	\end{enumerate}
\end{proofing}
\begin{rem}
	From the same proof of above theorem part 2, we can also find that $\overline{X}$ and $S_{n}^{2}$ are independent.
\end{rem}
\begin{eg}\named{Student's t-distribution} $T\sim t(r)$\\
	Assume that $X\sim\N(0,1)$ and $Y\sim\chi^{2}(r)$. Let:
	\begin{equation*}
		T=\frac{X}{\sqrt{\frac{Y}{r}}}
	\end{equation*}
	Then $T$ has a t-distribution with $t$ degree of freedom and:
	\begin{align*}
		f_{T}(t)&=\frac{\Gamma\left(\frac{r+1}{2}\right)}{\sqrt{r\pi}\Gamma\left(\frac{r}{2}\right)}\left(1+\frac{t^{2}}{r}\right)^{-\frac{r+1}{2}} & \E(T)&=\begin{cases}
			\text{Undefined}, &r\leq 1\\
			0, &n>1\\
		\end{cases} & \Var(T)&=\begin{cases}
			\text{Undefined}, &r\leq 1\\
			\infty, &1<r\leq 2\\
			\frac{r}{r-2}, &r>2
		\end{cases}
	\end{align*}
\end{eg}
\begin{rem}
	As $r\to\infty$, $T\to\N(0,1)$ by CLT.
\end{rem}
\begin{rem}
	If we fix $Y=y$, then we can find that $T\sim\N(0,\frac{r}{y})$.
\end{rem}
The t-distribution has the following properties.
\begin{thm}
	If $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n>1$ of random variable $X\sim\N(\mu,\sigma^{2})$, then we have:
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}\sim t(n-1)
	\end{equation*}
\end{thm}
\begin{proofing}
	From Theorem \ref{Chapter 1 (Theorem) Normal and chi-squared distribution related to sample mean and variance}, $\overline{X}$ and $S_{n-1}^{2}$ are independent, and:
	\begin{align*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}&\sim\N(0,1) & \frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}\sim\chi^{2}(n-1)
	\end{align*}
	Therefore, from definition,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}=\frac{\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}}{\sqrt{\frac{1}{n-1}\left(\frac{(n-1)S_{n-1}^{2}}{\sigma^{2}}\right)}}\sim t(n-1)
	\end{equation*}
\end{proofing}
\begin{eg}
	Assume that we want to find the $95\%$ confidence interval of $\mu$ without knowing the population variance $\sigma^{2}$. Then we can find:
	\begin{align*}
		0.95&=\prob\left(-t_{0.025,n-1}\leq\frac{\sqrt{n}(\overline{X}-\mu)}{S_{n-1}}\leq t_{0.025,n-1}\right)\\
		&=\prob\left(\overline{X}-t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\leq\mu\leq\overline{X}+t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\right)
	\end{align*}
	Therefore, we can find the $95\%$ confidence interval:
	\begin{equation*}
		\left(\overline{x}-t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}},\overline{x}+t_{0.025,n-1}\frac{S_{n-1}}{\sqrt{n}}\right)
	\end{equation*}
	Usually when $n>30$, $t_{0.025,n-1}\approx z_{0.025}$. Therefore, we may find the $95\%$ confidence interval:
	\begin{equation*}
		\left(\overline{x}-z_{0.025}\frac{S_{n-1}}{\sqrt{n}},\overline{x}+z_{0.025}\frac{S_{n-1}}{\sqrt{n}}\right)
	\end{equation*}
\end{eg}

\newpage
\begin{eg}\named{F distribution} $F\sim F(r_{1},r_{2})$\\
	Assume that $X$ and $Y$ are independent random variables with $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Let:
	\begin{equation*}
		F=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
	\end{equation*}
	Then $F$ has a F-distribution with $r_{1}$ and $r_{2}$ degrees of freedom with:
	\begin{equation*}
		f_{F}(w)=\frac{\Gamma\left(\frac{r_{1}+r_{2}}{2}\right)}{\Gamma\left(\frac{r_{1}}{2}\right)\Gamma\left(\frac{r_{2}}{2}\right)}\left(\frac{r_{1}}{r_{2}}\right)^{\frac{r_{1}}{2}}w^{\frac{r_{1}}{2}-1}\left(1+\frac{r_{1}w}{r_{2}}\right)^{-\frac{r_{1}+r_{2}}{2}}
	\end{equation*}
	where $0<w<\infty$. We define $F_{\alpha}(r_{1},r_{2})$ by:
	\begin{equation*}
		\prob(F\geq F_{\alpha}(r_{1},r_{2}))=\alpha
	\end{equation*}
\end{eg}
\begin{lem}
	Let $U\sim F(r_{1},r_{2})$. The F-distribution has the following properties:
	\begin{enumerate}
		\item $\frac{1}{U}\sim F(r_{2},r_{1})$
		\item If $F_{\alpha}(r_{1},r_{2})$ is defined by $\prob(U\geq F_{\alpha}(r_{1},r_{2}))=\alpha$, then:
		\begin{equation*}
			\frac{1}{F_{\alpha}(r_{1},r_{2})}=F_{1-\alpha}(r_{2},r_{1})
		\end{equation*}
	\end{enumerate}
\end{lem}
\begin{proofing}
	\begin{enumerate}
		\item By definition,
		\begin{equation*}
			U=\frac{\frac{X}{r_{1}}}{\frac{Y}{r_{2}}}
		\end{equation*}
		where $X\sim\chi^{2}(r_{1})$ and $Y\sim\chi^{2}(r_{2})$. Therefore,
		\begin{equation*}
			\frac{1}{U}=\frac{\frac{Y}{r_{2}}}{\frac{X}{r_{1}}}\sim F(r_{2},r_{1})
		\end{equation*}
		\item With $\prob(U\geq F_{\alpha}(r_{1},r_{2}))=\alpha$, since $f_{U}(w)$ is only defined for $w>0$, we can get that:
		\begin{align*}
			\prob\left(\frac{1}{U}\leq\frac{1}{F_{\alpha}(r_{1},r_{2})}\right)&=\alpha\\
			\prob\left(\frac{1}{U}>\frac{1}{F_{\alpha}(r_{1},r_{2})}\right)&=1-\alpha
		\end{align*}
		From Part 1, we find that $\frac{1}{U}\sim F(r_{2},r_{1})$. Therefore,
		\begin{equation*}
			\prob\left(\frac{1}{U}\geq F_{1-\alpha}(r_{2},r_{1})\right)=1-\alpha
		\end{equation*}
		We find that $\frac{1}{F_{\alpha}(r_{1},r_{2})}=F_{1-\alpha}(r_{2},r_{1})$
	\end{enumerate}
\end{proofing}

\newpage
\begin{eg}
	Assume that we try to compare two populations. Let $X_{1}\sim\N(\mu_{1},\sigma_{1}^{2})$ represents the random variable of the first population and $X_{2}\sim\N(\mu_{2},\sigma_{2}^{2})$ represents the random variable of the second population. We want to find an interval guess of their ratio of variance $\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}$.\\
	Let $\{X_{11},\cdots,X_{1n}\}$ be a random sample of size $n$ from $X_{1}$ and $\{X_{21},\cdots,X_{2m}\}$ be a random sample of size $m$ from $X_{2}$. We can find that:
	\begin{align*}
		\frac{(n-1)S_{n-1,1}^{2}}{\sigma_{1}^{2}}&\sim\chi^{2}(n-1) & \frac{(m-1)S_{m-1,2}^{2}}{\sigma_{2}^{2}}&\sim\chi^{2}(m-1)
	\end{align*}
	We can also find that $S_{n-1},1$ and $S_{m-1},2$ are independent since they are from different population. Therefore, we have:
	\begin{equation*}
		\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right)=\frac{\frac{1}{m-1}\left(\frac{(m-1)S_{m-1,2}^{2}}{\sigma_{2}^{2}}\right)}{\frac{1}{n-1}\left(\frac{(n-1)S_{n-1,1}^{2}}{\sigma_{1}^{2}}\right)}\sim F(m-1,n-1)
	\end{equation*}
	Then we can find the $95\%$ confidence interval by:
	\begin{align*}
		0.95&=\prob\left(f_{0.975,m-1,n-1}\leq\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\left(\frac{S_{m-1,2}^{2}}{S_{n-1,1}^{2}}\right)\leq f_{0.025,m-1,n-1}\right)\\
		&=\prob\left(\frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}}f_{0.975,m-1,n-1}\leq\frac{\sigma_{1}^{2}}{\sigma_{2}^{2}}\leq \frac{S_{n-1,1}^{2}}{S_{m-1,2}^{2}}f_{0.025,m-1,n-1}\right)
	\end{align*}
\end{eg}

\section{Moment generating function}
It would be useful if we have a function that could generate all moments.
\begin{defn}
	The \textbf{moment generating function} (MGF) of a random variable $X$, denoted by $M_{X}(t)$, is:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})
	\end{equation*}
	if the expectation exists for $t$ in some neighbourhood of $0$.
\end{defn}
\begin{rem}
	More precisely, there exists $h>0$ such that for all $t$ in $(-h,h)$, $\E(e^{tX})$ exists.
\end{rem}
\begin{rem}
	MGF of $X$ may not always exist. However, if it exists, then $M_{X}(t)$ is continuously differentiable in some neighbourhood of the origin.
\end{rem}
\begin{rem}
	If we replace $e^{tX}$ by its taylor series, then we would get:
	\begin{equation*}
		M_{X}(t)=\E\left[\sum_{i=0}^{\infty}\frac{(tX)^{i}}{i!}\right]=\sum_{i=0}^{\infty}\frac{t^{i}}{i!}\E(X^{i})=\sum_{i=0}^{\infty}\frac{t^{i}}{i!}\mu_{i}'
	\end{equation*}
\end{rem}
\begin{lem}
	If $M_{X}(t)$ is the MGF of a random variable $X$, then:
	\begin{equation*}
		\left.\pdv*[order={k}]{M_{X}(t)}{t}\right|_{t=0}=\E(X^{k})=\mu_{k}'
	\end{equation*}
\end{lem}
\begin{proofing}
	From the taylor series of MGF, we would see that:
	\begin{equation*}
		\pdv*[order={k}]{M_{X}(t)}{t}=\sum_{i=k}^{\infty}\frac{t^{i-k}}{(i-k)!}\E(X^{i})
	\end{equation*}
	Therefore, by having $t=0$, we would have:
	\begin{equation*}
		\left.\pdv*[order={k}]{M_{X}(t)}{t}\right|_{t=0}=\E(X^{k})
	\end{equation*}
\end{proofing}
\begin{eg}
	What is the MGF of $X\sim\Bern(p)$? We have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=e^{t(0)}(1-p)+e^{t(1)}(p)=pe^{t}+1-p
	\end{equation*}
\end{eg}
\begin{lem}
	If random variables $X$ and $Y$ are independent, then:
	\begin{equation*}
		M_{X+Y}(t)=M_{X}(t)M_{Y}(t)
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X$ and $Y$ are independent,
	\begin{equation*}
		M_{X+Y}(t)=\E(e^{t(X+Y)})=\E(e^{tX})\E(e^{tY})=M_{X}(t)M_{Y}(t)
	\end{equation*}
\end{proofing}
\begin{eg}
	By definition, if $Y=\Bin(n,p)$, then $Y=X_{1}+\cdots+X_{n}$ where $X_{i}\sim\Bern(p)$ for all $i$ and they are independent. Therefore,
	\begin{equation*}
		M_{Y}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=(pe^{t}+1-p)^{n}
	\end{equation*}
	We may also solve it without using the definition.
	\begin{equation*}
		M_{Y}(t)=\E(e^{tY})=\sum_{i=0}^{n}\binom{n}{i}(pe^{t})^{i}(1-p)^{n-i}=(pe^{t}+1-p)^{n}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider $X\sim\Poisson(\lambda)$, The MGF of $X$ can be obtained by:
	\begin{equation*}
		M_{X}(t)=\sum_{k=0}^{\infty}e^{tk}\frac{\lambda^{k}e^{-\lambda}}{k!}=e^{-\lambda}\sum_{k=0}^{\infty}\frac{(\lambda e^{t})^{k}}{k!}=e^{\lambda(e^{t}-1)}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider $X\sim\Exp(\lambda)$. If $t<\lambda$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=\int_{0}^{\infty}e^{tx}\lambda e^{-\lambda x}\,dx=\lambda\int_{0}^{\infty}e^{-(\lambda-t)x}\,dx=\frac{\lambda}{\lambda-t}
	\end{equation*}
\end{eg}
\begin{eg}
	What is the MGF of $X\sim\N(\mu,\sigma^{2})$? We may first find the MGF of $Z\sim\N(0,1)$.
	\begin{align*}
		M_{Z}(t)=\E(e^{tZ})&=\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z^{2}-2tz)}\,dz\\
		&=\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}((z-t)^{2}-t^{2})}\,dz\\
		&=e^{\frac{t^{2}}{2}}\intinfty\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(z-t)^{2}}\,dz\\
		&=e^{\frac{t^{2}}{2}}
	\end{align*}
	Therefore, by having $X=\sigma Z+\mu$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=e^{\mu t}\E(e^{t\sigma Z})=e^{\mu t+\frac{1}{2}\sigma^{2}t^{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider $X\sim\U[a,b]$ where $a<b$, we have:
	\begin{equation*}
		M_{X}(t)=\E(e^{tX})=\int_{a}^{b}\frac{e^{tx}}{b-a}\,dt=\left[\frac{e^{tx}}{t(b-a)}\right]_{a}^{b}=\frac{e^{bt}-e^{at}}{t(b-a)}
	\end{equation*}
\end{eg}

\newpage
\begin{eg}
	If $X\sim\NBin(r,p)$, then for $t<-\ln(1-p)$:
	\begin{equation*}
		M_{X}(t)=\left(\frac{pe^{t}}{1-(1-p)e^{t}}\right)^{r}
	\end{equation*}
	If $X\sim\Gam(\alpha,\beta)$, then for $t<\beta$:
	\begin{equation*}
		M_{X}(t)=\left(\frac{\beta}{\beta-t}\right)^{\alpha}
	\end{equation*}
\end{eg}
\begin{eg}
	Given $Y\sim\chi^{2}(r)$. How do we find the MGF of $Y$?\\
	Note that chi-squared distribution is actually a special case of gamma distribution. We have $\chi^{2}(r)=\Gamma(\frac{r}{2},\frac{1}{2})$. Therefore, by substituting, for $t<\frac{1}{2}$, we may get:
	\begin{equation*}
		M_{Y}(t)=\left(\frac{\frac{1}{2}}{\frac{1}{2}-t}\right)^{\frac{r}{2}}=(1-2t)^{-\frac{r}{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	Given that $Y\sim\chi^{2}(r)$. How do we find $\E(Y)$ without using the MGF of $Y$?\\
	By definition, we may let $Y=\sum_{i=1}^{r}X_{i}^{2}$, where $X_{i}\sim\N(0,1)$. Therefore,
	\begin{equation*}
		\E(Y)=\sum_{i=1}^{r}\E(X_{i}^{2})=\sum_{i=1}^{r}\left.\odv*[order={2}]{}{t}e^{\frac{1}{2}t^{2}}\right|_{t=0}=\left.r(1+t^{2})e^{\frac{1}{2}t^{2}}\right|_{t=0}=r
	\end{equation*}
\end{eg}
Ultimately, the reason why we use moment generating function is the following fact.
\begin{thm}\named{Uniqueness of MGF}
	Let $X$ and $Y$ be two random variables. Suppose that their MGFs exist and are equal for all $t\in(-h,h)$ for some $h>0$, then the distribution functions $F_{X}$ and $F_{Y}$ are equal.
\end{thm}
This means that by knowing the MGF of a particular random variable $X$, we can know its distribution.
\begin{eg}
	Assume that $X_{1},\cdots,X_{n}$ are independent and $X_{i}\sim\Bin(m_{i},p)$ for all $i=1,\cdots,n$. Then we have:
	\begin{equation*}
		M_{X_{1}+\cdots+X_{n}}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=\prod_{i=1}^{n}(pe^{t}+1-p)^{m_{i}}=(pe^{t}+1-p)^{\sum_{i=1}^{n}m_{i}}
	\end{equation*}
	Therefore, we have $X_{1}+\cdots+X_{n}\sim\Bin(\sum_{i=1}^{n}m_{i},p)$.
\end{eg}
\begin{eg}
	Assume that $X_{1},\cdots,X_{n}$ are independent and $X_{i}\sim\Poisson(\lambda_{i})$ for all $i=1,\cdots,n$. Then we have:
	\begin{equation*}
		M_{X_{1}+\cdots+X_{n}}(t)=\prod_{i=1}^{n}M_{X_{i}}(t)=\prod_{i=1}^{n}e^{\lambda_{i}(e^{t}-1)}=e^{\sum_{i=1}^{n}\lambda_{i}(e^{t}-1)}
	\end{equation*}
	Therefore, we have $X_{1}+\cdots+X_{n}\sim\Poisson(\sum_{i=1}^{n}\lambda_{i})$.
\end{eg}
\begin{eg}
	Similarly, given a set of independent random variables $\{X_{1},\cdots,X_{n}\}$.
	\begin{enumerate}
		\item If $X_{i}\sim\NBin(r_{i},p)$, then $X_{1}+\cdots+X_{n}\sim\NBin(\sum_{i=1}^{n}r_{i},p)$.
		\item If $X_{i}\sim\N(\mu_{i},\sigma_{i}^{2})$, then $X_{1}+\cdots+X_{n}\sim\N(\sum_{i=1}^{n}\mu_{i},\sum_{i=1}^{n}\sigma_{i}^{2})$.
		\item If $X_{i}\sim\Gam(\alpha_{i},\beta)$, then $X_{1}+\cdots+X_{n}\sim\Gam(\sum_{i=1}^{n}\alpha_{i},\beta)$ and $cX_{i}\sim\Gam(\alpha_{i},\frac{\beta}{c})$ for $c\neq 0$.
	\end{enumerate}
\end{eg}
\begin{rem}
	Not all sum of distributions will result in the same type of distribution.
\end{rem}

\newpage
More generally, we would deal with problems of limiting distribution.
\begin{thm}
	Suppose the $\{X_{n}\}$ is a sequence of random variables, each with MGF $M_{X_{n}}(t)$. If we have:
	\begin{equation*}
		\lim_{n\to\infty}M_{X_{n}}(t)=M_{Y}(t)
	\end{equation*}
	for all $t$ in a neighbourhood of $0$, where $M_{Y}(t)$ is a MGF for some random variables $Y$, then there is an unique distribution function $F_{Y}$ with corresponding $M_{Y}(t)$ such that:
	\begin{equation*}
		\lim_{n\to\infty}F_{X_{n}}(y)=F_{Y}(y)
	\end{equation*}
	for all $y$ where $F_{Y}(y)$ is continuous. We denote by $X_{n}\to Y$ or $X_{n}\xrightarrow{D}X$.
\end{thm}
\begin{rem}
	Simply, the limiting distribution of $X_{n}$ is equal to the distribution of $Y$.
\end{rem}
We may define the limiting convergence in truly theoretical way.
\begin{defn}
	A sequence of random variables $\{X_{n}\}$ \textbf{converges in distribution} to a random variable $X$, denoted by $X_{n}\xrightarrow{D}X$, if for all continuity point $x$ of $F_{X}$, as $n\to\infty$,
	\begin{equation*}
		F_{X_{n}}(x)\to F_{X}(x)
	\end{equation*}
\end{defn}
We also have a more strict convergence.
\begin{defn}
	A sequence of random variables $\{X_{n}\}$ \textbf{converges in probability} to a random variable $X$, denoted by $X_{n}\xrightarrow{\prob}X$, if for any $\varepsilon>0$, as $n\to\infty$,
	\begin{align*}
		\prob(\abs{X_{n}-X}<\varepsilon)&\to 1 & \prob(\abs{X_{n}-X}\geq\varepsilon)&\to 0
	\end{align*}
\end{defn}
\begin{rem}
	If $X_{n}\xrightarrow{P}X$, then $X_{n}\xrightarrow{D}X$. The converse is not necessarily true.
\end{rem}

\section{Limit Theorems}
Using the last two theorems, the next two theorems are very useful in both statistics and probability theory by giving us approximate distribution of an average without a lot of distributional assumption.
\begin{thm}\named{Weak Law of Large Numbers (WLLN)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables. Let $\E(X_{i})=\mu$ for all $i=1,2,\cdots$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$
	\begin{equation*}
		\overline{X}\xrightarrow{D}\mu
	\end{equation*}
\end{thm}
\begin{thm}\named{Classical Central Limit Theorem (CLT)}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables whose MGFs exist in a neighbourhood of $0$. Let $\E(X_{i})=\mu$ and $\Var(X_{i})=\sigma^{2}>0$ for all $i=1,2,\cdots$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\to\N(0,1)
	\end{equation*}
\end{thm}
\begin{rem}
	This is just an abuse of notations.
\end{rem}
This works generally to most of the distribution. However, it is probably very tedious to find the MGF. We cam apply the following version of CLT instead.
\begin{thm}\named{L\'evy-Linderberg Central Limit Theorem}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables with common population means $\mu$ and common population variance $\sigma^{2}$. Assume that $0<\sigma^{2}<\infty$. Define by $\overline{X}$ the sample mean of the random variables. Then as $n\to\infty$,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=\frac{\sum_{i=1}^{n}X_{i}-n\mu}{\sqrt{n}\sigma}\to\N(0,1)
	\end{equation*}
\end{thm}

\newpage
Sometimes, we will deal with a function of multiple random variables. We must establish how they converges.
\begin{thm}\named{Slutsky's Theorem}
	If $X_{n}\xrightarrow{D}X$ and $Y_{n}\xrightarrow{\prob}c$, then:
	\begin{enumerate}
		\item $X_{n}+Y_{n}\xrightarrow{D}X+c$
		\item $X_{n}Y_{n}\xrightarrow{D}cX$
		\item $\frac{X_{n}}{Y_{n}}\xrightarrow{D}\frac{X}{c}$ if $c\neq 0$.
	\end{enumerate}
\end{thm}
\begin{eg}
	Assume that $X_{i}\sim\Bern(p)$ for all $i$. We want to estimate the unknown $p$. We have common mean $\mu=p$ and common variance $\sigma^{2}=p(1-p)$. By applying CLT, as $n\to\infty$,
	\begin{equation*}
		\overline{X}\to\N\left(p,\frac{p(1-p)}{n}\right)
	\end{equation*}
	Therefore, we can use normal distribution to approximate the unknown parameter. We want an estimation that we can confident about, and commonly we use probability $0.95$.
	\begin{equation*}
		0.95=\prob\left(-z_{0.025}\leq\frac{\overline{X}-p}{\sqrt{\frac{p(1-p)}{n}}}\leq z_{0.025}\right)=\prob\left((\overline{X}-p)^{2}\leq z_{0.025}^{2}\frac{p(1-p)}{n}\right)
	\end{equation*} 
	Solving the inequality, we would find an interval that estimates the parameter $p$. However, this is highly inconvenient. We may use another method. Let us replace $\sqrt{\frac{p(1-p)}{n}}$ with $\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}$. As $n\to\infty$,
	\begin{equation*}
		\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}=\sqrt{\frac{p(1-p)}{n}}\sqrt{\frac{p(1-p)}{\overline{X}(1-\overline{X})}}\to\sqrt{\frac{p(1-p)}{n}}
	\end{equation*}
	since by Slutsky's Theorem, $\sqrt{\frac{p(1-p)}{\overline{X}(1-\overline{X})}}\to 1$ as $\overline{X}\to p$ by WLLN. We have:
	\begin{equation*}
		0.95=\prob\left(-z_{0.025}\leq\frac{\overline{X}-p}{\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}}\leq z_{0.025}\right)=\prob\left(\overline{X}-z_{0.025}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\leq p\leq\overline{X}+z_{0.025}\sqrt{\frac{\overline{X}(1-\overline{X})}{n}}\right)
	\end{equation*} 
\end{eg}
\begin{eg}
	In a survey before an election, a poll was taken of $300$ potential voters. Among them, $120$ said that they would vote for candidate A. Determine a $95\%$ confidence interval for the population proportion $p_{A}$ of voters who would vote for candidate A in the election.\\
	From the poll, we have a point estimate $\overline{x}=\hat{p}_{A}=\frac{120}{300}=0.4$. From the last example, we have found that the $95\%$ confidence interval is:
	\begin{equation*}
		\left(\overline{x}-z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}},\overline{x}+z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}\right)\approx(0.3446,0.4554)
	\end{equation*}
	Equivalently, the percentage of voters of candidate A would be from $34.46\%$ to $45.54\%$, with a margin of error $5.54\%$.
\end{eg}

\newpage
\begin{eg}
	Following the previous example. Assume that we have been given a margin of error $D$. How many data should we collect in order to to have the margin of error?\\
	From how we find the margin of error:
	\begin{equation*}
		z_{0.025}\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}=D\implies n=p(1-p)\frac{z_{0.025}^{2}}{D^{2}}
	\end{equation*}
	Since $p(1-p)\leq\frac{1}{4}$, if we specify that $D=0.05$, we have:
	\begin{equation*}
		n\leq\frac{z_{0.025}^{2}}{4D^{2}}=\frac{1.96^{2}}{4(0.05)^{2}}\leq\frac{2^{2}}{4(0.05)^{2}}=400
	\end{equation*}
	We may use this to determine whether we have obtained enough data.\\
	Assume that we have $n^{*}$ respondents. Is it enough? The number of required respondents is obtained by:
	\begin{equation*}
		n_{\text{required}}=\frac{\overline{x^{*}}(1-\overline{x^{*}})z_{0.025}^{2}}{D^{2}}
	\end{equation*}
	If $n^{*}<n_{\text{required}}$, then the current number of data is not enough. We would need to find more respondents.\\
	If $n^{*}\geq n_{\text{required}}$, then it is enough.
\end{eg}
\begin{eg}
	We try to use Poisson random variables to prove that as $n\to\infty$,
	\begin{equation*}
		e^{-n}\sum_{k=0}^{n}\frac{n^{k}}{k!}\to\frac{1}{2}
	\end{equation*}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Poisson(1)$ for $i=1,2,\cdots$. Let $Y_{n}=\sum_{i=1}^{n}X_{i}$. By CLT, we have:
	\begin{equation*}
		\frac{Y_{n}-n}{\sqrt{n}}\to\N(0,1)
	\end{equation*}
	Therefore, since $Y_{n}\sim\Poisson(n)$, we have:
	\begin{equation*}
		e^{-n}\sum_{k=0}^{n}\frac{n^{k}}{k!}=\prob(Y_{n}\leq n)=\prob(\frac{Y_{n}-n}{\sqrt{n}}\leq 0)\to\frac{1}{2}
	\end{equation*}
\end{eg}
\begin{eg}
	Given a sequence of i.i.d. random variables $\{X_{n}\}$. We want to find the asymptotic distribution for the $k$-th sample moment $\overline{X^{k}}$ as $n\to\infty$. Notice that $X_{i}^{k}$ are independent for $i=1,2,\cdots$. By CLT,
	\begin{equation*}
		\frac{\sqrt{n}(\overline{X^{k}}-\mu_{k}')}{\sqrt{\mu_{2k}'-(\mu_{k}')^{2}}}\to\N(0,1)
	\end{equation*}
	Therefore, the asymptotic distribution for $\overline{X^{k}}$ when $n\to\infty$ is $\N(\mu_{k}',\frac{1}{n}(\mu_{2k}'-(\mu_{k}')^{2}))$.
\end{eg}
Central Limit Theorem can provide us a limiting standard normal distribution of sample mean. However, we usually deal with some functions of the sample mean.
\begin{thm}\named{Continuous mapping theorem}
	Let $\{X_{n}\}$ be a sequence of random variables and $X$ be a random variable. Suppose there is a function $g$ has a set of discontinuity points $D_{g}$ such that $\prob(X\in D_{g})=0$, then:
	\begin{enumerate}
		\item If $X_{n}\xrightarrow{D}X$, then $g(X_{n})\xrightarrow{D}g(X)$.
		\item If $X_{n}\xrightarrow{\prob}X$, then $g(X_{n})\xrightarrow{\prob}g(X)$.
	\end{enumerate}
\end{thm}
\begin{thm}\named{Delta method}
	Let $\{X_{n}\}$ be a sequence of random variables such that for constants $a$ and $b>0$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(X_{n}-a)\to\N(0,b^{2})
	\end{equation*}
	Then for a given function $g$, suppose that $g'(a)$ exists and not $0$, as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(g(X_{n})-g(a))\to\N(0,[g'(a)b]^{2})
	\end{equation*}
\end{thm}
\begin{cor}
	\label{Chapter 1 (Corollary) CLT for functions of random variables}
	If $\overline{X}$ is the sample mean of a random sample $X_{1},\cdots,X_{n}$ of size $n$ from a distribution with a finite mean $\mu$ and finite variance $\sigma^{2}>0$. For a given function $g$, suppose that $g'(\mu)$ exists and is not $0$, as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\mu))\to\N(0,[g'(\mu)\sigma]^{2})
	\end{equation*}  
\end{cor}
\begin{eg}
	Assume that there are $70$ respondents, $68$ of which would vote for one candidate.\\
	If we use the same process from previous examples, we find that the $95\%$ confidence interval is $(0.9324,1.0105)$, which is out of the range. In fact, if the point estimate $\hat{p}$ is quite close to $0$ or $1$, the resulting interval guess may include values that is out of the range of $p$. This is a poor interval guess.\\
	We take a transformation, say $g(p)$, such that $g(p)\in(-\infty,\infty)$. We may find that since $0<p<1$, $\ln{p}<0$. Therefore, we can find that:
	\begin{equation*}
		g(p)=\ln(-\ln{p})\in(-\infty,\infty)
	\end{equation*}
	By Delta method,
	\begin{equation*}
		\frac{g(\overline{X})-g(p)}{g'(p)\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}}\to\N(0,1)
	\end{equation*}
	By WLLN and continuous mapping theorem, we can replace $g'(p)$ with $g'(\overline{X})$. Therefore, we have:
	\begin{align*}
		0.95=\prob\left(-z_{0.025}\leq	\frac{g(\overline{X})-g(p)}{g'(p)\sqrt{\frac{\overline{x}(1-\overline{x})}{n}}}\leq z_{0.025}\right)
	\end{align*}
	Solving the formula and we would get the a good $95\%$ confidence interval of $p$.
\end{eg}
\begin{eg}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Bern(\theta)$ for $i=1,2,\cdots$. Show that:
	\begin{equation*}
		Z_{n}=2\sqrt{n}\left(\sin^{-1}{\sqrt{\overline{X}}}-\sin^{-1}{\sqrt{\theta}}\right)\to\N(0,1)
	\end{equation*}
	Let $g(x)=\sin^{-1}{\sqrt{x}}$. We may obtain that:
	\begin{equation*}
		g'(x)=\frac{1}{2\sqrt{x}\sqrt{1-x}}
	\end{equation*}
	We can find that the derivative is well-defined and non-zero for $0<\theta<1$ by substituting $x=\theta$. Note that $\E(X_{i})=\theta$ and $\Var(X_{i})=\theta(1-\theta)$ for $i=1,\cdots,n$. By Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables},
	\begin{equation*}
		\sqrt{n}(g(\overline{X})-g(\theta))\to\N\left(0,\frac{1}{4}\right)
	\end{equation*}
	Since $Z_{n}=2\sqrt{n}(g(\overline{X})-g(\theta))$, we may find that as $n\to\infty$,
	\begin{equation*}
		Z_{n}\to\N(0,1)
	\end{equation*}
\end{eg}
\begin{eg}
	Let $\{X_{n}\}$ be a sequence of i.i.d. random variables and $X_{i}\sim\Exp(\theta)$ for $i=1,2,\cdots$. We want to find a variance-stabilizing transformation, which is to find a function $g(x)$ such that the limiting distribution of:
	\begin{equation*}
		Y_{n}=\sqrt{n}[g(\overline{X_{n}})-g(\theta)]
	\end{equation*}
	does not depend on $\theta$.\\
	We may find that $\E(X_{i})=\frac{1}{\theta}$ and $\Var(X_{i})=\frac{1}{\theta^{2}}$ for $i=1,2,\cdots$. We claim that $g(x)=\ln{x}$ is what we want. We can find that:
	\begin{equation*}
		g'(x)=\frac{1}{x}
	\end{equation*}
	By substituting $x=\frac{1}{\theta}$, we can see that the derivative is non-zero. Applying Corollary \ref{Chapter 1 (Corollary) CLT for functions of random variables},
	\begin{equation*}
		\sqrt{n}\left(g(\overline{X})-g\left(\frac{1}{\theta}\right)\right)\to\N(0,1)
	\end{equation*}
	Therefore, $g(x)=\ln{x}$ is the variance-stabilizing transformation.
\end{eg}

\newpage
However, we usually deal with more than 1 variables. Before we extend the theorems into multivariate case, we must first introduce the multivariate normal distribution.
\begin{eg}\named{Multivariate Normal Distribution} $\mathbf{X}\sim\N_{k}(\boldsymbol{\mu},\mathbf{\Sigma})$\\
	Given a random vector $\mathbf{X}$. Let the $k\times 1$ vector $\boldsymbol{\mu}$ be the expected value of $\mathbf{X}$ and the $k\times k$ matrix $\mathbf{\Sigma}$ be its variance-covariance matrix. Assume that $\mathbf{\Sigma}$ is positive-definite. (For all non-zero vectors $\mathbf{z}$ with real entries, we have $\mathbf{z}^{T}\mathbf{\Sigma z}>0$) The random vector $\mathbf{X}$ is $k$-dimensional normal if its PDF is:
	\begin{equation*}
		f_{\mathbf{X}}(\mathbf{x})=(2\pi)^{-\frac{k}{2}}\abs{\mathbf{\Sigma}}^{-\frac{1}{2}}e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}\mathbf{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}
	\end{equation*}
\end{eg}
\begin{rem}
	The i-th row and j-th column of the $k\times k$ variance-covariance matrix $\mathbf{\Sigma}$ is the element $a_{ij}$ found by:
	\begin{equation*}
		a_{ij}=\cov(X_{i},X_{j})
	\end{equation*}
	Note that if $i=j$, then $\cov(X_{i},X_{i})=\Var(X_{i})$.
\end{rem}
\begin{eg}
	If $k=2$, then $X\sim\N_{2}(\boldsymbol{\mu},\mathbf{\Sigma})$ is bivariate normal.
\end{eg}
\begin{lem}
	\label{Chapter 1 (Lemma) Distribution of matrix multiplication with random vector}
	If $\mathbf{X}\sim\N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$, then for any $q\times p$ matrix $\mathbf{A}$, we have:
	\begin{equation*}
		\mathbf{AX}\sim\N_{q}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\Sigma A}^{T})
	\end{equation*}
\end{lem}
\begin{eg}
	Using this lemma, one can isolate some of the random variables that made up the random vector $\mathbf{X}=(X_{1}\ \cdots\ X_{p})^{T}\sim\N_{p}(\boldsymbol{\mu},\mathbf{\Sigma})$. For example, setting $(p-1)\times p$ matrix $\mathbf{A}$ as:
	\begin{equation*}
		\mathbf{A}=\begin{pmatrix}
			0 & 1 & 0 & \hdots & 0\\
			\vdots & \ddots & \ddots & \ddots & \vdots\\
			\vdots & & \ddots & \ddots & 0\\
			0 & \hdots & \hdots & 0 & 1
		\end{pmatrix}=\begin{pmatrix}\begin{array}{c|c}
			0 & \\
			\vdots & \mathbf{I}_{(p-1)\times(p-1)}\\
			0 &
		\end{array}\end{pmatrix}
	\end{equation*}
	We may find that:
	\begin{equation*}
		\mathbf{AX}=\begin{pmatrix}
		X_{2}\\
		\vdots\\
		X_{p}
		\end{pmatrix}\sim\N_{p-1}(\mathbf{A}\boldsymbol{\mu},\mathbf{A\Sigma A}^{T})
	\end{equation*}
	where $\mathbf{A}\boldsymbol{\mu}$ is the mean vector of $(X_{2}\ \cdots\ X_{p})^{T}$ and $\mathbf{A\Sigma A}^{T}$ is the variance-covariance matrix of $(X_{2}\ \cdots\ X_{p})^{T}$.
\end{eg}
\begin{lem}
	\label{Chapter 1 (Lemma) Independence iff uncorrelated and bivariate}
	If we have:
	\begin{equation*}
		\begin{pmatrix}X_{1}\\X_{2}\end{pmatrix}\sim\N_{2}\left(\begin{pmatrix}\mu_{1}\\\mu_{2}\end{pmatrix},\begin{pmatrix}\sigma_{11}&\sigma_{12}\\\sigma_{21}&\sigma_{22}\end{pmatrix}\right),
	\end{equation*}
	then $X_{1}$ and $X_{2}$ are independent if and only if $\sigma_{12}=\sigma_{21}=0$.
\end{lem}
\begin{proofing}
	From the properties of covariance,
	\begin{equation*}
		\sigma_{12}=\cov(X_{1},X_{2})=\cov(X_{2},X_{1})=\sigma_{21}
	\end{equation*}
	Assume that $X_{1}$ and $X_{2}$ are independent. We have:
	\begin{equation*}
		\cov(X_{1},X_{2})=\E[(X_{1}-\E(X_{1}))(X_{2}-\E(X_{2}))]=\E(X_{1}X_{2})-\E(X_{1})\E(X_{2})=0
	\end{equation*}
	Therefore, $\sigma_{12}=\sigma_{21}=0$.\\
	Assume that $\sigma_{12}=\sigma_{21}=0$. Then $\E(X_{1}X_{2})=\E(X_{1})\E(X_{2})$. We check joint PDF to see if they are independent.\\
	Since $\cov(X_{1},X_{2})=0$, we have that:
	\begin{equation*}
		f_{X_{1},X_{2}}(x_{1},x_{2})=\frac{1}{2\pi\sigma_{11}\sigma_{22}}\exp\left(-\frac{1}{2}\left(\frac{(x_{1}-\mu_{1})^{2}}{2\sigma_{11}^{2}}+\frac{(x_{2}-\mu_{2})^{2}}{2\sigma_{22}^{2}}\right)\right)
	\end{equation*}
\end{proofing}
\begin{rem}
	Two random variables are uncorrelated does not mean they are independent. It is only true if they are bivariate normal.
\end{rem}

\newpage
We may extend the CLT to multivariate case.
\begin{thm}\named{Multivariate Central Limit Theorem}
	Let $\{\mathbf{X}_{n}=(X_{n1}\ \cdots\ X_{nk})^{T}\in\mathbb{R}^{k}\}$ be a sequence of i.i.d. random vectors with variance-covariance matrix $\mathbf{\Sigma}$. We assume that $\E(X_{ij}^{2})<\infty$ for $i=1,2,\cdots$ and $j=1,\cdots,k$. Define by $\mathbf{\overline{X}}$ the sample mean of the random vectors. Then as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\mathbf{\overline{X}}-\boldsymbol{\mu})\xrightarrow{D}\N_{k}(\mathbf{0},\mathbf{\Sigma})
	\end{equation*}
\end{thm}
We may extend the Delta method to multivariate cases.
\begin{thm}\named{Multivariate 1st-order Delta Method}
	Let $\{\mathbf{X}_{n}\in\mathbb{R}^{k}\}$ be a sequence of random vectors such that for constant vector $\mathbf{a}\in\mathbb{R}^{k}$, as $n\to\infty$,
	\begin{equation*}
		\sqrt{n}(\mathbf{X}_{n}-\mathbf{a})\xrightarrow{D}\mathbf{U}
	\end{equation*}
	where $\mathbf{U}$ is a random vector in $\mathbb{R}^{k}$. If a function $h:\mathbb{R}^{k}\to\mathbb{R}$ has a derivative $\nabla h(\mathbf{a})\neq\mathbf{0}$, then as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(h(\mathbf{X}_{n})-h(\mathbf{a}))\xrightarrow{D}\nabla h(\mathbf{a})\mathbf{U}
	\end{equation*}
	where
	\begin{equation*}
		\nabla h=\left(\pdv*[order=1]{h(t_{1},\cdots,t_{k})}{t_{1}},\cdots,\pdv*[order=1]{h(t_{1},\cdots,t_{k})}{t_{k}}\right)
	\end{equation*}
\end{thm}

\chapter{Point Estimation}
In this chapter, we will study two different general approaches to estimate an unknown parameters of any given parametric distribution.\\
The basic idea of point estimation is that we use a statistic $T$, an estimate $T(\mathbf{x})$ or an estimator $T(\mathbf{X})$ to estimate the unknown parameter $g(\theta)$, where $\mathbf{x}=(x_{1}\ \cdots\ x_{n})^{T}$ is a realization of random vector $\mathbf{X}=(X_{1}\ \cdots\ X_{n})^{T}$ with a PDF $f(x|\theta)$ or PMF $p(x|\theta)$ and $\theta$ in parameter space $\Theta$.
\begin{rem}
	Most often, the parameters of our interest to be estimated (\textbf{estimand}) is a function of the unknown distribution parameters $\theta$. E.g. $\mu^{2},\frac{\sigma}{\mu}$.
\end{rem}
\begin{rem}
	We only estimate an unknown parameters. There is no point to estimate an already known parameters.
\end{rem}
\begin{defn}
	An estimator or estimate $\hat{\theta}$ is \textbf{unbiased} or \textbf{mean-unbiased} for $\theta$ if $\E(\hat{\theta})=\theta$.
\end{defn}
\section{Methods of Moments Estimation}
Methods of moments estimation is one of the most popular methods in statistics to estimate an unknown parameters. As the name suggests, it is related to moments. The motivation is that in some situations, the parameter of interest can be written as a function of population moments about $0$.
\begin{defn}
	Suppose that there are $k$ unknown parameters $\theta_{1},\cdots,\theta_{k}$. If we can write them in terms of $k$ or more moments, i.e.:
	\begin{equation*}
		\begin{cases}
			\theta_{1}=g_{1}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)\\
			\theta_{2}=g_{2}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)\\
			\vdots\\
			\theta_{k}=g_{k}(\mu_{1}',\mu_{2}',\cdots,\mu_{k}',\cdots)
		\end{cases}
	\end{equation*}
	then the \textbf{method of moments estimator} (MME), denoted by $(\widetilde{\theta}_{1},\widetilde{\theta}_{2},\cdots,\widetilde{\theta}_{k})$, of $(\theta_{1},\theta_{2},\cdots,\theta_{k})$ is:
	\begin{equation*}
		\begin{cases}
			\widetilde{\theta}_{1}=g_{1}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)\\
			\widetilde{\theta}_{2}=g_{2}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)\\
			\vdots\\
			\widetilde{\theta}_{k}=g_{k}(\overline{X},\overline{X^{2}},\cdots,\overline{X^{k}},\cdots)
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	This estimation is quick and easy, but the MME obtained are often biased and heavily relies on the existence of the required population moments.
\end{rem}
\begin{rem}
	Do not mix up method of moment estimator or method of moment estimate.
\end{rem}
\begin{rem}
	Do not write the MME as $(\theta_{1},\theta_{2},\cdots,\theta_{k})$. This is wrong.
\end{rem}

\newpage
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\N(10,\sigma^{2})$. We want to estimate $\sigma^{2}$.\\
	We have $k=1$, $\theta_{1}=\sigma^{2}$. We can write it in terms of moments:
	\begin{equation*}
		\sigma^{2}=\E(X^{2})-100
	\end{equation*}
	Therefore, the MME of $\sigma^{2}$ is:
	\begin{equation*}
		\widetilde{\sigma}^{2}=\overline{X^{2}}-100
	\end{equation*}
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\N(\mu,\sigma^{2})$. We want to estimate $\mu$ and $\sigma^{2}$.\\
	We have $k=2$, $(\theta_{1},\theta_{2})=(\mu,\sigma^{2})$. We can write them in terms of moments: 
	\begin{equation*}
		\begin{cases}
			\mu=E(X)\\
			\sigma^{2}=\E(X^{2})-[\E(X)]^{2}
		\end{cases}
	\end{equation*}
	Therefore, the MME of $\mu$ and $\sigma^{2}$ are:
	\begin{equation*}
		\begin{cases}
			\widetilde{\mu}=\overline{X}\\
			\widetilde{\sigma}^{2}=\overline{X^{2}}-(\overline{X})^{2}=\frac{1}{n}\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}
		\end{cases}
	\end{equation*}
\end{eg}
\begin{rem}
	MME may not be unique because the parameter can be written as different functions of moments. To fix this problem, we usually prefer using fewer or lower moments to get MME.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Poisson(\lambda)$. We want to estimate $\lambda$. We have $k=1$, $\theta_{1}=\lambda$. We have multiple ways to write it in terms of moments. It can be $\lambda=\E(X)$, $\lambda=\E(X^{2})-[\E(X)]^{2}$ or any other combinations. From the remark, we would choose the one with fewer or lower moments, which is:
	\begin{equation*}
		\lambda=\E(X)
	\end{equation*}
	Therefore, the MME of $\lambda$ is:
	\begin{equation*}
		\lambda=\overline{X}
	\end{equation*}
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Gam(\alpha,\beta)$. Assume that we know that $\E(X)=3423$. We have $k=2$, $(\theta_{1},\theta_{2})=(\alpha,\beta)$. We can write them in terms of moments:
	\begin{equation*}
		\begin{cases}
			3423=\frac{\alpha}{\beta}\\
			\E(X^{2})=\frac{\alpha}{\beta^{2}}+3423^{2}
		\end{cases}\implies\begin{cases}
			\alpha=\frac{3423^{2}}{\E(X^{2})-3423^{2}}\\
			\beta=\frac{3423}{\E(X^{2})-3423^{2}}
		\end{cases}
	\end{equation*}
	Therefore, the MME of $\alpha$ and $\beta$ is:
	\begin{equation*}
		\begin{cases}
			\widetilde{\alpha}=\frac{3423^{2}}{\overline{X^{2}}-3423^{2}}\\
			\widetilde{\beta}=\frac{3423}{\overline{X^{2}}-3423^{2}}
		\end{cases}
	\end{equation*}
\end{eg}
\begin{lem}\named{Invariance property of MME}
	If $\widetilde{\theta}_{i}$ is the MME for $\theta_{i}$ for $i=1,\cdots,k$, then $h(\widetilde{\theta}_{1},\cdots,\widetilde{\theta}_{k})$ is the MME for $h(\theta_{1},\cdots,\theta_{k})$, where $h$ is a known function.
\end{lem}
\begin{thm}
	\label{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}
	A sequence of MME $\{\widetilde{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent and asymptotically unbiased for $\theta$. It is also asymptotically normally distributed. More precisely, under certain assumption like $\E\abs{X}^{2k}<\infty$, as $n\to\infty$, we have:
	\begin{equation*}
		\sqrt{n}(\widetilde{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathbf{GHG}^{T})
	\end{equation*}
	where $\mathbf{G}$ is a $k\times k$ matrix with $\pdv{g_{i}}{\mu_{j}'}$ as its $(i,j)$-th entry and $\mathbf{H}$ is a $k\times k$ matrix with $\mu_{i+j}'-\mu_{i}'\mu_{j}'$ as its $(i,j)$-th entry, for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{rem}
	In the theorem, "consistent" means convergence in probability. For any $\varepsilon>0$, as $n\to\infty$,
	\begin{equation*}
		\prob(|\widetilde{\theta}_{n}-\theta|>\varepsilon)\to 0
	\end{equation*}
\end{rem}

\newpage
\begin{rem}
	Also in the theorem, "asymptotically unbiased" means that we have:
	\begin{equation*}
		\lim_{n\to\infty}\E(\widetilde{\theta}_{n})=\theta
	\end{equation*}
	Note that $\E(\widetilde{\theta}_{n})\neq\theta$ for some $n$.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from a random variable $X$ with $\E\abs{X}^{4}<\infty$. We take:
	\begin{equation*}
		\theta=\begin{pmatrix}
			\mu\\ \sigma^{2}
		\end{pmatrix}=\begin{pmatrix}
			\mu_{1}'\\ \mu_{2}'-(\mu_{1}')^{2}
		\end{pmatrix}
	\end{equation*}
	We have:
	\begin{align*}
		\mathbf{G}&=\begin{pmatrix}
			1 & 0\\
			-2\mu_{1}' & 1
		\end{pmatrix} & \mathbf{H}&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
			\mu_{3}'-\mu_{2}'\mu_{1}' & \mu_{4}'-(\mu_{2}')^{2}
		\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		\mathbf{GHG}^{T}&=\begin{pmatrix}
			1 & 0\\
			-2\mu_{1}' & 1
		\end{pmatrix}\begin{pmatrix}
		\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
		\mu_{3}'-\mu_{2}'\mu_{1}' & \mu_{4}'-(\mu_{2}')^{2}
		\end{pmatrix}\mathbf{G}^{T}\\
		&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-\mu_{1}'\mu_{2}'\\
			\mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3} & \mu_{4}'-2\mu_{1}'\mu_{3}'-(\mu_{2}')^{2}+2(\mu_{1}')^{2}\mu_{2}'
		\end{pmatrix}\begin{pmatrix}
			1 & -2\mu_{1}'\\
			0 & 1
		\end{pmatrix}\\
		&=\begin{pmatrix}
			\mu_{2}'-(\mu_{1}')^{2} & \mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3}\\
			\mu_{3}'-3\mu_{1}'\mu_{2}'+2(\mu_{1}')^{3} & \mu_{4}'-4\mu_{1}'\mu_{3}'-(\mu_{2}')^{2}+8\mu_{2}'(\mu_{1}')^{2}-4(\mu_{1}')^{4}
		\end{pmatrix}
	\end{align*}
	Using the fact that:
	\begin{align*}
		\mu_{3}&=\mu_{3}'-3\mu_{2}'\mu_{1}'+2(\mu_{1}')^{3}\\ \mu_{4}&=\mu_{4}'-4\mu_{3}'\mu_{1}'+6\mu_{2}'(\mu_{1}')^{2}-3(\mu_{1}')^{4}\\
		\sigma^{4}&=(\mu_{2}')^{2}-2\mu_{2}'(\mu_{1}')^{2}+(\mu_{1}')^{4}
	\end{align*}
	We can find the resultant matrix:
	\begin{equation*}
		\mathbf{GHG}^{T}=\begin{pmatrix}
			\sigma^{2} & \mu_{3}\\
			\mu_{3} & \mu_{4}-\sigma^{4}
		\end{pmatrix}
	\end{equation*}
	Using Theorem \ref{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}, denote:
	\begin{equation*}
		\widetilde{\theta}_{n}=\begin{pmatrix}
			\overline{X}_{n}\\ S_{n}^{2}
		\end{pmatrix}
	\end{equation*}
	As $n\to\infty$,
	\begin{equation*}
		\sqrt{n}\left(\begin{pmatrix}
			\overline{X}_{n}\\ S_{n}^{2}
		\end{pmatrix}-\begin{pmatrix}
			\mu\\ \sigma^{2}
		\end{pmatrix}\right)\to\N_{2}\left(\begin{pmatrix}
			0\\ 0
		\end{pmatrix},\begin{pmatrix}
			\sigma^{2} & \mu_{3}\\
			\mu_{3} & \mu_{4}-\sigma^{4}
		\end{pmatrix}\right)
	\end{equation*}
	Based on the properties of variance-covariance matrix, we may find that as $n\to\infty$:
	\begin{equation*}
		\sqrt{n}(S_{n}^{2}-\sigma^{2})\to\N(0,\mu_{4}-\sigma^{4})
	\end{equation*}
	By Delta Method, in condition that $\sigma^{2}>0$,
	\begin{equation*}
		\sqrt{n}(S_{n}-\sigma)\to\N\left(0,\frac{\mu_{4}-\sigma^{4}}{4\sigma^{2}}\right)
	\end{equation*}
\end{eg}

\newpage
\section{Maximum Likelihood Estimation}
The method of maximum likelihood is by far the most popular technique for deriving estimators, popularized by Ronald Aylmer Fisher in 1922. Currently, there are still a lot of research studying the properties of this estimation method.
\begin{defn}
	Consider a random sample of size $n$ from a population with a PDF $f(\mathbf{x}|\theta)$ or a PMF $p(\mathbf{x}|\theta)$. Given a realization $\mathbf{x}=(x_{1}\ \cdots\ x_{n})^{T}$. The \textbf{likelihood function} is defined by:
	\begin{equation*}
		L(\theta)=L(\theta_{1},\cdots,\theta_{k}|\mathbf{x})=\begin{cases}
			\prod_{i=1}^{n}f(x_{i}|\theta), &\text{Continuous case}\\
			\prod_{i=1}^{n}p(x_{i}|\theta), &\text{Discrete case}
		\end{cases}
	\end{equation*}
	The likelihood function can be used to quantify how the observed data is likely to occur.
\end{defn}
\begin{rem}
	The likelihood function $L(\theta)$ is a function of $\theta$ with fixed $\mathbf{x}$.
\end{rem}
\begin{rem}
	Do not replace $x_{i}$ with $x$.
	\begin{equation*}
		L(\theta)=\begin{cases}
			\prod_{i=1}^{n}f(x_{i}|\theta)\neq\prod_{i=1}^{n}f(x|\theta), &\text{Continuous case}\\
			\prod_{i=1}^{n}p(x_{i}|\theta)\neq\prod_{i=1}^{n}p(x|\theta), &\text{Discrete case}
		\end{cases}
	\end{equation*}
\end{rem}
The idea is that for each realization of $\mathbf{x}$, we want to estimate a value of $\theta\in\Theta$ at which $L(\theta)$ attains its maximum.
\begin{defn}
	The \textbf{maximum likelihood estimate} (MLE), denoted by $\hat{\theta}$, is obtained by:
	\begin{equation*}
		\hat{\theta}=\argmax_{\theta\in\Theta}L(\theta)
	\end{equation*}
\end{defn}
\begin{rem}
	In some cases, especially when differentiation is used, it is easier to work with \textbf{log likelihood} defined by:
	\begin{equation*}
		l(\theta)=\log{L(\theta)}
	\end{equation*}
	We can do this because $l(\theta)$ and $L(\theta)$ are strictly increasing and they have the same maxima. 
\end{rem}
\begin{eg}
	Consider a random sample of size $n=10$ from $\Bern(\theta)$, where $\theta$ is unknown. Therefore,
	\begin{equation*}
		L(\theta)=\prod_{i=1}^{n}p(x_{i}|\theta)=\theta^{n\overline{x}}(1-\theta)^{n-n\overline{x}}
	\end{equation*}
	Suppose that there are only two possible values of $\theta$. We have either $\theta=0.1$ or $\theta=0.5$.\\
	From the data observed, assumed that we have found that $\overline{x}=0.4$. Substituting gets:
	\begin{align*}
		L(0.1)&=(0.1)^{4}(0.9)^{6}=0.0000531441 & L(0.5)&=(0.5)^{4}(0.5)^{6}=0.0009765625
	\end{align*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=0.5$.
\end{eg}
\begin{eg}
	In the case when $L(\theta)$ is differentiable on the interior of $\theta$. One possible way of finding an MLE of $\theta=(\theta_{1}\ \cdots\ \theta_{k})^{T}$ is to solve the first order equation for $i=1,\cdots,k$:
	\begin{equation*}
		\pdv*{L(\theta)}{\theta_{i}}=0\text{ or }\pdv*{l(\theta)}{\theta_{i}}=0
	\end{equation*}
	and check all the extrema.
\end{eg}
\begin{rem}
	Solving the first-order likelihood only gives you the maximum at critical points. You need to also check the extreme values too.
\end{rem}

\newpage
\begin{eg}
	Consider a random sample of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. We may obtain the log likelihood:
	\begin{equation*}
		l(\theta)=\ln\left(\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}(x_{i}-\theta)^{2}}\right)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\theta)^{2}-\frac{n}{2}\ln(2\pi)
	\end{equation*}
	We find the critical points by solving:
	\begin{equation*}
		0=\pdv*{l(\theta)}{\theta}=\sum_{i=1}^{n}(x_{i}-\theta)
	\end{equation*}
	This has the solution $\hat{\theta}=\overline{x}$. To check that the solution is in fact a global maximum, we can check that:
	\begin{equation*}
		\pdv*[order={2}]{l(\theta)}{\theta}=-n<0
	\end{equation*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=\overline{x}$.
\end{eg}
\begin{eg}
	Continues the previous example. Alternatively, we may find that for any $\theta\in\Theta$,
	\begin{equation*}
		\sum_{i=1}^{n}(x_{i}-\theta)^{2}\geq\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}
	\end{equation*}
	Thus, for any $\theta\in\Theta$,
	\begin{equation*}
		L(\theta)\leq L(\overline{x})
	\end{equation*}
	Therefore, the MLE of $\theta$ is $\hat{\theta}=\overline{x}$.
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. Previously, we have found that $\hat{\theta}=\overline{x}$, which maximize the log likelihood. Let us restrict that $\theta\geq 0$.\\
	If $\overline{x}\geq 0$, then it satisfies the constraint $\theta\geq 0$. Therefore, the MLE would be:
	\begin{equation*}
		\hat{\theta}=\overline{x}
	\end{equation*}
	If $\overline{x}<0$, then it does not satisfy the constraint $\mu\geq 0$. We analyse the log likelihood again:
	\begin{equation*}
		l(\theta)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\theta)^{2}-\frac{n}{2}\ln(2\pi)=-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}-\frac{n}{2}(\overline{x}-\theta)-\frac{n}{2}\ln(2\pi)
	\end{equation*}
	The term $(\overline{x}-\theta)^{2}$ is minimized while satisfying the constraint is when $\theta=0$.\\
	Therefore, if we restrict $\theta\geq 0$, the MLE of $\theta$ would be:
	\begin{equation*}
		\hat{\theta}=\max(\overline{x},0)
	\end{equation*}
\end{eg}
\begin{rem}
	Remember, when we estimate a parameter, we must use the data we have obtained.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $\U[0,\theta]$, where $\theta\in(0,\infty)$ is unknown. The likelihood function is:
	\begin{equation*}
		L(\theta)=\frac{1}{\theta^{n}}\mathbf{1}_{0\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq\theta}
	\end{equation*}
	where $x_{(i)}$ represents the $i$-th smallest data for $i=1,\cdots,n$. Therefore, the MLE is:
	\begin{equation*}
		\hat{\theta}=x_{(n)}
	\end{equation*}
\end{eg}
\begin{rem}
	MLE may be biased and it may not exist in $\Theta$, especially when $\Theta$ is an open set.
\end{rem}

\newpage
\begin{rem}
	MLE defined may not be unique.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from $\U[\theta-1,\theta+1]$, where $\theta$ is unknown. The likelihood function is:
	\begin{equation*}
		L(\theta)=\frac{1}{2^{n}}\mathbf{1}_{\theta-1\leq x_{(1)}\leq\cdots\leq x_{(n)}\leq\theta+1}=\frac{1}{2^{n}}\mathbf{1}_{x_{(n)}-1\leq\theta\leq x_{(1)}+1}
	\end{equation*}
	where $x_{(i)}$ represents the $i$-th smallest data for $i=1,\cdots,n$. We may find that any estimates in $[x_{(n)}-1,x_{(1)}+1]$ maximize $L(\theta)$. Therefore, there are infinitely many MLEs of $\theta$.
\end{eg}
\begin{lem}\named{Invariance property of MLE}
	If $\hat{\theta}_{i}$ is the MLE of $\theta_{i}$ for $i=1,\cdots,k$, then $h(\hat{\theta}_{1},\cdots,\hat{\theta}_{k})$ is the MLE for $h(\theta_{1},\cdots,\theta_{k})$, where $h$ is a known function.
\end{lem}
\begin{thm}
	\label{Chapter 2 (Thoerem) Sequence of MLE is asympt. normal}
	A sequence of MLE $\{\hat{\theta}_{n}\in\mathbb{R}^{k}\}$ is consistent, asymptotically unbiased for $\theta$, asymptotically efficient and asymptotically normally distributed. More precisely, under regularity assumption, as $n\to\infty$, we have:
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)\to\N_{k}(\mathbf{0},\mathcal{I}_{X}^{-1}(\theta))
	\end{equation*}
	where $\mathcal{I}_{X}(\theta)$ is known as the \textbf{Fisher Information matrix} and is a $k\times k$ matrix with the $(i,j)$-th entry defined as:
	\begin{equation*}
		\begin{cases}
			\E\left[\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{f_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Continuous case}\\
			\E\left[\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{i}}\right)\left(\pdv*{\ln{p_{X}(X|\theta)}}{\theta_{j}}\right)\right], &\text{Discrete case}
		\end{cases}
	\end{equation*}
	for $i=1,\cdots,k$ and $j=1,\cdots,k$.
\end{thm}
\begin{rem}
	In the theorem, "asymptotically efficient" means that the limiting variance is the smallest possible. This will be further discussed in Chapter 3.
\end{rem}
Notice that we have used a special matrix called "Fisher Information Matrix". What is Fisher Information?
\begin{defn}
	Given a set of random variables $\{X_{1},\cdots,X_{n}\}$. The \textbf{Fisher Information}, or \textbf{Fisher Information matrix} if more than one unknown parameter is considered, of the set is defined by:
	\begin{equation*}
		\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)=\begin{cases}
			\E\left[\odv*{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}\right]^{2}, &\text{Continuous case}\\
			\E\left[\odv*{}{\theta}\ln{p_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}\right]^{2}, &\text{Discrete case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{rem}
	Fisher Information is a measure of amount of information about an unknown parameter $\theta$ that a random variable or data carries. It is very important because we do want to know how to quantify this amount appropriately.
\end{rem}
\begin{eg}
	If $X\sim\N(\mu,\sigma^{2})$, where $\sigma^{2}$ is known but $\mu\in(-\infty,\infty)$ is unknown, then the Fisher Information about $\mu$ contained in $X$ is:
	\begin{equation*}
		\mathcal{I}_{X}(\mu)=\E\left[\odv*{\ln{f_{X}(X|\mu)}}{\mu}\right]^{2}=\E\left[\odv*{}{\mu}\left(-\frac{1}{2\sigma^{2}}(X-\mu)^{2}-\frac{1}{2}\ln(2\pi\sigma^{2})\right)\right]^{2}=\E\left[\frac{1}{\sigma^{2}}(X-\mu)\right]^{2}=\frac{1}{\sigma^{2}}
	\end{equation*}
\end{eg}
\begin{eg}
	If $X\sim\Bern(p)$, where $p\in(0,1)$ is unknown, then the Fisher Information about $p$ contained in $X$ is:
	\begin{align*}
		\mathcal{I}_{X}(p)=\E\left[\odv*{\ln{f_{X}(X|p)}}{p}\right]^{2}&=\E\left[\odv*{}{p}\left(X\ln{p}+(1-X)\ln(1-p)\right)\right]^{2}\\
		&=\E\left[\frac{X}{p}-\frac{1-X}{1-p}\right]^{2}\\
		&=\E\left[\frac{X-p}{p(1-p)}\right]^{2}\\
		&=\frac{p(1-p)}{p^{2}(1-p)^{2}}=\frac{1}{p(1-p)}
	\end{align*}
\end{eg}

\newpage
We will see some properties of the Fisher Information. \textit{For simplicity, we will only discuss continuous random variables.} Notice that we used something called "regularity assumption"? The following are the regularity conditions that we need.
\begin{enumerate}
	\item $\odv*{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}}(x_{1},\cdots,x_{n}|\theta)$ exists for all $x_{1},\cdots,x_{n}$ and all $\theta\in\Theta$
	\item For any statistic $T(x_{1},\cdots,x_{n})$,
	\begin{align*}
		&\odv*{}{\theta}\int\cdots\int T(x_{1},\cdots,x_{n})f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)\,dx_{1}\,\cdots\,dx_{n}\\
		=&\int\cdots\int T(x_{1},\cdots,x_{n})\odv*{}{\theta}f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)\,dx_{1}\,\cdots\,dx_{n}
	\end{align*}
	\item $0<\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)<\infty$ for all $\theta\in\Theta$
\end{enumerate}
Condition 2 can be satisfied when the support of $X$ does not depend on $\theta$, where the support of $X$ is defined in below:
\begin{defn}
	Suppose $X$ is a random variable with a PMF $p(x)$ or a PDF $f(x)$. The \textbf{support} of $X$ is defined as:
	\begin{equation*}
		\supp(X)=\begin{cases}
			\{x:p_{X}(x)>0\}, &\text{Discrete case}\\
			\{x:f_{X}(x)>0\}, &\text{Continuous case}
		\end{cases}
	\end{equation*}
\end{defn}
\begin{lem}
	\label{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}
	Suppose the $X$ is a random variable with PDF $f_{X}$. Under the regularity condition, we have:
	\begin{equation*}
		\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]=0
	\end{equation*}
\end{lem}
\begin{proofing}
	\begin{equation*}
		0=\odv*{}{\theta}\intinfty f_{X}(x|\theta)\,dx\intinfty\odv*{}{\theta}f_{X}(x|\theta)\,dx=\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]
	\end{equation*}
\end{proofing}
\begin{rem}
	Using this lemma, we can find that:
	\begin{equation*}
		\mathcal{I}_{X}(\theta)=\Var\left(\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right)
	\end{equation*}
\end{rem}
\begin{lem}
	\label{Chapter 2 (Lemma) Relationship between expectation of l'' and I}
	Suppose that $\{X_{1},\cdots,X_{n}\}$ is a set of random variables. Under the regularity conditions and the assumption that $\odv*[order={2}]{}{\theta}\ln{f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)}$ exists for all $x_{1},\cdots,x_{n}$ and all $\theta\in\Theta$, we have:
	\begin{equation*}
		\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}=-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]
	\end{equation*}
\end{lem}
\begin{proofing}
	From the proof of last Lemma,
	\begin{align*}
		0&=\odv*{}{\theta}\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx\\
		&=\intinfty\odv*{}{\theta}\left[\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\right]\,dx\\
		&=\intinfty\left(\odv*[order=2]{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx+\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)\odv*{}{\theta}f_{X}(x|\theta)\,dx\\
		&=\intinfty\left(\odv*[order=2]{}{\theta}\ln{f_{X}(x|\theta)}\right)f_{X}(x|\theta)\,dx+\intinfty\left(\odv*{}{\theta}\ln{f_{X}(x|\theta)}\right)^{2}f_{X}(x|\theta)\,dx\\
		&=\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]+\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}\\
		-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}
	\end{align*}
\end{proofing}

\newpage
Assume that we consider two independent random variables $X$ and $Y$. We can find the Fisher Information about $\theta$ contained in $(X,Y)$ by finding the Fisher Information about $\theta$ contained in each of them.
\begin{lem}
	If $X$ and $Y$ are independent and their PDFs satisfy the regularity conditions, then:
	\begin{equation*}
		\mathcal{I}_{X,Y}(\theta)=\mathcal{I}_{X}(\theta)+\mathcal{I}_{Y}(\theta)
	\end{equation*}
\end{lem}
\begin{proofing}
	Since $X$ and $Y$ are independent,
	\begin{align*}
		\mathcal{I}_{X,Y}(\theta)&=\E\left[\odv*{}{\theta}\ln{f_{X,Y}(X,Y|\theta)}\right]^{2}\\
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}+\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}+2\E\left[\left(\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right)\left(\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right)\right]+\E\left[\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		\tag{Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}}
		&=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}+\E\left[\odv*{}{\theta}\ln{f_{Y}(Y|\theta)}\right]^{2}\\
		&=\mathcal{I}_{X}(\theta)+\mathcal{I}_{Y}(\theta)
	\end{align*}
\end{proofing}
By applying the same result to a random sample of size $n$, we can obtain the following property.
\begin{lem}
	Suppose the $\{X_{1},\cdots,X_{n}\}$ is a random sample of size $n$ from a distribution. Then,
	\begin{equation*}
		\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)=\sum_{i=1}^{n}\mathcal{I}_{X_{i}}(\theta)=n\mathcal{I}_{X_{1}}(\theta)
	\end{equation*}
\end{lem}
\begin{rem}
	For any $i\neq j$, $\mathcal{I}_{X_{i}}(\theta)=\mathcal{I}_{X_{j}}(\theta)$ only means that $X_{i}$ and $X_{j}$ carries the same amount of the information about $\theta$. It does not mean they carry identical information.
\end{rem}
\begin{eg}
	Consider a set of i.i.d. random variables $\{X_{1},\cdots,X_{n}\}$ where for all $i=1,\cdots,n$, $X_{i}\sim\Cauchy(\theta)$ and has a PDF:
	\begin{equation*}
		f_{X_{i}}(x|\theta)=\frac{1}{\pi(1+(x-\theta)^{2})}
	\end{equation*}
	We may find that:
	\begin{align*}
		\mathcal{I}_{X_{i}}(\theta)=\E\left[\odv*{}{\theta}\ln{f_{X_{i}}(X_{i}|\theta)}\right]^{2}&=\E\left(\frac{\frac{2(X_{i}-\theta)}{\pi(1+(X_{i}-\theta)^{2})^{2}}}{\frac{1}{\pi(1+(X_{i}-\theta)^{2})}}\right)^{2}\\
		&=\E\left(\frac{2(X_{i}-\theta)}{1+(X_{i}-\theta)^{2}}\right)^{2}\\
		&=\intinfty\left(\frac{2(x-\theta)}{1+(x-\theta)^{2}}\right)^{2}\frac{1}{\pi(1+(x-\theta)^{2})}\,dx\\
		\tag{$u=x-\theta$, $du=dx$}
		&=\frac{4}{\pi}\intinfty\frac{u^{2}}{(1+u^{2})^{3}}\,du\\
		&=\frac{8}{\pi}\int_{0}^{\infty}\frac{u^{2}}{(1+u^{2})^{3}}\,du\\
		\tag{$y=\frac{1}{1+u^{2}}$, $dy=-\frac{2u}{(1+u^{2})^{2}}\,du$}
		&=\frac{4}{\pi}\int_{0}^{1}\sqrt{y}\sqrt{1-y}\,dy\\
		\tag{Beta integral}
		&=\frac{4}{\pi}\int_{0}^{1}(y)^{\frac{3}{2}-1}(1-y)^{\frac{3}{2}-1}\,dy\\
		\tag{$\frac{\Gamma(z_{1})\Gamma(z_{2})}{\Gamma(z_{1}+z_{2})}=\int_{0}^{1}t^{z_{1}-1}(1-t)^{z_{2}-1}\,dt$}
		&=\frac{4\Gamma(\frac{3}{2})\Gamma(\frac{3}{2})}{\pi\Gamma(3)}=\frac{4(0.5\sqrt{\pi})^{2}}{2!}=\frac{1}{2}
	\end{align*}
	Therefore, $I_{X_{1},\cdots,X_{n}}(\theta)=nI_{X_{1}}(\theta)=\frac{n}{2}$.
\end{eg}

\newpage
Note that a statistic or an estimator can be considered as a function for data condensation because we condense a random sample into a lower-dimensional quantity.
\begin{lem}
	Suppose that $\mathbf{X}$ is a random vector. Under the regularity conditions, for any statistic $T(\mathbf{X})$ for $\theta$, we have:
	\begin{equation*}
		\mathcal{I}_{T(\mathbf{X})}(\theta)\leq\mathcal{I}_{\mathbf{X}}(\theta)
	\end{equation*}
\end{lem}
\begin{rem}
	The Fisher Information of $T(\mathbf{X})$ is defined by:
	\begin{equation*}
		\mathcal{I}_{T(\mathbf{X})}(\theta)=\E\left[\odv*{}{\theta}\ln{f_{T(\mathbf{X})}(T(\mathbf{X})|\theta)}\right]^{2}
	\end{equation*}
\end{rem}
We may prove Theorem \ref{Chapter 2 (Thoerem) Sequence of MLE is asympt. normal} in one-parameter case:
\begin{thm}
	Consider a random sample $\{X_{1},\cdots,X_{n}\}$ of size $n$ from a parametric distribution with a PDF $f_{X}$. Then under the regularity and some other conditions, for $\theta\in\mathbb{R}$, a sequence of MLE $\{\hat{\theta}_{n}\in\mathbb{R}\}$ allows,
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)\to\N\left(0,\frac{1}{\mathcal{I}_{X}(\theta)}\right)
	\end{equation*}
\end{thm}
\begin{proofing}
	Since the MLE $\hat{\theta}_{n}$ is the solution to $l'(\theta)=0$, we can apply a Taylor expansion of $l'(\hat{\theta}_{n})$ at $\theta$ to find that:
	\begin{align*}
		0&=l'(\theta)+l''(\theta)(\hat{\theta}_{n}-\theta)+o((\hat{\theta}_{n}-\theta))\\
		\sqrt{n}(\hat{\theta}_{n}-\theta)&=\frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)}-o((\hat{\theta}_{n}-\theta))
	\end{align*}
	We first consider the numerator. Note that $\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)},\cdots,\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}$ are i.i.d.. By CLT, we have:
	\begin{equation*}
		\sqrt{n}\left(\frac{1}{n}\sum_{i=1}^{n}\odv*{}{\theta}\ln{f_{X}(X_{i}|\theta)}-\E\left[\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}\right]\right)\to\N\left(0,\Var\left[\odv*{}{\theta}\ln{f_{X}(X_{1}|\theta)}\right]\right)
	\end{equation*}
	By Lemma \ref{Chapter 2 (Lemma) Expectation of Fisher Information but with first moment is zero}, we have:
	\begin{equation*}
		\frac{1}{\sqrt{n}}l'(\theta)=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\odv*{}{\theta}\ln{f_{X}(X_{i}|\theta)}\to\N(0,\mathcal{I}_{X}(\theta))
	\end{equation*}
	Now we consider the denominator. By WLLN and Lemma \ref{Chapter 2 (Lemma) Relationship between expectation of l'' and I}, since $\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{1}|\theta)},\cdots,\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{1}|\theta)}$ are i.i.d.,
	\begin{equation*}
		-\frac{1}{n}l''(\theta)=-\frac{1}{n}\sum_{i=1}^{n}\odv*[order={2}]{}{\theta}\ln{f_{X}(X_{i}|\theta)}\to-\E\left[\odv*[order={2}]{}{\theta}\ln{f_{X}(X|\theta)}\right]=\E\left[\odv*{}{\theta}\ln{f_{X}(X|\theta)}\right]^{2}=\mathcal{I}_{X}(\theta)
	\end{equation*}
	Consequently, we have:
	\begin{equation*}
		\sqrt{n}(\hat{\theta}_{n}-\theta)=\frac{\frac{1}{\sqrt{n}}l'(\theta)}{-\frac{1}{n}l''(\theta)}-o((\hat{\theta}_{n}-\theta))\to\N\left(0,\frac{1}{\mathcal{I}_{X}(\theta)}\right)
	\end{equation*}
\end{proofing}
\begin{rem}
	Sometime, $\mathcal{I}_{X}(\theta)$ cannot be determined easily. We will replace it by an observed Fisher Information defined by $-\frac{1}{n}l''(\hat{\theta}_{n})$. Since $\hat{\theta}_{n}$ is consistent for $\theta$, $-\frac{1}{n}l''(\hat{\theta}_{n})$ is also consistent for $\mathcal{I}_{X}(\theta)$ by continuous mapping theorem. Therefore,
	\begin{equation*}
		\sqrt{-l''(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)=\sqrt{n}\sqrt{-\frac{1}{n}l''(\hat{\theta}_{n})}(\hat{\theta}_{n}-\theta)\to\N(0,1)
	\end{equation*}
\end{rem}

\newpage
\begin{eg}\named{Principal of Numerical solution to likelihood equations}
	Consider a random sample of size $n$ from $X\sim\Cauchy(\theta)$, similar to the previous example. We want to find the MLE of $\theta$. We have:
	\begin{equation*}
		l(\theta)=-n\ln{\pi}-\sum_{i=1}^{n}\ln(1+(x_{i}-\theta)^{2})
	\end{equation*}
	We want to find the solution of $l'(\theta)=0$, which is the MLE. Setting:
	\begin{equation*}
		\sum_{i=1}^{n}\frac{2(x_{i}-\theta)}{1+(x_{i}-\theta)^{2}}
	\end{equation*}
	However, this cannot be solved explicitly in this case.
\end{eg}
\begin{eg}\named{Newton-Raphson Algorithm}
	By Taylor expansion, we can get:
	\begin{equation*}
		0=\frac{1}{n}l'(\hat{\theta})\approx\frac{1}{n}l'(\theta)+\frac{1}{n}(\hat{\theta}-\theta)l''(\theta)
	\end{equation*}
	We may modify it to find that:
	\begin{equation*}
		\hat{\theta}\approx\theta-\frac{l'(\theta)}{l''(\theta)}
	\end{equation*}
	We may initially guess a number, say $\theta_{0}$. By iteratively applying the procedure for $j=0,1,\cdots$:
	\begin{equation*}
		\theta_{j+1}=\theta_{j}-\frac{l'(\theta_{j})}{l''(\theta_{j})}
	\end{equation*}
	and stop it at a certain stopping criterion, say $\abs{\theta_{j+1}-\theta_{j}}<K$ for some chosen constant $K$. E.g. $K=10^{-5}$.\\
	Using this algorithm, we can obtain or approximate the MLE of $\theta$.
\end{eg}
\begin{eg}
	Consider a random sample of size $n$ from $X\sim\Gam(\alpha,\beta)$, where $\beta=3423$ and $\alpha$ is unknown. The PDF is defined by:
	\begin{equation*}
		f_{X}(x|\theta)=\begin{cases}
			\frac{3423^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-3423x}, &x>0\\
			0, &\text{Otherwise}
		\end{cases}
	\end{equation*}
	We can find the log likelihood:
	\begin{equation*}
		l(\alpha)=\sum_{i=1}^{n}\ln{f_{X}(x_{i}|\alpha)}=n\alpha\ln{3423}-n\ln(\Gamma(\alpha))+(\alpha-1)\sum_{i=1}^{n}\ln{x_{i}}-3423\sum_{i=1}^{n}x_{i}
	\end{equation*}
	To find MLE, we try to solve the equation:
	\begin{equation*}
		0=\odv*{}{\alpha}l(\alpha)=n\ln{3423}-n\odv*{}{\alpha}\ln(\Gamma(\alpha))+\sum_{i=1}^{n}\ln{x_{i}}
	\end{equation*}
	However, we have no idea how to find $\odv*{}{\alpha}\ln(\Gamma(\alpha))$. Therefore, instead, we would use the numerical methods to approximate the MLE.
\end{eg}

\chapter{Uniformly Minimum Variance Unbiased Estimator}
We usually want to find the best estimator that can approximate some parameters. However, there are a lot of estimators we can provide based on the information given. In this chapter, we would try to find the best out of them.
\section{Introduction to UMVUE}
Consider a class $M$ defined as all the estimators for $\theta$. If there exists an estimator $\hat{\theta}^{*}\in M$ that is uniformly better than any other estimator in $M$, then we say $\hat{\theta}^{*}$ is the best estimator of $\theta$ in $M$. However, in general, this estimator does not exist partly because there are too many estimators to consider and some of them are poor or not reasonable. To avoid this problem, we only consider a particular class of estimators, which is the mean-unbiased estimators.
\begin{rem}
	In this context, $\hat{\theta}^{*}$ is "uniformly better" means that $\Var(\hat{\theta}^{*})<\Var(\hat{\theta})$ for any other $\hat{\theta}\in M$.
\end{rem}
Recall the definition of mean-unbiased estimator. If an estimator $\hat{\theta}$ satisfies:
\begin{equation*}
	\E(\hat{\theta})=\theta
\end{equation*}
for all $\theta\in\Theta$, then it is mean-unbiased or simply unbiased for $\theta$. Otherwise, it is biased.\\
From past experiences, we may get the following remarks.
\begin{rem}
	Unbiasedness means that by repeated sampling, $\hat{\theta}=\theta$ on average. The underestimation and overestimation will balance in the long run.
\end{rem}
\begin{rem}
	Sample variance $S_{n-1}^{2}$ is unbiased for $\sigma^{2}$ but $S_{n}^{2}$ is not. This is why we use $S_{n-1}^{2}$ to estimate $\sigma^{2}$ instead of $S_{n}^{2}$.
\end{rem}
\begin{rem}
	MME and MLE are usually biased, but they are asymptotically unbiased.
\end{rem}
\begin{rem}
	It is possible to have infinitely many different unbiased estimators for $\theta$.
\end{rem}
\begin{eg}
	Consider $\{X_{1},\cdots,X_{n}\}$ a random sample of size $n$ from a distribution with a finite mean $\theta$.\\
	Any estimators $\hat{\theta}$ in the form of:
	\begin{equation*}
		\hat{\theta}=\frac{\sum_{i=1}^{n}a_{i}X_{i}}{\sum_{i=1}^{n}a_{i}}
	\end{equation*}
	where $a_{i}\in\mathbb{R}$ for $i=1,\cdots,n$ and $\sum_{i=1}^{n}a_{i}\neq 0$, are unbiased for $\theta$.
\end{eg}
\begin{rem}
	It is possible to have no unbiased estimators for $\theta$.
\end{rem}
\begin{eg}
	Consider a random sample of size $n$ from a random variable $X\sim\Bin(1,\theta)$ with $g(\theta)=\frac{\theta}{1-\theta}$ as the parameter being estimated. There does not exist an unbiased estimator for $g(\theta)$.
\end{eg}
\begin{rem}
	Unbiasedness does not have an invariance property. $\hat{\theta}$ is unbiased for $\theta$ does not mean $h(\hat{\theta})$ is unbiased for $h(\theta)$.
\end{rem}
\begin{eg}
	We have $\overline{X}$ is unbiased for $\mu$, but $(\overline{X})^{2}$ is not unbiased for $\mu^{2}$ when $\sigma>0$.
\end{eg}
The best unbiased estimator is the unbiased estimator with the smallest variance.
\begin{defn}
	The \textbf{Uniformly Minimum Variance Unbiased Estimator (UMVUE)} $\hat{\theta}^{*}$ for $\theta$ is an unbiased estimator such that for all other unbiased estimator $\hat{\theta}$ for $\theta$,
	\begin{equation*}
		\Var(\hat{\theta}^{*})\leq\Var(\hat{\theta})
	\end{equation*}
	for all $\theta\in\Theta$.
\end{defn}
\begin{lem}\named{Uniqueness of UMVUE}
	Assume that UMVUE for $\theta$ exists. Then, it is unique.
\end{lem}
\begin{proofing}
	Assume that there are two distinct UMVUEs, $\hat{\theta}^{*}$ and $\hat{\theta}^{**}$, for $\theta$. We may find that for all $\theta\in\theta$ and any unbiased estimator $\hat{\theta}$ of $\theta$:
	\begin{equation*}
		\Var(\hat{\theta}^{*})=\Var(\hat{\theta}^{**})\leq\Var(\hat{\theta})
	\end{equation*}
	Let $\hat{\theta}'=\frac{1}{2}(\hat{\theta}^{*}+\hat{\theta}^{**})$. We can easily find that $\hat{\theta}'$ is unbiased for $\theta$. We have:
	\begin{align*}
		\Var(\hat{\theta}')&=\frac{1}{4}\Var(\hat{\theta}^{*})+\frac{1}{4}\Var(\hat{\theta}^{**})+\frac{1}{2}\cov(\hat{\theta}^{*},\hat{\theta}^{**})\\
		&\leq\frac{1}{2}\Var(\hat{\theta}^{*})+\frac{1}{2}\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}\\
		&\leq\Var(\hat{\theta}^{*})\leq\Var(\hat{\theta}')
	\end{align*}
	Thus $\Var(\hat{\theta}')=\Var(\hat{\theta}^{*})$ and $\cov(\hat{\theta}^{*},\hat{\theta}^{**})=\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}$.\\
	We recall the Pearson correlation coefficient. We can find that:
	\begin{equation*}
		\rho=\frac{\cov(\hat{\theta}^{*},\hat{\theta}^{**})}{\sqrt{\Var(\hat{\theta}^{*})\Var(\hat{\theta}^{**})}}=1
	\end{equation*}
	This means $\hat{\theta}^{*}$ and $\hat{\theta}^{**}$ have a perfectly linear positive relationship. We say $\hat{\theta}^{*}=a\hat{\theta}^{**}+b$ where $a>0$ and $b\in\mathbb{R}$.\\
	We solve the equation:
	\begin{equation*}
		\begin{cases}
			\Var(\hat{\theta}^{**})=\Var(\hat{\theta}^{*})=a^{2}\Var(\hat{\theta}^{**})\\
			\E(\hat{\theta}^{**})=\theta=\E(\hat{\theta}^{*})=a\E(\hat{\theta}^{**})+b
		\end{cases}
	\end{equation*}
	We may find that $a=1$ and $b=0$. Therefore, $\hat{\theta}^{*}=\hat{\theta}^{**}$ and UMVUE is unique.
\end{proofing}

\section{Sufficient Statistic}
It is not easy to find UMVUE for a parameter being estimated. However, we have Rao-Blackwell Theorem (we will talk about it later) which tells us that UMVUE must be a function of sufficient statistic. Let us discuss about sufficient statistic.\\
Note that a statistic or an estimator can be considered as a function for data condensation because we condense a random sample into a lower-dimensional quantity. However, in the process, we may lose some information about the parameter $\theta$.\\
We recall this lemma. Under regularity conditions, for any statistic $T=T(\mathbf{X})$ for $\theta$, we have:
\begin{equation*}
	\mathcal{I}_{T}(\theta)\leq\mathcal{I}_{\mathbf{X}}(\theta)
\end{equation*}
Most statistics would lose some information about $\theta$, but there exists some statistics that we can substantially reduce the dimension without losing any information. We call this sufficient statistic.
\begin{defn}
	Under regularity conditions, the \textbf{sufficient statistic} for $\theta$, denoted by $S=S(\mathbf{X})$, is a statistic that satisfies:
	\begin{equation*}
		\mathcal{I}_{S}(\theta)=\mathcal{I}_{\mathbf{X}}(\theta)
	\end{equation*}
\end{defn}
\begin{rem}
	If the conditional distribution of the sample given a statistic $T$ depends on $\theta$, then there is still some information about $\theta$ contained in the sample that $T$ does not carry. Therefore, $T$ is not sufficient.
\end{rem}
\begin{eg}
	Let $\{X_{1},X_{2}\}$ be a random sample of size $2$ from $X\sim\Bin(m,\theta)$. We show that $T=T(X_{1},X_{2})=X_{1}+X_{2}$ is sufficient.
	\begin{align*}
		p_{X_{1},X_{2}|T}(x_{1},x_{2}|t)&=\frac{p_{X_{1},X_{2},T}(x_{1},x_{2},t)}{p_{T}(t)}\\
		&=\frac{p_{X_{1},X_{2},T}(x_{1},t-x_{1},t)}{p_{T}(t)}\\
		&=\frac{p_{X_{1}}(x_{1})p_{X_{2}}(t-x_{1})}{p_{T}(t)}\\
		&=\frac{\binom{m}{x_{1}}\binom{m}{t-x_{1}}\theta^{t}(1-\theta)^{n-t}}{\binom{2m}{t}\theta^{t}(1-\theta)^{n-t}}=\frac{\binom{m}{x_{1}}\binom{m}{t-x_{1}}}{\binom{2m}{t}}
	\end{align*}
	Therefore, since the conditional distribution of the sample given a statistic $T$ does not depend on $\theta$, we can find that $T$ is a sufficient statistic.
\end{eg}
We may rewrite the definition of sufficient statistic as follows:
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random vector of random sample of size $n$ from a PDF $f(x|\theta)$ or PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for integer $k>1$. A set of statistics $\{S_{1},S_{2},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$, is said to be \textbf{jointly sufficient}, if and only if the conditional distribution:
	\begin{equation*}
		\begin{cases}
			p_{\mathbf{X}|S_{1},\cdots,S_{r}}(\mathbf{x}|S_{1}=s_{1},\cdots,S_{r}=s_{r},\theta), &\text{Discrete case}\\
			f_{\mathbf{X}|S_{1},\cdots,S_{r}}(\mathbf{x}|S_{1}=s_{1},\cdots,S_{r}=s_{r},\theta), &\text{Continuous case}
		\end{cases}
	\end{equation*}
	does not depend on $\theta$, for all values $s_{1}$ of $S_{1}$, $\cdots$, $s_{r}$ of $S_{r}$.
\end{defn}
or in one-parameter case,
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. A statistic $S=S(\mathbf{X})$ is said to be \textbf{sufficient} if and only if the conditional distribution:
	\begin{equation*}
		\begin{cases}
			p_{\mathbf{X}|S}(\mathbf{x}|S=s,\theta), &\text{Discrete case}\\
			f_{\mathbf{X}|S}(\mathbf{x}|S=s,\theta), &\text{Continuous case}
		\end{cases}
	\end{equation*}
	does not depend on $\theta$, for all values $s$ of $S$.
\end{defn}
\begin{thm}\named{Fisher-Neyman Factorization Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$. A set of statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,n$, is jointly sufficient if and only if:
	\begin{equation*}
		\begin{cases}
			p_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Discrete case}\\
			f_{\mathbf{X}}(\mathbf{x}|\theta)=g(S_{1}(\mathbf{x}),\cdots,S_{r}(\mathbf{x})|\theta)h(\mathbf{x}), &\text{Continuous case}
		\end{cases}
	\end{equation*}
	where $g$ is a non-negative function of $x_{1},\cdots,x_{n}$ only through the statistics $S_{1},\cdots,S_{r}$ and depends on $\theta$, and $h$ is a non-negative function of $x_{1},\cdots,x_{n}$ not depending on $\theta$.
\end{thm}
\begin{proofing}
	We shall prove it in discrete case with only one statistic. The proof in continuous case or more than one statistic is out of our scope.\\
	Note that if $\mathbf{X}\in B$ for a set $B$, then $S(X)\in S(B)$. Therefore, for $i=1,\cdots,n$,
	\begin{equation*}
		\{\mathbf{X}\in B\}\cap\{S(\mathbf{X})\in S(B)\}=\{\mathbf{X}\in B\}
	\end{equation*}
	\begin{align*}
		\prob(\mathbf{X}\in B|\theta)&=\prob(\mathbf{X}\in B,S(\mathbf{X})\in S(B)|\theta)\\
		\tag{$\prob(A\cap B|D)=\prob(A|B\cap D)\prob(B\cap D)$}
		&=\prob(\mathbf{X}\in B|S(\mathbf{X})\in S(B),\theta)\prob(S(\mathbf{X})\in S(B)|\theta)
	\end{align*}
	
	\newpage
	If $S$ is sufficient, then by definition,
	\begin{equation*}
		\prob(\mathbf{X}\in B|S(\mathbf{X})\in S(B),\theta)=\prob(X_{i}\in B|S(X_{i})\in S(B))
	\end{equation*}
	Therefore, by substituting $B=\{\mathbf{x}\}$, we get:
	\begin{equation*}
		p_{\mathbf{X}}(\mathbf{x}|\theta)=p_{\mathbf{X}|S}(\mathbf{x}|S(\mathbf{x}))p_{S}(S(\mathbf{x})|\theta)
	\end{equation*}
	We may find that $g(S(\mathbf{x})|\theta)=p_{S}(S(\mathbf{x})|\theta)$ and $h(\mathbf{x})=p_{\mathbf{X}|S}(\mathbf{x}|S(\mathbf{x}))$.\\
	If $p_{X}(\mathbf{x}|\theta)=g(T(\mathbf{x})|\theta)h(\mathbf{x})$, then:
	\begin{equation*}
		p_{\mathbf{X}|T}(\mathbf{x}|t)=\frac{p_{\mathbf{X},T}(\mathbf{x},t|\theta)}{p_{T}(t|\theta)}=\begin{cases}
			0, &t\neq T(\mathbf{x})\\
			\frac{p_{\mathbf{X}}(\mathbf{x}|\theta)}{p_{T}(t|\theta)}, &t=T(\mathbf{x})
		\end{cases}
	\end{equation*}
	 We only need to consider when $t=T(\mathbf{x})$, we have:
	\begin{equation*}
		p_{\mathbf{X}|T}(\mathbf{x}|t)=\frac{p_{\mathbf{X}}(\mathbf{x}|\theta)}{\sum_{\mathbf{x}:T(\mathbf{x})=t}p_{\mathbf{x}}(\mathbf{x}|\theta)}=\frac{g(t|\theta)h(\mathbf{x})}{\sum_{\mathbf{x}:T(\mathbf{x})=t}g(t|\theta)h(\mathbf{x})}=\frac{h(\mathbf{x})}{\sum_{\mathbf{x}:T(\mathbf{x})=t}h(\mathbf{x})}
	\end{equation*}
	We have found that $p_{\mathbf{X}|T}(\mathbf{x}|t)$ does not depend on $\theta$. Therefore, by definition, $T$ is sufficient.
\end{proofing}
\begin{eg}
	\label{Chapter 1 (Example) Sufficient statistics of Bern(theta)}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample from $\Bern(\theta)$, where $\theta\in[0,1]$ is unknown. The joint PMF of the random sample is:
	\begin{align*}
		p_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)=\prod_{i=1}^{n}\theta^{x_{i}}(1-\theta)^{1-x_{i}}&=\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n-\sum_{i=1}^{n}x_{i}}\times 1\\
		&=g\left(\left.\sum_{i=1}^{n}x_{i}\right|\theta\right)\times h(x_{1},\cdots,x_{n})		
	\end{align*}
	Therefore, $S=\sum_{i=1}^{n}X_{i}$ is a sufficient statistic.
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample from $\N(\mu,\sigma^{2})$, where $\mu$ and $\sigma^{2}>0$ are unknown. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\mu,\sigma^{2})&=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x_{i}-\mu)^{2}}{2\sigma^{2}}\right)\\
		&=\frac{1}{(2\pi\sigma^{2})^{\frac{n}{2}}}\exp\left(-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)\\
		&=\sigma^{-n}\exp\left[-\frac{1}{2\sigma^{2}}\left(\sum_{i=1}^{n}x_{i}^{2}-2\mu\sum_{i=1}^{n}x_{i}+n\mu^{2}\right)\right]\times\left(\frac{1}{(2\pi)^{
		\frac{n}{2}}}\right)\\
		&=g\left(\left.\sum_{i=1}^{n}x_{i},\sum_{i=1}^{n}x_{i}^{2}\right|\mu,\sigma^{2}\right)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S_{1}=\sum_{i=1}^{n}X_{i}$ and $S_{2}=\sum_{i=1}^{n}X_{i}^{2}$ are jointly sufficient.
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)&=\frac{1}{\theta^{n}}\prod_{i=1}^{n}\mathbf{1}_{0\leq x_{i}\leq\theta}\\
		\tag{$x_{(i)}$ is the $i$-th smallest sample}
		&=\frac{1}{\theta^{n}}\mathbf{1}_{0\leq x_{(1)}<x_{(n)}\leq\theta}\\
		&=\frac{1}{\theta^{n}}\mathbf{1}_{x_{(n)}\leq\theta}\times\mathbf{1}_{x_{(1)}\geq 0}\\
		&=g(x_{(n)}|\theta)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S=X_{(n)}=\max\{X_{1},\cdots,X_{n}\}$ is sufficient.
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[\theta-\frac{1}{2},\theta+\frac{1}{2}]$, where $\theta$ is unknown. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)&=\prod_{i=1}^{n}\mathbf{1}_{\theta-\frac{1}{2}\leq x_{i}\leq\theta+\frac{1}{2}}\\
		\tag{$x_{i}$ is the $i$-th smallest sample}
		&=\mathbf{1}_{\theta-\frac{1}{2}\leq x_{(1)}<x_{(n)}\leq\theta+\frac{1}{2}}\\
		&=\mathbf{1}_{x_{(n)}-\frac{1}{2}\leq\theta\leq x_{(1)}+\frac{1}{2}}\times 1\\
		&=g(x_{(1)},x_{(n)}|\theta)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S_{1}=X_{(1)}=\min\{X_{1},\cdots,X_{n}\}$ and $S_{2}=X_{(n)}=\max\{X_{1},\cdots,X_{n}\}$ are jointly sufficient.
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[\theta_{1},\theta_{2}]$, where $\theta_{1},\theta_{2}$ are unknown with $\theta_{1}<\theta_{2}$ and $\theta_{1}$ is not a function of $\theta_{2}$. The joint PDF of the random sample is:
	\begin{align*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta_{1},\theta_{2})&=\frac{1}{(\theta_{2}-\theta_{1})^{n}}\prod_{i=1}^{n}\mathbf{1}_{\theta_{1}\leq x_{i}\leq\theta_{2}}\\
		\tag{$x_{i}$ is the $i$-th smallest sample}
		&=\frac{1}{(\theta_{2}-\theta_{1})^{n}}\mathbf{1}_{\theta_{1}\leq x_{(1)}<x_{(n)}\leq\theta_{2}}\times 1\\
		&=g(x_{(1)},x_{(n)}|\theta_{1},\theta_{2})\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, $S_{1}=X_{(1)}=\min\{X_{1},\cdots,X_{n}\}$ and $S_{2}=X_{(n)}=\max\{X_{1},\cdots,X_{n}\}$ are jointly sufficient.
\end{eg}
\begin{rem}
	Sufficient statistic may not be unique because we may have more than one factorization.
\end{rem}
\begin{eg}
	\label{Chapter 3 (Example) Sufficient statistic for normal distribution with unit variance}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\mu,1)$, where $\mu$ is unknown. The joint PDF of the random sample is:
	\begin{equation*}
		f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\mu)=\prod_{i=1}^{n}\frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(x_{i}-\mu)^{2}\right)=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)		
	\end{equation*}
	One of the ways to factorize is:
	\begin{align*}
		\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)&=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left[-\frac{1}{2}\left(\sum_{i=1}^{n}x_{i}^{2}-2\mu\sum_{i=1}^{n}x_{i}+n\mu^{2}\right)\right]\\
		&=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(\mu\sum_{i=1}^{n}x_{i}-\frac{n}{2}\mu^{2}\right)\times\exp\left(-\frac{1}{2}\sum_{i=1}^{n}x_{i}^{2}\right)\\
		&=g\left(\left.\sum_{i=1}^{n}x_{i}\right|\mu\right)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, we find that $S_{1}=\sum_{i=1}^{n}X_{i}$ is sufficient.\\
	Another way to factorize is:
	\begin{align*}
		\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{2}\right)&=	\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left[-\frac{1}{2}\left(\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}+n(\overline{x}-\mu)^{2}\right)\right]\\
		&=\frac{1}{(2\pi)^{\frac{n}{2}}}\exp\left(-\frac{n}{2}(\overline{x}-\mu)^{2}\right)\times\exp\left(-\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\overline{x})^{2}\right)\\
		&=g(\overline{x}|\mu)\times h(x_{1},\cdots,x_{n})
	\end{align*}
	Therefore, we find that $S_{2}=\overline{X}$ is sufficient.
\end{eg}

\newpage
Based on the above example, we may notice that $\overline{X}$ and $\sum_{i=1}^{n}X_{i}$ are functions of each other. Is any transformation of $S$ also sufficient? Yes if it is one-to-one!
\begin{lem}\named{One-to-one sufficiency}
	\label{Chapter 3 (Lemma) One-to-one sufficiency}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$. If a set of statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(X_{1},\cdots,X_{r})$ for $i=1,\cdots,r$, is jointly sufficient, then any set of one-to-one functions $\{h_{1},\cdots,h_{m}\}$ for some $m$, where $m\geq r$ for $i=1,\cdots,m$:
	\begin{equation*}
		h_{i}=h_{i}(S_{1},\cdots,S_{r})
	\end{equation*}
	is also jointly sufficient.
\end{lem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$. Assume that $\sum_{i=1}^{n}X_{i}$ and $\sum_{i=1}^{n}X_{i}^{2}$ are jointly sufficient. We may find that:
	\begin{align*}
		\overline{X}&=\frac{1}{n}\sum_{i=1}^{n}X_{i} & \sum_{i=1}^{n}(X_{i}-\overline{X})^{2}&=\sum_{i=1}^{n}X_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n}X_{i}\right)^{2}
	\end{align*}
	Both are one-to-one functions of $\sum_{i=1}^{n}X_{i}$ and $\sum_{i=1}^{n}X_{i}^{2}$.\\
	Therefore, by Lemma \ref{Chapter 3 (Lemma) One-to-one sufficiency}, $\overline{X}$ and $\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}$ are jointly sufficient.\\
	However,
	\begin{equation*}
		(\overline{X})^{2}=\frac{1}{n^{2}}\left(\sum_{i=1}^{n}X_{i}\right)
	\end{equation*}
	It is not an one-to-one function. Therefore, $(\overline{X})^{2}$ and $\sum_{i=1}^{n}(X_{i}-\overline{X})^{2}$ may not be jointly sufficient.
\end{eg}
\begin{rem}
	Number of one-to-one functions of statistics can be smaller than the number of the statistics themselves.
\end{rem}
From previous examples, the number of sufficient statistics can sometimes be more than the number of unknown parameters. How much should data be condense most without losing any information about unknown parameter $\theta$?
\begin{defn}
	A set of jointly sufficient statistics $\{S_{1},\cdots,S_{n}\}$ is \textbf{minimal jointly sufficient} if and only if for any other set of jointly sufficient statistics $\{T_{1},\cdots,T_{m}\}$, there exists a set of functions $\{f_{1},\cdots,f_{n}\}$ such that for $i=1,\cdots,n$,
	\begin{equation*}
		S_{i}=f_{i}(T_{1},\cdots,T_{m})
	\end{equation*}
\end{defn}
or in one-statistic case,
\begin{defn}
	A sufficient statistic $S$ is \textbf{minimal sufficient} if and only if for nay other sufficient statistic $T$, there exists a function $f$ such that:
	\begin{equation*}
		S=f(T)
	\end{equation*}
\end{defn}
\begin{rem}
	Minimal jointly sufficient statistics may not be unique. We can say minimal joint sufficiency is closed under any one-to-one transformation.
\end{rem}
In general, it is not easy to find the minimal jointly sufficient statistics except form some special distribution. One of those special distributions are called exponential family, which we will discuss later.

\newpage
\section{Relationship of Sufficiency with UMVUE}
Recall that we actually want to find UMVUE. Does sufficiency help us find UMVUE? Turns out, by Rao-Blackwell Theorem, it helps us find an improved unbiased estimator!
\begin{thm}\named{Rao-Blackwell Theorem}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$, and a set of jointly sufficient statistics $\{S_{1},\cdots,S_{r}\}$, where $r\geq k$ and $S_{i}=S_{i}(\mathbf{X})$ for $i=1,\cdots,r$. Suppose that $T=T(\mathbf{X})$ is an unbiased estimator for $g(\theta)$ for a function $g$. Define $T'$ by $\E(T|S_{1},\cdots,S_{r})$. Then:
	\begin{enumerate}
		\item $T'$ is a statistic, and it is a function of jointly sufficient statistics.
		\item $T'$ is unbiased for $g(\theta)$.
		\item $\Var(T')\leq\Var(T)$.
	\end{enumerate}
\end{thm}
\begin{proofing}
	\begin{enumerate}
		\item 
		\begin{equation*}
			T'=\E(T|S_{1},\cdots,S_{r})=\intinfty tf_{T|S_{1},\cdots,S_{r}}(t|S_{1},\cdots,S_{r})\,dt
		\end{equation*}
		By definition, $T'$ is a statistic and it is a function of jointly sufficient statistic $\{S_{1},\cdots,S_{r}\}$.
		\item 
		\begin{equation*}
			\E(T')=\E(\E(T|S_{1},\cdots,S_{r}))=\E(T)=g(\theta)
		\end{equation*}
		Therefore, $T'$ is unbiased for $g(\theta)$.
		\item 
		\begin{align*}
			\Var(T')&=\Var(\E(T|S_{1},\cdots,S_{r}))\\
			\tag{$\Var(Y)=\Var(\E(Y|X))+\E[\Var(Y|X)]$}
			&=\Var(T)-\E[\Var(T|S_{1},\cdots,S_{r})]\leq\Var(T)
		\end{align*}
	\end{enumerate}
\end{proofing}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$ where $\theta$ is unknown.
	\begin{enumerate}
		\item Since $\E(X_{1})=\theta$, $X_{1}$ is an unbiased estimator of $\theta$.
		\item From Example \ref{Chapter 1 (Example) Sufficient statistics of Bern(theta)}, we have found that $\sum_{i=1}^{n}X_{i}$ is a sufficient statistic. It is also obvious that it is minimal.
		\item By Rao-Blackwell Theorem, $T'=\E(X_{1}|\sum_{i=1}^{n}X_{i})$ is an unbiased estimator for $\theta$ with $\Var(T')\leq\Var(X_{1})$.
	\end{enumerate}
	We want to find $T'$. Assume that we have given that $\sum_{i=1}^{n}X_{i}=s$. We have:
	\begin{align*}
		\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\frac{\prob\left(X_{1}=0,\sum_{i=1}^{n}X_{i}=s\right)}{\prob\left(\sum_{i=1}^{n}X_{i}=s\right)}&=\frac{\prob(X_{1}=0)\prob\left(\sum_{i=2}^{n}X_{i}=s\right)}{\prob\left(\sum_{i=1}^{n}X_{i}=s\right)}\\
		&=\frac{(1-\theta)\binom{n-1}{s}\theta^{s}(1-\theta)^{n-1-s}}{\binom{n}{s}\theta^{s}(1-\theta)^{n-s}}\\
		&=\frac{n-s}{n}
	\end{align*}
	Therefore, we have:
	\begin{equation*}
		\E\left(X_{1}\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\prob\left(X_{1}=1\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=1-\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\frac{s}{n}
	\end{equation*}
	We have found that $T'=\E\left(X_{1}|\sum_{i=1}^{n}X_{i}\right)=\frac{1}{n}\sum_{i=1}^{n}X_{i}$. We may find that:
	\begin{equation*}
		\Var(T')=\frac{1}{n}\theta(1-\theta)\leq\theta(1-\theta)=\Var(X_{1})
	\end{equation*}
\end{eg}

\newpage
\begin{rem}
	If $T$ is already a function of a jointly sufficient statistics, then $T'$ would be identical to $T$.
\end{rem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$ and let $\overline{X}$ be the sample mean. We know that:
	\begin{equation*}
		\E(\overline{X})=\theta
	\end{equation*}
	Therefore, $\overline{X}$ is an unbiased estimator of $\theta$. We want to find $T'$. We have:
	\begin{equation*}
		T'=\E\left(\overline{X}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\overline{X}=T
	\end{equation*}
\end{eg}
\begin{rem}
	Although Rao-Blackwell Theorem can provide us with a constructive way to improve a given unbiased estimator. it does not guarantee that the one constructed must be a UMVUE.
\end{rem}
\begin{eg}
	Consider a random sample $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ of size $n$ from $\N(\theta,1)$, where $\theta$ is unknown. Let $g(\theta)=\theta$. Consider $T=T(\mathbf{X})=X_{1}$ and the random sample is a set of jointly sufficient statistics $\{S_{1},\cdots,S_{n}\}$. We may find that:
	\begin{equation*}
		\tag{Expectation of $X_{1}$ given $X_{1}$ is of course $X_{1}$}
		\E(X_{1}|X_{1},\cdots,X_{n})=X_{1}
	\end{equation*}
	However, from Example \ref{Chapter 3 (Example) Sufficient statistic for normal distribution with unit variance}, we have found a better statistic $S^{*}=\overline{X}=\frac{1}{n}\sum_{i=1}^{n}X_{i}$ since:
	\begin{equation*}
		\Var(\overline{X})=\frac{1}{n}\leq\Var(X_{1})
	\end{equation*}
	Therefore, $T'=X_{1}$ is not a UMVUE.
\end{eg}

\section{Complete Statistics}
In addition to sufficiency, we would need completeness in order to find the UMVUE.
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ and a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$. A set of statistics $\{T_{1},\cdots,T_{r}\}$, where $r\geq k$ and $T_{1}=T_{1}(\mathbf{X})$ for $i=1,\cdots,r$, is said to be \textbf{jointly complete} if and only if for any function $g$:
	\begin{equation*}
		\E[g(T_{1},\cdots,T_{r})]\text{ for all }\theta\in\Theta\implies\prob(g(T_{1},\cdots,T_{r})=0)=1\text{ for all }\theta\in\Theta
	\end{equation*}
\end{defn}
or in one-parameter case,
\begin{defn}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. A statistic $T=T(\mathbf{X})$ is said to be \textbf{complete} if and only if for any function $g$:
	\begin{equation*}
		\E[g(T)]=0\text{ for all }\theta\in\Theta\implies\prob(g(T)=0)=1\text{ for all }\theta\in\Theta
	\end{equation*} 
\end{defn}
\begin{rem}
	Function $g(T)$ or $g(T_{1},\cdots,T_{r})$ is not an unbiased estimator for $\theta$.
\end{rem}
\begin{rem}
	If there exists a function $g^{*}$ such that $\E[g^{*}(T)]=0$ but $\prob(g^{*}(T)\neq 0)>0$, then $T$ is not complete. This is the same for joint completeness. 
\end{rem}

\newpage
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. Let $T_{1}=X_{1}-X_{2}$. We may easily find that for all $\theta\in(0,1)$:
	\begin{equation*}
		\E(X_{1}-X_{2})=0\text{ but }\prob(X_{1}-X_{2}\neq 0)>0
	\end{equation*}
	Therefore, $T_{1}=X_{1}-X_{2}$ is not a complete statistic.\\
	Let $T_{2}=\sum_{i=1}^{n}X_{i}$. For any function $g$,
	\begin{equation*}
		\E[g(T_{2})]=\sum_{i=0}^{n}g(t)\binom{n}{t}\theta^{t}(1-\theta)^{n-t}=(1-\theta)^{n}\sum_{i=1}^{n}g(t)\binom{n}{t}\left(\frac{\theta}{1-\theta}\right)^{t}
	\end{equation*}
	Thus, $\E(g(T_{2}))=0$ for all $\theta\in(0,1)$ implies that the equation $\sum_{i=1}^{n}g(t)\binom{n}{t}\left(\frac{\theta}{1-\theta}\right)^{t}=0$ for all $\theta\in(0,1)$.\\
	If not all coefficients $g(t)\binom{n}{t}$ are equal to zero, then there are at most $n$ solutions of the equation for all $\theta\in(0,1)$. This means only $n$ values of $\theta\in\Theta$ satisfy the equation, but not all $\theta\in(0,1)$.\\
	Therefore, $g(t)\binom{n}{t}=0$ and thus $g(t)=0$ for $t=0,\cdots,n$ and for all $\theta\in(0,1)$.\\
	Since the only possible values of $T_{2}=\sum_{i=1}^{n}X_{i}$ are in $\{0,\cdots,n\}$, we find that:
	\begin{equation*}
		\prob(g(T_{2})=0)=1
	\end{equation*}
	We conclude that $T_{2}=\sum_{i=1}^{n}X_{i}$ is complete.
\end{eg}
\begin{eg}
	\label{Chapter 3 (Example) complete and sufficient statistic of U[0,theta]}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. We check if the sufficient statistic $X_{(n)}$ is complete. Note that for any function $g$,
	\begin{equation*}
		\E[g(X_{(n)})]=\intinfty g(y)f_{X_{(n)}}(y)\,dy=\frac{n}{\theta^{n}}\int_{0}^{\theta}g(y)y^{n-1}\,dy
	\end{equation*}
	Therefore, if $\E[g(X_{(n)})]=0$ for all $\theta>0$, then:
	\begin{equation*}
		\int_{0}^{\theta}g(y)y^{n-1}\,dy=0
	\end{equation*}
	Differentiating both sides with respect to $\theta$ will get $g(\theta)\theta^{n-1}=0$ and hence $g(\theta)=0$ for $\theta>0$. We replace a number $y$ with the parameter $\theta$ and get $g(y)=0$ for $y\in(0,\theta]$ for all $\theta>0$.\\
	Since $0\leq x_{(n)}\leq\theta$, we may find that for all $\theta\in\Theta$:
	\begin{equation*}
		\prob(g(X_{(n)})=0)=1
	\end{equation*}
	Therefore, $X_{(n)}$ is complete.
\end{eg}

\section{Exponential Family}
In most of the time, it is quite difficult to check the completeness and minimal sufficiency of a statistic by definition, especially for joint completeness. However, there is one special distribution which we can check easily. It is called exponential family.
\begin{defn}
	Suppose that a random variable $X$ has a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$. If we find that:
	\begin{enumerate}
		\item $\supp(X)$ does not depend on $\theta$.
		\item The PDF or PMF of $X$ can be written in the form of:
		\begin{equation*}
			\exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right)
		\end{equation*}
		where $a(\theta)$, $b(x)$, $c_{i}(\theta)$ and $d_{i}(x)$ for $i=1,\cdots,k$, are real-valued functions.
	\end{enumerate}
	then the distribution of $X$ is said to be an member of \textbf{$k$-parameter exponential family}.
\end{defn}

\newpage
or in one-parameter case,
\begin{defn}
	Suppose that a random variable $X$ has a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$. If we find that:
	\begin{enumerate}
		\item $\supp(X)$ does not depend on $\theta$.
		\item The PDF or PMF of $X$ can be written in the form of:
		\begin{equation*}
			\exp[a(\theta)+b(x)+c(\theta)d(x)]
		\end{equation*}
		where $a(\theta)$, $b(x)$, $c(\theta)$ and $d(x)$ are real-valued functions.
	\end{enumerate}
	then the distribution of $X$ is said the be an member of \textbf{one-parameter exponential family}.
\end{defn}
\begin{rem}
	The distribution whose support depends on $\theta$ does not belong to the exponential family. E.g. $\U[0,\theta]$.
\end{rem}
\begin{rem}
	Most of the parametric distributions we discussed are members of an exponential family. E.g. normal distribution, gamma distribution, Poisson distribution, binomial distribution, and so on.
\end{rem}
The following results show that we can find complete and minimal sufficient and statistic from exponential family.
\begin{thm}
	\label{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in an one-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}$, that can be written in the form of:
	\begin{equation*}
		\exp[a(\theta)+b(x)+c(\theta)d(x)]
	\end{equation*}
	Then, $\sum_{i=1}^{n}d(X_{i})$ is a complete and minimal sufficient statistic.
\end{thm}
\begin{thm}
	\label{Chapter 3 (Theorem) Exponential family to jointly complete and minimal sufficient statistics}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from a distribution in a $k$-parameter exponential family with a PDF $f(x|\theta)$ or a PMF $p(x|\theta)$, where $\theta\in\Theta\subset\mathbb{R}^{k}$ for an integer $k>1$, that can be written in the form of:
	\begin{equation*}
		\exp\left(a(\theta)+b(x)+\sum_{j=1}^{k}c_{j}(\theta)d_{j}(x)\right)
	\end{equation*}
	Then the set $\{\sum_{i=1}^{n}d_{1}(X_{i}),\cdots,\sum_{i=1}^{n}d_{k}(X_{i})\}$ is a set of jointly complete and minimal sufficient statistics.
\end{thm}
\begin{eg}
	\label{Chapter 3 (Example) Complete and minimal sufficient statistic of Poisson distribution}
	Consider a random sample from $\Poisson(\lambda)$, where $\lambda\in(0,\infty)$ is unknown. We have:
	\begin{equation*}
		p(x|\lambda)=\frac{\lambda^{x}e^{-\lambda}}{x!}=\exp[-\lambda-\ln(x!)+x\ln{\lambda}]
	\end{equation*}
	Since the support $\{0,1,\cdots\}$ does not depend on $\lambda$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic.
\end{eg}
\begin{eg}
	Consider a random sample from $\Bern(\theta)$, where $\theta\in(0,1)$ is unknown. We have:
	\begin{equation*}
		p(x|\theta)=\theta^{x}(1-\theta)^{1-x}=\exp\left[\ln(1-\theta)+x\ln\left(\frac{\theta}{1-\theta}\right)\right]
	\end{equation*}
	Since the support $\{0,1\}$ does not depend on $\theta$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic.
\end{eg}
\begin{eg}
	Consider a random sample from $\N(\mu,\sigma^{2})$, where $\mu$ and $\sigma>0$ are unknown. We have:
	\begin{equation*}
		f(x|\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi\sigma^{2}}}\exp\left(-\frac{(x-\mu)^{2}}{2\sigma^{2}}\right)=\exp\left(-\frac{\mu^{2}}{2\sigma^{2}}-\frac{1}{2}\ln(2\pi\sigma^{2})+\frac{\mu}{\sigma^{2}}x-\frac{1}{2\sigma^{2}}x^{2}\right)
	\end{equation*}
	Since the support does not depend on $\mu$ and $\sigma^{2}$, by Theorem \ref{Chapter 3 (Theorem) Exponential family to jointly complete and minimal sufficient statistics}, $\sum_{i=1}^{n}X_{i}$ and $\sum_{i=1}^{n}X_{i}^{2}$ are jointly complete and minimal sufficient statistics.
\end{eg}

\newpage
\section{Relationship of completeness and sufficiency with UMVUE}
We still haven't explained why complete and minimal sufficient statistic can lead to UMVUE. This is due to the following theorem.
\begin{thm}\named{Lehmann-Scheff\'e Theorem} Let $CS$ be a complete and (minimal) sufficient statistic. If there exists a function $h(CS)$ which is unbiased for $g(\theta)$, then $h(CS)$ is the unique UMVUE of $g(\theta)$.
\end{thm}
\begin{thm}
	Let $CS$ be a complete and (minimal) sufficient statistic. If $\E[f(\mathbf{X})]=g(\theta)$ for all $\theta$, then $h(CS)=\E[f(\mathbf{X})|T]$ is the UMVUE for $g(\theta)$.
\end{thm}
\begin{rem}
	From this theorem, we can formulate some strategies to find the UMVUE which is a function of $CS$:
	\begin{enumerate}
		\item Guess the correct form of the function of $CS$.
		\item Solve for $h(CS)$ by $\E[h(CS)]=g(\theta)$.
		\item Use Rao-Blackwell Theorem to construct $h(CS)$ by guessing or finding any unbiased estimator $T$ for $g(\theta)$ and then evaluating $h(CS)=\E(T|CS)$.
	\end{enumerate}
\end{rem}
\begin{eg}
	\label{Chapter 3 (Example) UMVUE of Exp(theta)}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $X\sim\Exp(\theta)$, where $\theta\in(0,\infty)$ is unknown. Find the UMVUE of $g(\theta)=\theta$. We try Strategy 1.\\
	We have $\theta=\frac{1}{\E(X)}$. Since exponential distribution of $X$ belongs to an exponential family, we can find that $CS=\sum_{i=1}^{n}X_{i}$. We suspect that UMVUE is related to $\frac{n}{\sum_{i=1}^{n}X_{i}}$. For $n>1$, since $\Exp(\theta)=\Gam(1,\theta)$,
	\begin{equation*}
		\E\left(\frac{1}{\sum_{i=1}^{n}X_{i}}\right)=\int_{0}^{\infty}\frac{\theta^{n}}{x\Gamma(n)}x^{n-1}e^{-\theta x}\,dx=\frac{\theta}{\Gamma(n)}\int_{0}^{\infty}\theta(\theta x)^{n-2}e^{-\theta x}\,dx=\frac{\theta\Gamma(n-1)}{\Gamma(n)}=\frac{\theta}{n-1}
	\end{equation*}
	Therefore, we have found that $\frac{n-1}{\sum_{i=1}^{n}X_{i}}$ is the UMVUE.
\end{eg}
\begin{eg}
	We continue the example above. This time we try Strategy 2, which is solving for $h(CS)$ by $\E[h(CS)]=g(\theta)=\theta$.
	\begin{align*}
		\int_{0}^{\infty}h(x)\frac{\theta^{n}}{\Gamma(n)}x^{n-1}e^{-\theta x}\,dx&=\theta\\
		\int_{0}^{\infty}h(x)\frac{\theta^{n-1}}{\Gamma(n)}x^{n-1}e^{-\theta x}\,dx&=1\\
		\int_{0}^{\infty}\left(h(x)\frac{x}{n-1}\right)\frac{\theta^{n-1}}{\Gamma(n-1)}x^{(n-1)-1}e^{-\theta x}\,dx&=1
	\end{align*} 
	It is only true if $h(x)\frac{x}{n-1}=1$ for all $s>0$. Thus, $h(x)=\frac{n-1}{x}$ and therefore:
	\begin{equation*}
		h\left(\sum_{i=1}^{n}X_{i}\right)=\frac{n-1}{\sum_{i=1}^{n}X_{i}}
	\end{equation*}
	Since it is unbiased for $\theta$ and is a function of $CS$, by Lehmann-Scheff\'e Theorem, it is the UMVUE of $\theta$.
\end{eg}

\newpage
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Poisson(\lambda)$, where $\lambda\in(0,\infty)$ is unknown. Find the UMVUE of $g(\lambda)=e^{-\lambda}$. We try Strategy 3.\\
	From Example \ref{Chapter 3 (Example) Complete and minimal sufficient statistic of Poisson distribution}, $\sum_{i=1}^{n}X_{i}$ is a complete and minimal sufficient statistic. Note that $g(\lambda)=e^{-\lambda}=\prob(X_{1}=0)=\mathbf{1}_{X_{1}=0}$ and thus it is a trivial unbiased estimator of $g(\lambda)$. By Rao-Blackwell Theorem, $\E(\mathbf{1}_{X_{1}=0}|\sum_{i=1}^{n}X_{i})$ is unbiased for $g(\lambda)$. By Lehmann-Scheff\'e Theorem, it is the unique UMVUE of $g(\lambda)$. We compute the UMVUE.\\
	For $n=1$,
	\begin{equation*}
		\E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\E(\mathbf{1}_{X_{1}=0}|X_{1})=\prob(X_{1}=0|X_{1})=\mathbf{1}_{X_{1}=0}
	\end{equation*}
	For $n>1$,
	\begin{align*}
		\E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}=s\right.\right)=\prob\left(X_{1}=0\left|\sum_{i=1}^{n}X_{i}=s\right.\right)&=\frac{\prob(X_{1}=0,\sum_{i=1}^{n}X_{i}=s)}{\prob(\sum_{i=1}^{n}X_{i}=s)}\\
		&=\frac{\prob(X_{1}=0)\prob(\sum_{i=2}^{n}X_{i}=s)}{\prob(\sum_{i=1}^{n}X_{i}=s)}\\
		&=\frac{e^{-\lambda}e^{-(n-1)\lambda}[(n-1)\lambda]^{s}s!}{e^{-n\lambda}(n\lambda)^{s}s!}\\
		&=\left(\frac{n-1}{n}\right)^{s}
	\end{align*}
	Therefore, the UMVUE for $g(\lambda)=e^{-\lambda}$ is:
	\begin{equation*}
		\E\left(\mathbf{1}_{X_{1}=0}\left|\sum_{i=1}^{n}X_{i}\right.\right)=\begin{cases}
			\mathbf{1}_{X_{1}=0}, &n=1\\
			\left(\frac{n-1}{n}\right)^{\sum_{i=1}^{n}X_{i}}, &n>1
		\end{cases}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\U[0,\theta]$, where $\theta>0$ is unknown. Since uniform distribution is not in an exponential family, we cannot use Theorem \ref{Chapter 3 (Theorem) Exponential family to complete and minimal sufficient statistic} to find a complete and minimal sufficient statistic.\\
	From Example \ref{Chapter 3 (Example) complete and sufficient statistic of U[0,theta]}, we have found that $X_{(n)}$ is a complete and sufficient statistic. By checking for unbiasedness,
	\begin{equation*}
		\E(X_{(n)})=\frac{n}{n+1}\theta
	\end{equation*}
	Therefore, by Lehmann-Scheff\'e Theorem, the UMVUE of $\theta$ is $\frac{n+1}{n}X_{(n)}$.
\end{eg}
\section{Cram\'er-Rao Inequality}
Recall Theorem \ref{Chapter 2 (Thoerem) Sequence of MME is asympt. normal}, we claim that a sequence of MLE is asymptotically efficient, which means that:
\begin{equation*}
	\mathcal{I}_{X}^{-1}(\theta)
\end{equation*}
is the lowest possible bound for any unbiased estimator. The reason is because of the Cram\'er-Rao Inequality (C-R Inequality).
\begin{thm}\named{Cram\'er-Rao Inequality}
	Under the regularity conditions, the variance of an unbiased estimator $T(\mathbf{X})=T(X_{1},\cdots,X_{n})$ for $\theta$, based on a set of random variables $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ from their joint PDF $f_{X_{1},\cdots,X_{n}}(x_{1},\cdots,x_{n}|\theta)$ satisfies the following inequality:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{1}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{1}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}
	\end{equation*}
	The lower bound is called \textbf{Cram\'er-Rao lower bound} (CRLB).
\end{thm}
\begin{rem}
	The Cram\'er-Rao Inequality can be written as:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{1}{-\E\left[\odv*[order={2}]{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]}
	\end{equation*}
\end{rem}

\newpage
\begin{rem}
	If $\mathbf{X}$ is a random sample of size $n$, then we have:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{1}{n\E\left[\odv*{\ln{f_{X_{1}}(X_{1}|\theta)}}{\theta}\right]^{2}}=\frac{1}{-n\E\left[\odv*[order={2}]{\ln{f_{X_{1}}(X_{1}|\theta)}}{\theta}\right]}
	\end{equation*}
\end{rem}
\begin{eg}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\N(\theta,\sigma^{2})$., where $\sigma^{2}$ is known and $\theta$ is unknown. The CRLB for $\theta$ is:
	\begin{equation*}
		\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{\sigma^{2}}{n}
	\end{equation*}
\end{eg}
\begin{eg}
	Let $\mathbf{X}=\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Bern(p)$, where $p$ is unknown. The CRLB for $p$ is:
	\begin{equation*}
		\frac{1}{n\mathcal{I}_{X_{1}}(p)}=\frac{p(1-p)}{n}
	\end{equation*}
\end{eg}
Often time, we want to estimate a function of $\theta$, $g(\theta)$, instead of $\theta$.
\begin{thm}
	Under the regularity conditions, if $T(\mathbf{X})=T(X_{1},\cdots,X_{n})$ is an unbiased estimator for $g(\theta)$, then the Cram\'er-Rao Inequality for $g(\theta)$ is:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{X_{1},\cdots,X_{n}}(\theta)}=\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\E\left[\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}\right]^{2}}
	\end{equation*}
\end{thm}
\begin{proofing}
	Let $U=\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}$ and $V=T(\mathbf{X})$. We have:
	\begin{equation*}
		-1\leq\frac{\cov(U,V)}{\sqrt{\Var(U)\Var(V)}}\leq 1\implies\frac{(\cov(U,V))^{2}}{\Var(U)}\leq\Var(V)
	\end{equation*}
	We may find that $\Var(U)=\Var\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right)=\mathcal{I}_{\mathbf{X}}(\theta)$. In addition,
	\begin{align*}
		\cov\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta},T(\mathbf{X})\right)&=\E\left[T(\mathbf{X})\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]-\E\left[\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]\E[T(\mathbf{X})]\\
		&=\E\left[T(\mathbf{X})\odv*{\ln{f_{\mathbf{X}}(\mathbf{X}|\theta)}}{\theta}\right]\\
		&=\intinfty\cdots\intinfty T(\mathbf{x})\left(\odv*{\ln{f_{\mathbf{X}}(\mathbf{x}|\theta)}}{\theta}\right)f_{\mathbf{X}}(\mathbf{x}|\theta)\,d\mathbf{x}\\
		&=\intinfty\cdots\intinfty T(\mathbf{x})\odv*{f_{\mathbf{X}}(\mathbf{x}|\theta)}{\theta}\,d\mathbf{x}\\
		&=\odv*{\E[T(\mathbf{X})]}{\theta}\\
		&=\odv*{g(\theta)}{\theta}
	\end{align*}
	Therefore, we have that:
	\begin{equation*}
		\Var(T(\mathbf{X}))\geq\frac{\left[\odv*{g(\theta)}{\theta}\right]^{2}}{\mathcal{I}_{\mathbf{X}}(\theta)}
	\end{equation*}
\end{proofing}
\begin{rem}
	Since CRLB is the lowest bound of variance for any unbiased estimator, any unbiased estimator whose variance can achieve the CRLB for $g(\theta)$ is the UMVUE for $g(\theta)$.
\end{rem}
\begin{rem}
	It is not necessary that a UMVUE has a variance equal to the CRLB.
\end{rem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta\in(0,\infty)$ is unknown.\\
	The CRLB of $\theta$ is $\frac{\theta^{2}}{n}$. From Example \ref{Chapter 3 (Example) UMVUE of Exp(theta)}, we have found that $\frac{n-1}{\sum_{i=1}^{n}X_{i}}$ is the UMVUE of $\theta$ when $n>1$.\\
	After some tedious calculation, for $n>2$, we would find that $\Var\left(\frac{n-1}{\sum_{i=1}^{n}X_{i}}\right)=\frac{\theta^{2}}{n-2}\geq\frac{\theta^{2}}{n}$.
\end{eg}

\newpage
When would the equality for C-R inequality holds?
\begin{thm}
	\label{Chapter 3 (Theorem) C-R equality}
	Under the regularity conditions, the C-R equality holds if and only if:
	\begin{equation*}
		\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]
	\end{equation*}
	where $A(\theta,n)$ is a non-zero function. The statistic $T(X_{1},\cdots,X_{n})$ is an UMVUE of $g(\theta)$. 
\end{thm}
\begin{lem}
	If $T(X_{1},\cdots,X_{n})$ is an UMVUE of $g(\theta)$ such that C-R equality holds, then $aT(X_{1},\cdots,X_{n})+b$ is an UMVUE of $ag(\theta)+b$, where $a\neq 0$.
\end{lem}
\begin{proofing}
	\begin{equation*}
		\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}=A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]=\frac{A(\theta,n)}{a}[(aT(X_{1},\cdots,X_{n})+b)-(ag(\theta)+b)]
	\end{equation*}
	By having $B(\theta,n)=\frac{A(\theta,n)}{a}$, we can find that $aT(X_{1},\cdots,X_{n})+b$ is an UMVUE of $ag(\theta)+b$.
\end{proofing}
This theorem has an interesting result that if we can write $\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}$ into $A(\theta,n)[T(X_{1},\cdots,X_{n})-g(\theta)]$, then the statistic must be an UMVUE.
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Poisson(\lambda)$, where $\lambda$ is unknown.
	\begin{align*}
		\odv*{\ln{p_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\lambda)}}{\lambda}&=\odv*{\ln\left(\prod_{i=1}^{n}\frac{\lambda^{X_{i}}e^{-\lambda}}{X_{i}!}\right)}{\lambda}\\
		&=\odv*{\sum_{i=1}^{n}[X_{i}\ln{\lambda}-\lambda-\ln(X_{i}!)]}{\lambda}\\
		&=\sum_{i=1}^{n}\frac{X_{i}}{\lambda}-1\\
		&=\frac{n}{\lambda}(\overline{X}-\lambda)
	\end{align*}
	Therefore, by Theorem \ref{Chapter 3 (Theorem) C-R equality}, $\overline{X}$ is an UMVUE of $\lambda$ and:
	\begin{equation*}
		\Var(\overline{X})=\frac{1}{n\mathcal{I}_{X_{1}}(\lambda)}=\frac{\lambda}{n}
	\end{equation*}
\end{eg}
\begin{rem}
	For any particular function of $\theta$ other than Euclidean transformation, Theorem \ref{Chapter 3 (Theorem) C-R equality} is not useful.
\end{rem}
\begin{eg}
	Let $\{X_{1},\cdots,X_{n}\}$ be a random sample of size $n$ from $\Exp(\theta)$, where $\theta$ is unknown.
	\begin{align*}
		\odv*{\ln{f_{X_{1},\cdots,X_{n}}(X_{1},\cdots,X_{n}|\theta)}}{\theta}&=\odv*{\ln\left(\prod_{i=1}^{n}\theta e^{-\theta X_{i}}\right)}{\theta}\\
		&=\odv*{\sum_{i=1}^{n}(\ln{\theta}-\theta X_{i})}{\theta}\\
		&=\sum_{i=1}^{n}\left(\frac{1}{\theta}-X_{i}\right)\\
		&=-n\left(\overline{X}-\frac{1}{\theta}\right)
	\end{align*}
	Therefore, by Theorem \ref{Chapter 3 (Theorem) C-R equality}, $\overline{X}$ is an UMVUE of $\frac{1}{\theta}$ and:
	\begin{equation*}
		\Var(\overline{X})=\frac{1}{n\mathcal{I}_{X_{1}}(\theta)}=\frac{\theta^{2}}{n}
	\end{equation*}
	However, since $\theta$ cannot be written as Euclidean transformation of $\frac{1}{\theta}$, we cannot use the Theorem to find the UMVUE of $\theta$.
\end{eg}
\begin{rem}
	Theorem \ref{Chapter 3 (Theorem) C-R equality} also can only be used under regularity conditions. For instance, for $\U[0,\theta]$, we cannot use this Theorem.
\end{rem}

\chapter{Hypothesis Testing}
This chapter will be primarily focus on comparing different unbiased point estimators. 

In engineering and science field, people usually hypothesize something about a system. Before they prove the conjecture using experimental data, they need to define a hypothesis.
\begin{defn}
	\textbf{Statistical hypothesis} is an assertion or conjecture of the random variable of our interest. If a parametric distribution is considered, then a statistical hypothesis can be a conjecture about the true value of the unknown parameters of the parametric distribution.
\end{defn}
\begin{eg}
	\label{Chapter 4 (Example) Engineer example}
	An engineer decides on the basis of sample data whether the true average lifetime of a certain kind of tire is at least $22,000$ miles.
	
	The engineer has to test the hypothesis that $\theta$ in $\Exp(\theta)$ is at least $22,000$.
\end{eg}
\begin{eg}
	An agronomist want to decide on the basis of experiments whether one kind of fertilizer produces a higher yield of soybeans than another.
	
	The agronomist has to test the hypothesis that $\mu_{1}>\mu_{2}$ from two distributions $\N(\mu_{1},\sigma_{1}^{2})$ and $\N(\mu_{2},\sigma_{2}^{2})$.
\end{eg}
\begin{eg}
	\label{Chapter 4 (Example) Manufacturer example}
	A manufacturer of pharmaceutical products decides on the basis of samples whether $90\%$ of all patients given a new medication will recover from a certain disease.
	
	The manufacturer has to test the hypothesis that $\theta$ in $\Bin(n,\theta)$ equals $0.90$.
\end{eg}

\section{Null and Alternative Hypotheses}
The hypothesis of our interest is related to a particular class of $\theta$, say $\Theta_{0}$, and its complement $\Theta_{1}=\Theta\setminus\Theta_{0}$. This two classes are subsets of the parameter space $\Theta$ of $\theta$.
\begin{defn}
	The hypothesis with $\theta\in\Theta_{0}$ is the \textbf{null hypothesis} $H_{0}$, and the hypothesis with $\theta\in\Theta_{1}$ is the \textbf{alternative hypothesis}.
\end{defn}
\begin{rem}
	We usually use the signs with implied equals in $H_{0}$.
\end{rem}
\begin{defn}
	The hypothesis is \textbf{simple} if the parametric distribution would be fully specified under the hypothesis. Otherwise, the hypothesis is \textbf{composite}.
\end{defn}
\begin{eg}
	Using the Example \ref{Chapter 4 (Example) Engineer example}, we have that:
	\begin{equation*}
		\begin{cases}
			H_{0}: & \theta\geq 22,000\\
			H_{1}: & \theta<22,000
		\end{cases}
	\end{equation*}
	Both $H_{0}$ and $H_{1}$ are composite because it does not specify the parameter.
\end{eg}

\newpage
\begin{eg}
	Using the Example \ref{Chapter 4 (Example) Manufacturer example}, we have that:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\theta=0.9\\
			H_{1}: &\theta\neq 0.9
		\end{cases}
	\end{equation*}
	$H_{0}$ is simple while $H_{1}$ is composite.
\end{eg}
In hypothesis testing, we want to see whether or not we can find an evidence to say that $H_{0}$ is false.
\begin{rem}
	Hypothesis testing usually follows these three steps:
	\begin{enumerate}
		\item Determine $H_{0}$ and $H_{1}$.
		\item Under $H_{0}$, define a rare event, event that happens with a very small probability in one experiment with $n$ data.
		\item Collect data.
		\begin{enumerate}
			\item If data causes the rare event to happen, it contradicts $H_{0}$. This mean we can say that $H_{0}$ is false and reject $H_{0}$.
			\item If data does not cause the rare event to happen, it does not contradicts $H_{0}$. This mean we cannot say that $H_{0}$ is false and we do not reject $H_{0}$.
		\end{enumerate}
	\end{enumerate}
\end{rem}
\begin{rem}
	Not rejecting $H_{0}$ does not mean we accept $H_{0}$. It just mean there is no sufficient evidence to reject $H_{0}$. The whole idea is to try gathering enough evidence to have great confidence that $H_{0}$ is false and $H_{1}$ is true.
\end{rem}
\begin{eg}
	We want to know whether or not a coin is fair. Consider a random experiment of flipping the coin $10$ times. We may determine that:
	\begin{equation*}
		\begin{cases}
			H_{0}: &\prob(\{H\})=\prob(\{T\})=0.5\\
			H_{1}: &\prob(\{H\})\neq\prob(\{T\})
		\end{cases}
	\end{equation*}
	Under $H_{0}$, we define that the event of getting $10$ tails to be the rare event under $H_{0}$ since the probability of getting $10$ tails in one experiment is $0.5^{10}\approx 0.00098$.
	
	We can them preform the experiment to collect data by flipping the coin $10$ times. If we get $10$ tails, then the collected data tells us that getting $10$ tails is not a rare event, which contradicts $H_{0}$. Therefore, we have evidence to suspect the reliability of $H_{0}$ and thus reject $H_{0}$ and accept $H_{1}$.
\end{eg}

\section{Test Errors and Error Probabilities}
After we decide the null and alternative hypotheses, we need to determine a test statistic, i.e. the point estimator, to construct a test of rejecting or not rejecting the null hypothesis.
\begin{defn}
	\textbf{Non-rejection region} $C_{0}$ is a subset of $\Theta$ such that we do not reject $H_{0}$, i.e.
	\begin{equation*}
		C_{0}=\{\mathbf{x}:\text{Not reject }H_{0}\}
	\end{equation*}
	\textbf{Rejection region} $C_{1}$ is a subset of $\Theta$ such that we reject $H_{0}$, i.e.
	\begin{equation*}
		C_{1}=\{\mathbf{x}:\text{Reject }H_{0}\}
	\end{equation*}
\end{defn}
However, there is no prefect test statement due to randomness of sample of data. Each test would lead to the following two kinds of errors.
\begin{defn}
	\textbf{Type I Error} is the error of rejecting $H_{0}$ when it is true. \textbf{Type II Error} is the error of not rejecting $H_{0}$ when it is false.
\end{defn}
\begin{figure}[h]
	\centering
	\begin{tabular}{||c|c|c|}
		\hline
		& Not reject $H_{0}$ & Reject $H_{0}$\\
		\hline
		If $H_{0}$ is true & No error & Type I Error\\
		If $H_{0}$ is false & Type II Error & No error\\
		\hline
	\end{tabular}
\end{figure}

\newpage
We may define their corresponding probabilities.
\begin{defn}
	\textbf{Type I error probability}, denoted by $\gamma(\theta)$, is the probability of rejecting $H_{0}$ for $\theta\in\Theta_{0}$.
	\begin{equation*}
		\gamma(\theta)=\prob(\text{Reject }H_{0}|\theta)=\prob(\mathbf{X}\in C_{1}|\theta)
	\end{equation*}
	\textbf{Type II error probability}, denoted by $\beta(\theta)$, is the probability of not rejecting $H_{0}$ for $\theta\in\Theta_{1}$.
	\begin{equation*}
		\beta(\theta)=\prob(\text{Not reject }H_{0}|\theta)=\prob(\mathbf{X}\in C_{0}|\theta)
	\end{equation*}
\end{defn}
\begin{rem}
	Since we cannot control $\gamma(\theta)$ and $\beta(\theta)$ at the same time, conventionally, we assign an upper bound to $\gamma(\theta)$ over $\Theta_{0}$ and find a test with $\beta(\theta)$ as small as possible. If we are dealing with continuous case,
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\gamma(\theta)=\alpha
	\end{equation*}
	If we are dealing with discrete case,
	\begin{equation*}
		\sup_{\theta\in\Theta_{0}}\gamma(\theta)\leq\alpha
	\end{equation*}
\end{rem}
\end{document}